{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9a856e43-5c29-4200-91e0-7b976df0bcbb",
      "metadata": {
        "id": "9a856e43-5c29-4200-91e0-7b976df0bcbb"
      },
      "source": [
        "# A Vector Database of Classic English Literature\n",
        "\n",
        "In the [previous notebook](https://github.com/tommyliphysics/tommyli-ml/blob/main/literature_vdb/notebooks/create.ipynb) we created a vector database of English texts downloaded from [Project Gutenberg](https://www.gutenberg.org). We will now look at accessing this database to perform a vector search and add new texts. We will do this by creating a class called LiteratureSearch with the functions add(), save() and search()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "lx4fQ4ZuUh7i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx4fQ4ZuUh7i",
        "outputId": "023515bf-746f-4e5c-ee60-cf4cb72f1c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence_transformers\n",
            "Successfully installed faiss-cpu-1.8.0.post1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 sentence_transformers-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark sentence_transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6wVC45GaXmds",
      "metadata": {
        "id": "6wVC45GaXmds"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "\n",
        "import urllib.request\n",
        "import regex\n",
        "\n",
        "def read_book(url):\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "    else:\n",
        "        if response is None:\n",
        "            print(\"Error: no response.\")\n",
        "            return None, None\n",
        "\n",
        "        content = response.read().decode(\"utf-8\")\n",
        "        book_title = regex.findall(\"Title: (.*?)[\\n|\\r]\", content)\n",
        "        author = regex.findall(\"Author: (.*?)[\\n|\\r]\", content)\n",
        "\n",
        "        if len(book_title) == 0:\n",
        "            print(\"Could not find name of book.\")\n",
        "            book_title = None\n",
        "        else:\n",
        "            print(\"Book title: \", book_title[0])\n",
        "\n",
        "        if len(author) == 0:\n",
        "            print(\"Could not find name of author.\")\n",
        "            author = None\n",
        "        else:\n",
        "            print(\"author: \", author[0])\n",
        "        if len(regex.findall(r'[\\*]+[\\s]+START OF THE PROJECT GUTENBERG EBOOK', content)) > 0:\n",
        "            content = regex.split(r'[\\*]+[\\s]+START OF THE PROJECT GUTENBERG EBOOK[^\\*]+[\\*]+', content)[1]\n",
        "            if len(regex.findall(r'[\\*]+[\\s]+END OF THE PROJECT GUTENBERG EBOOK', content)) > 0:\n",
        "                content = regex.split(r'[\\*]+[\\s]+END OF THE PROJECT GUTENBERG EBOOK[^\\*]+[\\*]+', content)[0]\n",
        "\n",
        "        blocks = regex.split(r'[\\n\\r]{3,}', content)\n",
        "\n",
        "        samples = []\n",
        "        for block in blocks:\n",
        "            block = regex.sub(r'[\\r\\n]+', ' ', block)\n",
        "            block = regex.sub(r'[\\s]+', ' ', block)\n",
        "            if len(block) > 0:\n",
        "                samples.append({'author': author[0], 'title': book_title[0], 'text': block})\n",
        "        response.close()\n",
        "\n",
        "        return samples\n",
        "\n",
        "class LiteratureSearch:\n",
        "    def __init__(self, books_fn, embeddings_fn, index_fn):\n",
        "      # load the Spark DataFrame\n",
        "        self.spark = SparkSession.builder.appName(\"Read\").getOrCreate()\n",
        "        self.df = self.spark.read.csv(books_fn, header=True, inferSchema=True)\n",
        "        self.df.show()\n",
        "\n",
        "      # get list of authors and book titles\n",
        "        authors = self.df.select('author').distinct().collect()\n",
        "        self.author_list = []\n",
        "\n",
        "        for author in authors:\n",
        "            filtered = self.df.filter(self.df.author == author['author']).select('title').distinct().collect()\n",
        "            for row in filtered:\n",
        "                self.author_list.append({'author': author['author'], 'title': row['title']})\n",
        "\n",
        "        print(\"Found titles:\")\n",
        "        for row in self.author_list:\n",
        "            print(f\"{row['title']}\\tby {row['author']}\")\n",
        "\n",
        "      # load the sentence transformer model\n",
        "        self.sentence_transformer_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "      # load embeddings\n",
        "        self.embeddings = np.load(embeddings_fn)\n",
        "      # load search index\n",
        "        self.index_L2 = faiss.read_index(index_fn)\n",
        "\n",
        "        self.max_id = self.df.count()\n",
        "\n",
        "\n",
        "    def add(self, url):\n",
        "        new_samples = read_book(url)\n",
        "        for sample in new_samples:\n",
        "            sample['id'] = self.max_id\n",
        "            self.max_id += 1\n",
        "        new_samples_df = self.spark.createDataFrame(new_samples)\n",
        "        self.df = self.df.union(new_samples_df)\n",
        "        print(f\"Added {new_samples_df.count()} new samples to the pyspark dataframe.\")\n",
        "\n",
        "        new_texts = [sample['text'] for sample in new_samples]\n",
        "        new_embeddings = self.sentence_transformer_model.encode(new_texts)\n",
        "        self.embeddings = np.concatenate((self.embeddings, new_embeddings))\n",
        "        print(f\"Added new embeddings of shape {new_embeddings.shape}. New shape: {self.embeddings.shape}\")\n",
        "        self.index_L2.add(new_embeddings)\n",
        "        print(f\"Added to index. New index size: {self.index_L2.ntotal}\")\n",
        "\n",
        "    def save(self, books_fn, embeddings_fn, index_fn):\n",
        "        self.df.repartition(1).write.mode('overwrite').csv(books_fn, header=True)\n",
        "        np.save('embeddings.npy', self.embeddings)\n",
        "        faiss.write_index(self.index_L2, \"index_L2.index\")\n",
        "\n",
        "    def search(self, query_text, k):\n",
        "        query_vector = self.sentence_transformer_model.encode(query_text)\n",
        "        distances, sorted_ids = self.index_L2.search(np.array([query_vector]), k)\n",
        "\n",
        "        sorted_ids = sorted_ids[0].tolist()\n",
        "        results = self.df.filter(self.df.id.isin(sorted_ids)).toPandas()\n",
        "        results['result'] = results['id'].apply(lambda x: sorted_ids.index(x)+1)\n",
        "        return results.sort_values(by='result').to_dict(orient='records')\n",
        "\n",
        "    def __del__(self):\n",
        "        self.spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fdf427a-b38a-4e62-bb62-806439314d31",
      "metadata": {
        "id": "5fdf427a-b38a-4e62-bb62-806439314d31"
      },
      "source": [
        "Let's create an instance of LiteratureSearch, which will load the text as a Spark DataFrame, embeddings and search index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8da633ac-8499-4587-b6e0-e7c0647ad90d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8da633ac-8499-4587-b6e0-e7c0647ad90d",
        "outputId": "49101d7e-ecc7-4266-d5b6-e7ab3d340a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---+--------------------+--------------------+\n",
            "|         author| id|                text|               title|\n",
            "+---------------+---+--------------------+--------------------+\n",
            "|Charles Dickens|  0|A TALE OF TWO CITIES|A Tale of Two Cities|\n",
            "|Charles Dickens|  1|A STORY OF THE FR...|A Tale of Two Cities|\n",
            "|Charles Dickens|  2|  By Charles Dickens|A Tale of Two Cities|\n",
            "|Charles Dickens|  3|            CONTENTS|A Tale of Two Cities|\n",
            "|Charles Dickens|  4|Book the First--R...|A Tale of Two Cities|\n",
            "|Charles Dickens|  5|CHAPTER I The Per...|A Tale of Two Cities|\n",
            "|Charles Dickens|  6|Book the Second--...|A Tale of Two Cities|\n",
            "|Charles Dickens|  7|CHAPTER I Five Ye...|A Tale of Two Cities|\n",
            "|Charles Dickens|  8|Book the Third--t...|A Tale of Two Cities|\n",
            "|Charles Dickens|  9|CHAPTER I In Secr...|A Tale of Two Cities|\n",
            "|Charles Dickens| 10|Book the First--R...|A Tale of Two Cities|\n",
            "|Charles Dickens| 11|CHAPTER I. The Pe...|A Tale of Two Cities|\n",
            "|Charles Dickens| 12|It was the best o...|A Tale of Two Cities|\n",
            "|Charles Dickens| 13|There were a king...|A Tale of Two Cities|\n",
            "|Charles Dickens| 14|It was the year o...|A Tale of Two Cities|\n",
            "|Charles Dickens| 15|France, less favo...|A Tale of Two Cities|\n",
            "|Charles Dickens| 16|In England, there...|A Tale of Two Cities|\n",
            "|Charles Dickens| 17|All these things,...|A Tale of Two Cities|\n",
            "|Charles Dickens| 18|CHAPTER II. The Mail|A Tale of Two Cities|\n",
            "|Charles Dickens| 19|It was the Dover ...|A Tale of Two Cities|\n",
            "+---------------+---+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Found titles:\n",
            "A Christmas Carol\tby Charles Dickens\n",
            "A Tale of Two Cities\tby Charles Dickens\n",
            "Great Expectations\tby Charles Dickens\n"
          ]
        }
      ],
      "source": [
        "litsearch = LiteratureSearch('books.csv', 'embeddings.npy', 'index_L2.index')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add a few more books to the database:"
      ],
      "metadata": {
        "id": "7uvgiE0nWesA"
      },
      "id": "7uvgiE0nWesA"
    },
    {
      "cell_type": "code",
      "source": [
        "litsearch.add('https://www.gutenberg.org/cache/epub/730/pg730.txt')"
      ],
      "metadata": {
        "id": "T9hF2xvHS3nr",
        "outputId": "45074e68-9f09-4620-deb1-f3ab96bc56eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "T9hF2xvHS3nr",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Book title:  Oliver Twist\n",
            "author:  Charles Dickens\n",
            "Added 3893 new samples to the pyspark dataframe.\n",
            "Added new embeddings of shape (3893, 384). New shape: (10838, 384)\n",
            "Added to index. New index size: 10838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "litsearch.add('https://www.gutenberg.org/cache/epub/766/pg766.txt')"
      ],
      "metadata": {
        "id": "kmTMedg5TpCX",
        "outputId": "20dbecf0-51b0-420d-8bee-9182b3563bfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kmTMedg5TpCX",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Book title:  David Copperfield\n",
            "author:  Charles Dickens\n",
            "Added 7192 new samples to the pyspark dataframe.\n",
            "Added new embeddings of shape (7192, 384). New shape: (18030, 384)\n",
            "Added to index. New index size: 18030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now save the database to file."
      ],
      "metadata": {
        "id": "59hHoKeLW4x8"
      },
      "id": "59hHoKeLW4x8"
    },
    {
      "cell_type": "code",
      "source": [
        "litsearch.save('books.csv', 'embeddings.npy', 'index_L2.index')"
      ],
      "metadata": {
        "id": "Wpug0r9ITCSI"
      },
      "id": "Wpug0r9ITCSI",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "17eae2b6-e75a-4018-8a0d-65a6d1ad40b1",
      "metadata": {
        "id": "17eae2b6-e75a-4018-8a0d-65a6d1ad40b1"
      },
      "source": [
        "Let's now load a new instance of LiteratureSearch and see if our new books are in the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8qJq3pq7djb2",
      "metadata": {
        "id": "8qJq3pq7djb2"
      },
      "outputs": [],
      "source": [
        "del(litsearch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "skTLhur5dlbD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skTLhur5dlbD",
        "outputId": "f78a75a2-6b76-42b8-8f26-6680c2a154bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---+--------------------+--------------------+\n",
            "|         author| id|                text|               title|\n",
            "+---------------+---+--------------------+--------------------+\n",
            "|Charles Dickens|  0|A TALE OF TWO CITIES|A Tale of Two Cities|\n",
            "|Charles Dickens|  1|A STORY OF THE FR...|A Tale of Two Cities|\n",
            "|Charles Dickens|  2|  By Charles Dickens|A Tale of Two Cities|\n",
            "|Charles Dickens|  3|            CONTENTS|A Tale of Two Cities|\n",
            "|Charles Dickens|  4|Book the First--R...|A Tale of Two Cities|\n",
            "|Charles Dickens|  5|CHAPTER I The Per...|A Tale of Two Cities|\n",
            "|Charles Dickens|  6|Book the Second--...|A Tale of Two Cities|\n",
            "|Charles Dickens|  7|CHAPTER I Five Ye...|A Tale of Two Cities|\n",
            "|Charles Dickens|  8|Book the Third--t...|A Tale of Two Cities|\n",
            "|Charles Dickens|  9|CHAPTER I In Secr...|A Tale of Two Cities|\n",
            "|Charles Dickens| 10|Book the First--R...|A Tale of Two Cities|\n",
            "|Charles Dickens| 11|CHAPTER I. The Pe...|A Tale of Two Cities|\n",
            "|Charles Dickens| 12|It was the best o...|A Tale of Two Cities|\n",
            "|Charles Dickens| 13|There were a king...|A Tale of Two Cities|\n",
            "|Charles Dickens| 14|It was the year o...|A Tale of Two Cities|\n",
            "|Charles Dickens| 15|France, less favo...|A Tale of Two Cities|\n",
            "|Charles Dickens| 16|In England, there...|A Tale of Two Cities|\n",
            "|Charles Dickens| 17|All these things,...|A Tale of Two Cities|\n",
            "|Charles Dickens| 18|CHAPTER II. The Mail|A Tale of Two Cities|\n",
            "|Charles Dickens| 19|It was the Dover ...|A Tale of Two Cities|\n",
            "+---------------+---+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Found titles:\n",
            "A Christmas Carol\tby Charles Dickens\n",
            "David Copperfield\tby Charles Dickens\n",
            "A Tale of Two Cities\tby Charles Dickens\n",
            "Oliver Twist\tby Charles Dickens\n",
            "Great Expectations\tby Charles Dickens\n"
          ]
        }
      ],
      "source": [
        "litsearch_new = LiteratureSearch('books.csv', 'embeddings.npy', 'index_L2.index')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that we have successfully updated the database. Let's now perform a vector search."
      ],
      "metadata": {
        "id": "lCUzBWDLXQ3q"
      },
      "id": "lCUzBWDLXQ3q"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "jFjc69Yhhu_r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFjc69Yhhu_r",
        "outputId": "aef05d5f-d07e-4a1c-df0b-0f0efdd3b46b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'author': 'Charles Dickens',\n",
              "  'id': 11486,\n",
              "  'text': '‘Can you cook this young gentleman’s breakfast for him, if you please?’ said the Master at Salem House.',\n",
              "  'title': 'David Copperfield',\n",
              "  'result': 1},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 3247,\n",
              "  'text': 'Must they! Let them not hope to taste it!',\n",
              "  'title': 'Great Expectations',\n",
              "  'result': 2},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 11435,\n",
              "  'text': '‘What have we got here?’ he said, putting a fork into my dish. ‘Not chops?’',\n",
              "  'title': 'David Copperfield',\n",
              "  'result': 3},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 14122,\n",
              "  'text': 'What with the novelty of this cookery, the excellence of it, the bustle of it, the frequent starting up to look after it, the frequent sitting down to dispose of it as the crisp slices came off the gridiron hot and hot, the being so busy, so flushed with the fire, so amused, and in the midst of such a tempting noise and savour, we reduced the leg of mutton to the bone. My own appetite came back miraculously. I am ashamed to record it, but I really believe I forgot Dora for a little while. I am satisfied that Mr. and Mrs. Micawber could not have enjoyed the feast more, if they had sold a bed to provide it. Traddles laughed as heartily, almost the whole time, as he ate and worked. Indeed we all did, all at once; and I dare say there was never a greater success.',\n",
              "  'title': 'David Copperfield',\n",
              "  'result': 4},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 14137,\n",
              "  'text': 'I thanked him and said, No; but would he take no dinner himself?',\n",
              "  'title': 'David Copperfield',\n",
              "  'result': 5},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 14639,\n",
              "  'text': 'I did not like to leave him, under such circumstances, and we all three dined together off a beefsteak pie--which was one of the many good things for which Peggotty was famous--and which was curiously flavoured on this occasion, I recollect well, by a miscellaneous taste of tea, coffee, butter, bacon, cheese, new loaves, firewood, candles, and walnut ketchup, continually ascending from the shop. After dinner we sat for an hour or so near the window, without talking much; and then Mr. Peggotty got up, and brought his oilskin bag and his stout stick, and laid them on the table.',\n",
              "  'title': 'David Copperfield',\n",
              "  'result': 6},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 9751,\n",
              "  'text': 'The soft-hearted cook added his intercession, and the result was that the man who had first appeared undertook its delivery.',\n",
              "  'title': 'Oliver Twist',\n",
              "  'result': 7},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 13118,\n",
              "  'text': '‘Ain’t you?’ said the waiter. ‘Young gentlemen is generally tired of beef and mutton: have a weal cutlet!’',\n",
              "  'title': 'David Copperfield',\n",
              "  'result': 8},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 17932,\n",
              "  'text': '‘Let him come in here!’ said I.',\n",
              "  'title': 'David Copperfield',\n",
              "  'result': 9},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 11438,\n",
              "  'text': 'So he took a chop by the bone in one hand, and a potato in the other, and ate away with a very good appetite, to my extreme satisfaction. He afterwards took another chop, and another potato; and after that, another chop and another potato. When we had done, he brought me a pudding, and having set it before me, seemed to ruminate, and to become absent in his mind for some moments.',\n",
              "  'title': 'David Copperfield',\n",
              "  'result': 10}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "litsearch_new.search(\"Let him cook!\", 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this series of notebooks we've seen how to create a vector database using a combination of PySpark, a sentence transformer and FAISS. Another option is to use existing vector search capabilities in cloud database services, e.g. AWS OpenSearch or MongoDB Atlas. The procedure there is similar to the one described here, but with a few differences: rather than storing the text and embeddings in separate files, one can store the embeddings as well as the text as attributes within the same collection. A search index can then be created using cloud services, and a vector search can be performed via an API."
      ],
      "metadata": {
        "id": "Y3-csEfIYsN9"
      },
      "id": "Y3-csEfIYsN9"
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}