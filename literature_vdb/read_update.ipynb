{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9a856e43-5c29-4200-91e0-7b976df0bcbb",
      "metadata": {
        "id": "9a856e43-5c29-4200-91e0-7b976df0bcbb"
      },
      "source": [
        "# A Vector Database of Classic English Literature\n",
        "\n",
        "In the [previous notebook](https://github.com/tommyliphysics/tommyli-ml/blob/main/literature_vdb/notebooks/create.ipynb) we created a vector database of English texts downloaded from [Project Gutenberg](https://www.gutenberg.org). We will now look at accessing this database to perform a vector search and add new texts. We will do this by creating a class called LiteratureSearch with the functions add(), save() and search()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "lx4fQ4ZuUh7i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx4fQ4ZuUh7i",
        "outputId": "3aa9e1ae-b58d-45c1-bba3-c03c3bd21e4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.6.20)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark sentence_transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6wVC45GaXmds",
      "metadata": {
        "id": "6wVC45GaXmds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ce185a-307c-46e6-9cb9-d2b8f76bf7ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "\n",
        "import gc\n",
        "\n",
        "import urllib.request\n",
        "import regex\n",
        "\n",
        "def read_book(url):\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "    else:\n",
        "        if response is None:\n",
        "            print(\"Error: no response.\")\n",
        "            return None, None\n",
        "\n",
        "        content = response.read().decode(\"utf-8\")\n",
        "        book_title = regex.findall(\"Title: (.*?)[\\n|\\r]\", content)\n",
        "        author = regex.findall(\"Author: (.*?)[\\n|\\r]\", content)\n",
        "\n",
        "        if len(book_title) == 0:\n",
        "            print(\"Could not find name of book.\")\n",
        "            book_title = None\n",
        "        else:\n",
        "            book_title = book_title[0]\n",
        "            print(\"Book title: \", book_title)\n",
        "\n",
        "        if len(author) == 0:\n",
        "            print(\"Could not find name of author.\")\n",
        "            author = None\n",
        "        else:\n",
        "            author = author[0]\n",
        "            print(\"author: \", author)\n",
        "        if len(regex.findall(r'[\\*]+[\\s]+START OF THE PROJECT GUTENBERG EBOOK', content)) > 0:\n",
        "            content = regex.split(r'[\\*]+[\\s]+START OF THE PROJECT GUTENBERG EBOOK[^\\*]+[\\*]+', content)[1]\n",
        "            if len(regex.findall(r'[\\*]+[\\s]+END OF THE PROJECT GUTENBERG EBOOK', content)) > 0:\n",
        "                content = regex.split(r'[\\*]+[\\s]+END OF THE PROJECT GUTENBERG EBOOK[^\\*]+[\\*]+', content)[0]\n",
        "\n",
        "        blocks = regex.split(r'[\\n\\r]{3,}', content)\n",
        "\n",
        "        samples = []\n",
        "        for block in blocks:\n",
        "            block = regex.sub(r'[\\r\\n]+', ' ', block)\n",
        "            block = regex.sub(r'[\\s]+', ' ', block)\n",
        "            if len(block) > 0:\n",
        "                samples.append({'author': author[0], 'title': book_title[0], 'text': block})\n",
        "        response.close()\n",
        "\n",
        "        return samples, book_title, author\n",
        "\n",
        "class LiteratureSearch:\n",
        "    def __init__(self, books_fn, embeddings_fn, index_fn):\n",
        "      # load the Spark DataFrame\n",
        "        self.spark = SparkSession.builder.appName(\"Read\").getOrCreate()\n",
        "        self.df = self.spark.read.csv(books_fn, header=True, inferSchema=True)\n",
        "        self.df.show()\n",
        "\n",
        "      # get list of authors and book titles\n",
        "        authors = self.df.select('author').distinct().collect()\n",
        "        self.author_list = []\n",
        "\n",
        "        for author in authors:\n",
        "            filtered = self.df.filter(self.df.author == author['author']).select('title').distinct().collect()\n",
        "            for row in filtered:\n",
        "                self.author_list.append({'author': author['author'], 'title': row['title']})\n",
        "\n",
        "        print(\"Found titles:\")\n",
        "        for row in self.author_list:\n",
        "            print(f\"{row['title']}\\tby {row['author']}\")\n",
        "        self.author_list = pd.DataFrame(self.author_list)\n",
        "\n",
        "      # load the sentence transformer model\n",
        "        self.sentence_transformer_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "      # load embeddings\n",
        "        self.embeddings = np.load(embeddings_fn)\n",
        "      # load search index\n",
        "        self.index_L2 = faiss.read_index(index_fn)\n",
        "\n",
        "        self.max_id = self.df.count()\n",
        "\n",
        "    def add(self, url):\n",
        "        new_samples, title, author = read_book(url)\n",
        "        if author in self.author_list['author'].unique():\n",
        "            if title in self.author_list[self.author_list['author']==author]['title'].unique():\n",
        "                print(f\"Error: {title} by {author} already exists in the collection.\")\n",
        "                return -1\n",
        "\n",
        "        for sample in new_samples:\n",
        "            sample['id'] = self.max_id\n",
        "            self.max_id += 1\n",
        "        new_samples_df = self.spark.createDataFrame(new_samples)\n",
        "        self.df = self.df.union(new_samples_df)\n",
        "        print(f\"Added {new_samples_df.count()} new samples to the pyspark dataframe.\")\n",
        "\n",
        "        new_texts = [sample['text'] for sample in new_samples]\n",
        "        new_embeddings = self.sentence_transformer_model.encode(new_texts)\n",
        "        self.embeddings = np.concatenate((self.embeddings, new_embeddings))\n",
        "        print(f\"Added new embeddings of shape {new_embeddings.shape}. New shape: {self.embeddings.shape}\")\n",
        "        self.index_L2.add(new_embeddings)\n",
        "        print(f\"Added to index. New index size: {self.index_L2.ntotal}\")\n",
        "\n",
        "    def save(self, books_fn, embeddings_fn, index_fn):\n",
        "        self.df.repartition(1).write.mode('overwrite').csv(books_fn, header=True)\n",
        "        np.save('embeddings.npy', self.embeddings)\n",
        "        faiss.write_index(self.index_L2, \"index_L2.index\")\n",
        "\n",
        "    def search(self, query_text, k):\n",
        "        query_vector = self.sentence_transformer_model.encode(query_text)\n",
        "        distances, sorted_ids = self.index_L2.search(np.array([query_vector]), k)\n",
        "\n",
        "        sorted_ids = sorted_ids[0].tolist()\n",
        "        results = self.df.filter(self.df.id.isin(sorted_ids)).toPandas()\n",
        "        results['result'] = results['id'].apply(lambda x: sorted_ids.index(x)+1)\n",
        "        return results.sort_values(by='result').to_dict(orient='records')\n",
        "\n",
        "    def close(self):\n",
        "        del(self.index_L2)\n",
        "        del(self.embeddings)\n",
        "        self.spark.stop()\n",
        "        print(\"Stopped Spark session.\")\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fdf427a-b38a-4e62-bb62-806439314d31",
      "metadata": {
        "id": "5fdf427a-b38a-4e62-bb62-806439314d31"
      },
      "source": [
        "Let's create an instance of LiteratureSearch, which will load the text as a Spark DataFrame, embeddings and search index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8da633ac-8499-4587-b6e0-e7c0647ad90d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8da633ac-8499-4587-b6e0-e7c0647ad90d",
        "outputId": "0ed4bd73-58f1-4592-e58b-80f1464caeaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---+--------------------+--------------------+\n",
            "|         author| id|                text|               title|\n",
            "+---------------+---+--------------------+--------------------+\n",
            "|Charles Dickens|  0|A TALE OF TWO CITIES|A Tale of Two Cities|\n",
            "|Charles Dickens|  1|A STORY OF THE FR...|A Tale of Two Cities|\n",
            "|Charles Dickens|  2|  By Charles Dickens|A Tale of Two Cities|\n",
            "|Charles Dickens|  3|            CONTENTS|A Tale of Two Cities|\n",
            "|Charles Dickens|  4|Book the First--R...|A Tale of Two Cities|\n",
            "|Charles Dickens|  5|CHAPTER I The Per...|A Tale of Two Cities|\n",
            "|Charles Dickens|  6|Book the Second--...|A Tale of Two Cities|\n",
            "|Charles Dickens|  7|CHAPTER I Five Ye...|A Tale of Two Cities|\n",
            "|Charles Dickens|  8|Book the Third--t...|A Tale of Two Cities|\n",
            "|Charles Dickens|  9|CHAPTER I In Secr...|A Tale of Two Cities|\n",
            "|Charles Dickens| 10|Book the First--R...|A Tale of Two Cities|\n",
            "|Charles Dickens| 11|CHAPTER I. The Pe...|A Tale of Two Cities|\n",
            "|Charles Dickens| 12|It was the best o...|A Tale of Two Cities|\n",
            "|Charles Dickens| 13|There were a king...|A Tale of Two Cities|\n",
            "|Charles Dickens| 14|It was the year o...|A Tale of Two Cities|\n",
            "|Charles Dickens| 15|France, less favo...|A Tale of Two Cities|\n",
            "|Charles Dickens| 16|In England, there...|A Tale of Two Cities|\n",
            "|Charles Dickens| 17|All these things,...|A Tale of Two Cities|\n",
            "|Charles Dickens| 18|CHAPTER II. The Mail|A Tale of Two Cities|\n",
            "|Charles Dickens| 19|It was the Dover ...|A Tale of Two Cities|\n",
            "+---------------+---+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Found titles:\n",
            "A Christmas Carol\tby Charles Dickens\n",
            "A Tale of Two Cities\tby Charles Dickens\n",
            "Great Expectations\tby Charles Dickens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "litsearch = LiteratureSearch('books.csv', 'embeddings.npy', 'index_L2.index')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add a few more books to the database:"
      ],
      "metadata": {
        "id": "7uvgiE0nWesA"
      },
      "id": "7uvgiE0nWesA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now save the database to file."
      ],
      "metadata": {
        "id": "59hHoKeLW4x8"
      },
      "id": "59hHoKeLW4x8"
    },
    {
      "cell_type": "code",
      "source": [
        "litsearch.save('books.csv', 'embeddings.npy', 'index_L2.index')"
      ],
      "metadata": {
        "id": "Wpug0r9ITCSI"
      },
      "id": "Wpug0r9ITCSI",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "17eae2b6-e75a-4018-8a0d-65a6d1ad40b1",
      "metadata": {
        "id": "17eae2b6-e75a-4018-8a0d-65a6d1ad40b1"
      },
      "source": [
        "Let's now load a new instance of LiteratureSearch and see if our new books are in the database."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "litsearch.close()"
      ],
      "metadata": {
        "id": "cXk-X4-akTPB",
        "outputId": "4d783312-aa9b-4c91-8483-80bb12987d4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cXk-X4-akTPB",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopped Spark session.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "skTLhur5dlbD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skTLhur5dlbD",
        "outputId": "fee54ce0-20eb-431c-d0ab-b3f7928f317a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---+--------------------+--------------------+\n",
            "|         author| id|                text|               title|\n",
            "+---------------+---+--------------------+--------------------+\n",
            "|Charles Dickens|  0|A TALE OF TWO CITIES|A Tale of Two Cities|\n",
            "|Charles Dickens|  1|A STORY OF THE FR...|A Tale of Two Cities|\n",
            "|Charles Dickens|  2|  By Charles Dickens|A Tale of Two Cities|\n",
            "|Charles Dickens|  3|            CONTENTS|A Tale of Two Cities|\n",
            "|Charles Dickens|  4|Book the First--R...|A Tale of Two Cities|\n",
            "|Charles Dickens|  5|CHAPTER I The Per...|A Tale of Two Cities|\n",
            "|Charles Dickens|  6|Book the Second--...|A Tale of Two Cities|\n",
            "|Charles Dickens|  7|CHAPTER I Five Ye...|A Tale of Two Cities|\n",
            "|Charles Dickens|  8|Book the Third--t...|A Tale of Two Cities|\n",
            "|Charles Dickens|  9|CHAPTER I In Secr...|A Tale of Two Cities|\n",
            "|Charles Dickens| 10|Book the First--R...|A Tale of Two Cities|\n",
            "|Charles Dickens| 11|CHAPTER I. The Pe...|A Tale of Two Cities|\n",
            "|Charles Dickens| 12|It was the best o...|A Tale of Two Cities|\n",
            "|Charles Dickens| 13|There were a king...|A Tale of Two Cities|\n",
            "|Charles Dickens| 14|It was the year o...|A Tale of Two Cities|\n",
            "|Charles Dickens| 15|France, less favo...|A Tale of Two Cities|\n",
            "|Charles Dickens| 16|In England, there...|A Tale of Two Cities|\n",
            "|Charles Dickens| 17|All these things,...|A Tale of Two Cities|\n",
            "|Charles Dickens| 18|CHAPTER II. The Mail|A Tale of Two Cities|\n",
            "|Charles Dickens| 19|It was the Dover ...|A Tale of Two Cities|\n",
            "+---------------+---+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Found titles:\n",
            "A Christmas Carol\tby Charles Dickens\n",
            "A Tale of Two Cities\tby Charles Dickens\n",
            "Great Expectations\tby Charles Dickens\n"
          ]
        }
      ],
      "source": [
        "litsearch_new = LiteratureSearch('books.csv', 'embeddings.npy', 'index_L2.index')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that we have successfully updated the database. Let's now perform a vector search."
      ],
      "metadata": {
        "id": "lCUzBWDLXQ3q"
      },
      "id": "lCUzBWDLXQ3q"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "jFjc69Yhhu_r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFjc69Yhhu_r",
        "outputId": "4284f823-bf73-4496-fe57-17fbf2f00f5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'author': 'Charles Dickens',\n",
              "  'id': 3247,\n",
              "  'text': 'Must they! Let them not hope to taste it!',\n",
              "  'title': 'Great Expectations',\n",
              "  'result': 1},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 3014,\n",
              "  'text': \"Let him in! It is a mercy he didn't shake his arm off. He was at home in five minutes. Nothing could be heartier. His niece looked just the same. So did Topper when _he_ came. So did the plump sister when _she_ came. So did every one when _they_ came. Wonderful party, wonderful games, wonderful unanimity, won-der-ful happiness!\",\n",
              "  'title': 'A Christmas Carol',\n",
              "  'result': 2},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 3177,\n",
              "  'text': '“Leave any for him? Who’s him?” said my friend, stopping in his crunching of pie-crust.',\n",
              "  'title': 'Great Expectations',\n",
              "  'result': 3},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 3202,\n",
              "  'text': 'We were to have a superb dinner, consisting of a leg of pickled pork and greens, and a pair of roast stuffed fowls. A handsome mince-pie had been made yesterday morning (which accounted for the mincemeat not being missed), and the pudding was already on the boil. These extensive arrangements occasioned us to be cut off unceremoniously in respect of breakfast; “for I ain’t,” said Mrs. Joe,—“I ain’t a-going to have no formal cramming and busting and washing up now, with what I’ve got before me, I promise you!”',\n",
              "  'title': 'Great Expectations',\n",
              "  'result': 4},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 6673,\n",
              "  'text': '“Let’s go in!”',\n",
              "  'title': 'Great Expectations',\n",
              "  'result': 5},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 4398,\n",
              "  'text': '“He is dressed like a ’spectable pieman. A sort of a pastry-cook.”',\n",
              "  'title': 'Great Expectations',\n",
              "  'result': 6},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 672,\n",
              "  'text': '“Now your dinner is done,” Carton presently said, “why don’t you call a health, Mr. Darnay; why don’t you give your toast?”',\n",
              "  'title': 'A Tale of Two Cities',\n",
              "  'result': 7},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 4673,\n",
              "  'text': '“Very much,” was Wemmick’s reply, “for I have had my legs under the desk all day, and shall be glad to stretch them. Now, I’ll tell you what I have got for supper, Mr. Pip. I have got a stewed steak,—which is of home preparation,—and a cold roast fowl,—which is from the cook’s-shop. I think it’s tender, because the master of the shop was a Juryman in some cases of ours the other day, and we let him down easy. I reminded him of it when I bought the fowl, and I said, “Pick us out a good one, old Briton, because if we had chosen to keep you in the box another day or two, we could easily have done it.” He said to that, “Let me make you a present of the best fowl in the shop.” I let him, of course. As far as it goes, it’s property and portable. You don’t object to an aged parent, I hope?”',\n",
              "  'title': 'Great Expectations',\n",
              "  'result': 8},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 430,\n",
              "  'text': '“Don’t do it!” said Mr. Crunches looking about, as if he rather expected to see the loaf disappear under the efficacy of his wife’s petitions. “I ain’t a going to be blest out of house and home. I won’t have my wittles blest off my table. Keep still!”',\n",
              "  'title': 'A Tale of Two Cities',\n",
              "  'result': 9},\n",
              " {'author': 'Charles Dickens',\n",
              "  'id': 3120,\n",
              "  'text': '“Been bolting his food, has he?” cried my sister.',\n",
              "  'title': 'Great Expectations',\n",
              "  'result': 10}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "litsearch_new.search(\"Let him cook!\", 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this series of notebooks we've seen how to create a vector database using a combination of PySpark, a sentence transformer and FAISS. Another option is to use existing vector search capabilities in cloud database services, e.g. AWS OpenSearch or MongoDB Atlas. The procedure there is similar to the one described here, but with a few differences: rather than storing the text and embeddings in separate files, one can store the embeddings as well as the text as attributes within the same collection. A search index can then be created using cloud services, and a vector search can be performed via an API."
      ],
      "metadata": {
        "id": "Y3-csEfIYsN9"
      },
      "id": "Y3-csEfIYsN9"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "68GKstTZaHo4"
      },
      "id": "68GKstTZaHo4",
      "execution_count": 7,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}