{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5d75882b-0eae-4de8-9ff9-cd69967ad825",
      "metadata": {
        "id": "5d75882b-0eae-4de8-9ff9-cd69967ad825"
      },
      "source": [
        "# Historical trends in scientific publishing\n",
        "\n",
        "The ArXiv provides the complete metadata on kaggle for all publications it hosts that is updated weekly. In this notebook we'll be looking for publications in the condensed matter (cond-mat) category. Let's load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "771eefca-9fe7-4772-933c-7ad191451b9d",
      "metadata": {
        "id": "771eefca-9fe7-4772-933c-7ad191451b9d",
        "outputId": "b0f092c5-6e15-4a75-880f-628c3cbc97fe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>categories</th>\n",
              "      <th>id</th>\n",
              "      <th>authors</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>versions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cond-mat.mes-hall</td>\n",
              "      <td>0704.0006</td>\n",
              "      <td>Y. H. Pong and C. K. Law</td>\n",
              "      <td>Bosonic characters of atomic Cooper pairs acro...</td>\n",
              "      <td>We study the two-particle wave function of p...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cond-mat.mtrl-sci</td>\n",
              "      <td>0704.0008</td>\n",
              "      <td>Damian C. Swift</td>\n",
              "      <td>Numerical solution of shock and ramp compressi...</td>\n",
              "      <td>A general formulation was developed to repre...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cond-mat.str-el cond-mat.stat-mech</td>\n",
              "      <td>0704.0025</td>\n",
              "      <td>A. S. Mishchenko (1 and 2) and N. Nagaosa (1 a...</td>\n",
              "      <td>Spectroscopic Properties of Polarons in Strong...</td>\n",
              "      <td>We present recent advances in understanding ...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cond-mat.mes-hall</td>\n",
              "      <td>0704.0027</td>\n",
              "      <td>M. O. Goerbig, J.-N. Fuchs, K. Kechedzhi, Vlad...</td>\n",
              "      <td>Filling-Factor-Dependent Magnetophonon Resonan...</td>\n",
              "      <td>We describe a peculiar fine structure acquir...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>cond-mat.str-el</td>\n",
              "      <td>0704.0030</td>\n",
              "      <td>J.P.Hague and N.d'Ambrumenil</td>\n",
              "      <td>Tuning correlation effects with electron-phono...</td>\n",
              "      <td>We investigate the effect of tuning the phon...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369013</th>\n",
              "      <td>supr-con cond-mat.supr-con</td>\n",
              "      <td>supr-con/9608008</td>\n",
              "      <td>R. Prozorov, M. Konczykowski, B. Schmidt, Y. Y...</td>\n",
              "      <td>On the origin of the irreversibility line in t...</td>\n",
              "      <td>We report on measurements of the angular dep...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Mon, 26 Aug 199...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369014</th>\n",
              "      <td>supr-con cond-mat.supr-con</td>\n",
              "      <td>supr-con/9609001</td>\n",
              "      <td>Durga P. Choudhury, Balam A. Willemsen, John S...</td>\n",
              "      <td>Nonlinear Response of HTSC Thin Film Microwave...</td>\n",
              "      <td>The non-linear microwave surface impedance o...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Sat, 31 Aug 199...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369015</th>\n",
              "      <td>supr-con cond-mat.supr-con</td>\n",
              "      <td>supr-con/9609002</td>\n",
              "      <td>Balam A. Willemsen, J. S. Derov and S.Sridhar ...</td>\n",
              "      <td>Critical State Flux Penetration and Linear Mic...</td>\n",
              "      <td>The vortex contribution to the dc field (H) ...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Tue, 3 Sep 1996...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369016</th>\n",
              "      <td>supr-con cond-mat.supr-con</td>\n",
              "      <td>supr-con/9609003</td>\n",
              "      <td>Yasumasa Hasegawa (Himeji Institute of Technol...</td>\n",
              "      <td>Density of States and NMR Relaxation Rate in A...</td>\n",
              "      <td>We show that the density of states in an ani...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Wed, 18 Sep 199...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369017</th>\n",
              "      <td>supr-con cond-mat.supr-con</td>\n",
              "      <td>supr-con/9609004</td>\n",
              "      <td>Naoki Enomoto, Masanori Ichioka and Kazushige ...</td>\n",
              "      <td>Ginzburg Landau theory for d-wave pairing and ...</td>\n",
              "      <td>The Ginzburg Landau theory for d_{x^2-y^2}-w...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Wed, 25 Sep 199...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>369018 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                categories                id  \\\n",
              "0                        cond-mat.mes-hall         0704.0006   \n",
              "1                        cond-mat.mtrl-sci         0704.0008   \n",
              "2       cond-mat.str-el cond-mat.stat-mech         0704.0025   \n",
              "3                        cond-mat.mes-hall         0704.0027   \n",
              "4                          cond-mat.str-el         0704.0030   \n",
              "...                                    ...               ...   \n",
              "369013          supr-con cond-mat.supr-con  supr-con/9608008   \n",
              "369014          supr-con cond-mat.supr-con  supr-con/9609001   \n",
              "369015          supr-con cond-mat.supr-con  supr-con/9609002   \n",
              "369016          supr-con cond-mat.supr-con  supr-con/9609003   \n",
              "369017          supr-con cond-mat.supr-con  supr-con/9609004   \n",
              "\n",
              "                                                  authors  \\\n",
              "0                                Y. H. Pong and C. K. Law   \n",
              "1                                         Damian C. Swift   \n",
              "2       A. S. Mishchenko (1 and 2) and N. Nagaosa (1 a...   \n",
              "3       M. O. Goerbig, J.-N. Fuchs, K. Kechedzhi, Vlad...   \n",
              "4                            J.P.Hague and N.d'Ambrumenil   \n",
              "...                                                   ...   \n",
              "369013  R. Prozorov, M. Konczykowski, B. Schmidt, Y. Y...   \n",
              "369014  Durga P. Choudhury, Balam A. Willemsen, John S...   \n",
              "369015  Balam A. Willemsen, J. S. Derov and S.Sridhar ...   \n",
              "369016  Yasumasa Hasegawa (Himeji Institute of Technol...   \n",
              "369017  Naoki Enomoto, Masanori Ichioka and Kazushige ...   \n",
              "\n",
              "                                                    title  \\\n",
              "0       Bosonic characters of atomic Cooper pairs acro...   \n",
              "1       Numerical solution of shock and ramp compressi...   \n",
              "2       Spectroscopic Properties of Polarons in Strong...   \n",
              "3       Filling-Factor-Dependent Magnetophonon Resonan...   \n",
              "4       Tuning correlation effects with electron-phono...   \n",
              "...                                                   ...   \n",
              "369013  On the origin of the irreversibility line in t...   \n",
              "369014  Nonlinear Response of HTSC Thin Film Microwave...   \n",
              "369015  Critical State Flux Penetration and Linear Mic...   \n",
              "369016  Density of States and NMR Relaxation Rate in A...   \n",
              "369017  Ginzburg Landau theory for d-wave pairing and ...   \n",
              "\n",
              "                                                 abstract  \\\n",
              "0         We study the two-particle wave function of p...   \n",
              "1         A general formulation was developed to repre...   \n",
              "2         We present recent advances in understanding ...   \n",
              "3         We describe a peculiar fine structure acquir...   \n",
              "4         We investigate the effect of tuning the phon...   \n",
              "...                                                   ...   \n",
              "369013    We report on measurements of the angular dep...   \n",
              "369014    The non-linear microwave surface impedance o...   \n",
              "369015    The vortex contribution to the dc field (H) ...   \n",
              "369016    We show that the density of states in an ani...   \n",
              "369017    The Ginzburg Landau theory for d_{x^2-y^2}-w...   \n",
              "\n",
              "                                                 versions  \n",
              "0       [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  \n",
              "1       [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  \n",
              "2       [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  \n",
              "3       [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  \n",
              "4       [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  \n",
              "...                                                   ...  \n",
              "369013  [{'version': 'v1', 'created': 'Mon, 26 Aug 199...  \n",
              "369014  [{'version': 'v1', 'created': 'Sat, 31 Aug 199...  \n",
              "369015  [{'version': 'v1', 'created': 'Tue, 3 Sep 1996...  \n",
              "369016  [{'version': 'v1', 'created': 'Wed, 18 Sep 199...  \n",
              "369017  [{'version': 'v1', 'created': 'Wed, 25 Sep 199...  \n",
              "\n",
              "[369018 rows x 6 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 500)\n",
        "\n",
        "import regex\n",
        "\n",
        "cond_mat = pd.read_csv('cond_mat.csv', low_memory=False)\n",
        "cond_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa5985fd-4772-4cb3-abc8-8578f126dc03",
      "metadata": {
        "id": "fa5985fd-4772-4cb3-abc8-8578f126dc03"
      },
      "source": [
        "We will analyse the data by time, grouping together articles that were published in the same time period, with different levels of fine-graining -- weeks, months and semesters (half-years). We can add additional columns to the dataframe corresponding to the amount of time elapsed since the publication of the earliest article in cond-mat in March 1992."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e32c8c28-a34f-430f-9514-7253e01defe7",
      "metadata": {
        "id": "e32c8c28-a34f-430f-9514-7253e01defe7",
        "outputId": "5d4a4ac2-0755-4075-cffa-240b37180d7c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>categories</th>\n",
              "      <th>id</th>\n",
              "      <th>authors</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>versions</th>\n",
              "      <th>semesters_since_March_1992</th>\n",
              "      <th>weeks_since_March_1992</th>\n",
              "      <th>months_since_March_1992</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cond-mat.mes-hall</td>\n",
              "      <td>0704.0006</td>\n",
              "      <td>Y. H. Pong and C. K. Law</td>\n",
              "      <td>Bosonic characters of atomic Cooper pairs acro...</td>\n",
              "      <td>We study the two-particle wave function of p...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
              "      <td>30</td>\n",
              "      <td>786</td>\n",
              "      <td>180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cond-mat.mtrl-sci</td>\n",
              "      <td>0704.0008</td>\n",
              "      <td>Damian C. Swift</td>\n",
              "      <td>Numerical solution of shock and ramp compressi...</td>\n",
              "      <td>A general formulation was developed to repre...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
              "      <td>30</td>\n",
              "      <td>786</td>\n",
              "      <td>180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cond-mat.str-el cond-mat.stat-mech</td>\n",
              "      <td>0704.0025</td>\n",
              "      <td>A. S. Mishchenko (1 and 2) and N. Nagaosa (1 a...</td>\n",
              "      <td>Spectroscopic Properties of Polarons in Strong...</td>\n",
              "      <td>We present recent advances in understanding ...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
              "      <td>30</td>\n",
              "      <td>787</td>\n",
              "      <td>181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cond-mat.mes-hall</td>\n",
              "      <td>0704.0027</td>\n",
              "      <td>M. O. Goerbig, J.-N. Fuchs, K. Kechedzhi, Vlad...</td>\n",
              "      <td>Filling-Factor-Dependent Magnetophonon Resonan...</td>\n",
              "      <td>We describe a peculiar fine structure acquir...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
              "      <td>30</td>\n",
              "      <td>787</td>\n",
              "      <td>181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>cond-mat.str-el</td>\n",
              "      <td>0704.0030</td>\n",
              "      <td>J.P.Hague and N.d'Ambrumenil</td>\n",
              "      <td>Tuning correlation effects with electron-phono...</td>\n",
              "      <td>We investigate the effect of tuning the phon...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
              "      <td>30</td>\n",
              "      <td>786</td>\n",
              "      <td>180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369013</th>\n",
              "      <td>supr-con cond-mat.supr-con</td>\n",
              "      <td>supr-con/9608008</td>\n",
              "      <td>R. Prozorov, M. Konczykowski, B. Schmidt, Y. Y...</td>\n",
              "      <td>On the origin of the irreversibility line in t...</td>\n",
              "      <td>We report on measurements of the angular dep...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Mon, 26 Aug 199...</td>\n",
              "      <td>9</td>\n",
              "      <td>234</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369014</th>\n",
              "      <td>supr-con cond-mat.supr-con</td>\n",
              "      <td>supr-con/9609001</td>\n",
              "      <td>Durga P. Choudhury, Balam A. Willemsen, John S...</td>\n",
              "      <td>Nonlinear Response of HTSC Thin Film Microwave...</td>\n",
              "      <td>The non-linear microwave surface impedance o...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Sat, 31 Aug 199...</td>\n",
              "      <td>9</td>\n",
              "      <td>234</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369015</th>\n",
              "      <td>supr-con cond-mat.supr-con</td>\n",
              "      <td>supr-con/9609002</td>\n",
              "      <td>Balam A. Willemsen, J. S. Derov and S.Sridhar ...</td>\n",
              "      <td>Critical State Flux Penetration and Linear Mic...</td>\n",
              "      <td>The vortex contribution to the dc field (H) ...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Tue, 3 Sep 1996...</td>\n",
              "      <td>9</td>\n",
              "      <td>235</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369016</th>\n",
              "      <td>supr-con cond-mat.supr-con</td>\n",
              "      <td>supr-con/9609003</td>\n",
              "      <td>Yasumasa Hasegawa (Himeji Institute of Technol...</td>\n",
              "      <td>Density of States and NMR Relaxation Rate in A...</td>\n",
              "      <td>We show that the density of states in an ani...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Wed, 18 Sep 199...</td>\n",
              "      <td>9</td>\n",
              "      <td>237</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369017</th>\n",
              "      <td>supr-con cond-mat.supr-con</td>\n",
              "      <td>supr-con/9609004</td>\n",
              "      <td>Naoki Enomoto, Masanori Ichioka and Kazushige ...</td>\n",
              "      <td>Ginzburg Landau theory for d-wave pairing and ...</td>\n",
              "      <td>The Ginzburg Landau theory for d_{x^2-y^2}-w...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Wed, 25 Sep 199...</td>\n",
              "      <td>9</td>\n",
              "      <td>238</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>369018 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                categories                id  \\\n",
              "0                        cond-mat.mes-hall         0704.0006   \n",
              "1                        cond-mat.mtrl-sci         0704.0008   \n",
              "2       cond-mat.str-el cond-mat.stat-mech         0704.0025   \n",
              "3                        cond-mat.mes-hall         0704.0027   \n",
              "4                          cond-mat.str-el         0704.0030   \n",
              "...                                    ...               ...   \n",
              "369013          supr-con cond-mat.supr-con  supr-con/9608008   \n",
              "369014          supr-con cond-mat.supr-con  supr-con/9609001   \n",
              "369015          supr-con cond-mat.supr-con  supr-con/9609002   \n",
              "369016          supr-con cond-mat.supr-con  supr-con/9609003   \n",
              "369017          supr-con cond-mat.supr-con  supr-con/9609004   \n",
              "\n",
              "                                                  authors  \\\n",
              "0                                Y. H. Pong and C. K. Law   \n",
              "1                                         Damian C. Swift   \n",
              "2       A. S. Mishchenko (1 and 2) and N. Nagaosa (1 a...   \n",
              "3       M. O. Goerbig, J.-N. Fuchs, K. Kechedzhi, Vlad...   \n",
              "4                            J.P.Hague and N.d'Ambrumenil   \n",
              "...                                                   ...   \n",
              "369013  R. Prozorov, M. Konczykowski, B. Schmidt, Y. Y...   \n",
              "369014  Durga P. Choudhury, Balam A. Willemsen, John S...   \n",
              "369015  Balam A. Willemsen, J. S. Derov and S.Sridhar ...   \n",
              "369016  Yasumasa Hasegawa (Himeji Institute of Technol...   \n",
              "369017  Naoki Enomoto, Masanori Ichioka and Kazushige ...   \n",
              "\n",
              "                                                    title  \\\n",
              "0       Bosonic characters of atomic Cooper pairs acro...   \n",
              "1       Numerical solution of shock and ramp compressi...   \n",
              "2       Spectroscopic Properties of Polarons in Strong...   \n",
              "3       Filling-Factor-Dependent Magnetophonon Resonan...   \n",
              "4       Tuning correlation effects with electron-phono...   \n",
              "...                                                   ...   \n",
              "369013  On the origin of the irreversibility line in t...   \n",
              "369014  Nonlinear Response of HTSC Thin Film Microwave...   \n",
              "369015  Critical State Flux Penetration and Linear Mic...   \n",
              "369016  Density of States and NMR Relaxation Rate in A...   \n",
              "369017  Ginzburg Landau theory for d-wave pairing and ...   \n",
              "\n",
              "                                                 abstract  \\\n",
              "0         We study the two-particle wave function of p...   \n",
              "1         A general formulation was developed to repre...   \n",
              "2         We present recent advances in understanding ...   \n",
              "3         We describe a peculiar fine structure acquir...   \n",
              "4         We investigate the effect of tuning the phon...   \n",
              "...                                                   ...   \n",
              "369013    We report on measurements of the angular dep...   \n",
              "369014    The non-linear microwave surface impedance o...   \n",
              "369015    The vortex contribution to the dc field (H) ...   \n",
              "369016    We show that the density of states in an ani...   \n",
              "369017    The Ginzburg Landau theory for d_{x^2-y^2}-w...   \n",
              "\n",
              "                                                 versions  \\\n",
              "0       [{'version': 'v1', 'created': 'Sat, 31 Mar 200...   \n",
              "1       [{'version': 'v1', 'created': 'Sat, 31 Mar 200...   \n",
              "2       [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...   \n",
              "3       [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...   \n",
              "4       [{'version': 'v1', 'created': 'Sat, 31 Mar 200...   \n",
              "...                                                   ...   \n",
              "369013  [{'version': 'v1', 'created': 'Mon, 26 Aug 199...   \n",
              "369014  [{'version': 'v1', 'created': 'Sat, 31 Aug 199...   \n",
              "369015  [{'version': 'v1', 'created': 'Tue, 3 Sep 1996...   \n",
              "369016  [{'version': 'v1', 'created': 'Wed, 18 Sep 199...   \n",
              "369017  [{'version': 'v1', 'created': 'Wed, 25 Sep 199...   \n",
              "\n",
              "        semesters_since_March_1992  weeks_since_March_1992  \\\n",
              "0                               30                     786   \n",
              "1                               30                     786   \n",
              "2                               30                     787   \n",
              "3                               30                     787   \n",
              "4                               30                     786   \n",
              "...                            ...                     ...   \n",
              "369013                           9                     234   \n",
              "369014                           9                     234   \n",
              "369015                           9                     235   \n",
              "369016                           9                     237   \n",
              "369017                           9                     238   \n",
              "\n",
              "        months_since_March_1992  \n",
              "0                           180  \n",
              "1                           180  \n",
              "2                           181  \n",
              "3                           181  \n",
              "4                           180  \n",
              "...                         ...  \n",
              "369013                       53  \n",
              "369014                       53  \n",
              "369015                       54  \n",
              "369016                       54  \n",
              "369017                       54  \n",
              "\n",
              "[369018 rows x 9 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import ast\n",
        "\n",
        "format_date = \"%a, %d %b %Y %H:%M:%S %Z\"\n",
        "March_1992 = datetime(1992, 3, 1)\n",
        "\n",
        "def get_created_date(versions):\n",
        "    versions = ast.literal_eval(versions)\n",
        "    created_date = datetime.strptime(versions[0]['created'], format_date)\n",
        "    return created_date\n",
        "\n",
        "def count_semesters(versions):\n",
        "    created_date = get_created_date(versions)\n",
        "    return (created_date.year - 1992)*2 + created_date.month//6\n",
        "\n",
        "def count_months(versions):\n",
        "    created_date = get_created_date(versions)\n",
        "    return (created_date.year - 1992)*12 + created_date.month-3\n",
        "\n",
        "def count_weeks(versions):\n",
        "    created_date = get_created_date(versions)\n",
        "    return (created_date-March_1992).days//7\n",
        "\n",
        "cond_mat['semesters_since_March_1992'] = cond_mat['versions'].apply(count_semesters)\n",
        "cond_mat['weeks_since_March_1992'] = cond_mat['versions'].apply(count_weeks)\n",
        "cond_mat['months_since_March_1992'] = cond_mat['versions'].apply(count_months)\n",
        "cond_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c3a16a7-147e-41ae-a72f-ac7506258a42",
      "metadata": {
        "id": "7c3a16a7-147e-41ae-a72f-ac7506258a42"
      },
      "source": [
        "We can check if any weeks or months are missing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef129a23-0461-4f20-ab3d-93afa18b3799",
      "metadata": {
        "id": "ef129a23-0461-4f20-ab3d-93afa18b3799",
        "outputId": "8df66bc0-ed25-4f59-f24f-d08d76fac257"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "1676"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(min(cond_mat['weeks_since_March_1992']))\n",
        "display(max(cond_mat['weeks_since_March_1992']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48cd2ec6-c646-4414-85dc-f3d924566737",
      "metadata": {
        "id": "48cd2ec6-c646-4414-85dc-f3d924566737",
        "outputId": "0e886099-afe5-48be-e609-113d1ff8ad7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[4]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display([week for week in range(3,1677) if week not in cond_mat['weeks_since_March_1992'].unique()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f0ba613-d3b3-421a-ba46-f0310360e4cc",
      "metadata": {
        "id": "2f0ba613-d3b3-421a-ba46-f0310360e4cc",
        "outputId": "906225cf-6615-4d38-cab6-82a090f2a7a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "385"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(min(cond_mat['months_since_March_1992']))\n",
        "display(max(cond_mat['months_since_March_1992']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "808c9aa5-3b58-468a-8eee-c41cafcaa2de",
      "metadata": {
        "id": "808c9aa5-3b58-468a-8eee-c41cafcaa2de",
        "outputId": "bae38b79-f6eb-4545-cdcb-d8838f0d04d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display([month for month in range(386) if month not in cond_mat['months_since_March_1992'].unique()])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4649dfe7-214d-4485-9160-e1da621a8ba7",
      "metadata": {
        "id": "4649dfe7-214d-4485-9160-e1da621a8ba7"
      },
      "source": [
        "There was one week in March 1992 where no published papers appeared in the cond-mat.\n",
        "\n",
        "Let's look at the monthly publication numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1824d5c-c634-40dc-9244-3dd438e46710",
      "metadata": {
        "id": "b1824d5c-c634-40dc-9244-3dd438e46710",
        "outputId": "89ba5cd6-f9df-4caf-bf23-ec2801878527"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "monthly_publications = [len(cond_mat[cond_mat['months_since_March_1992']==month]) for month in range(0,386)]\n",
        "\n",
        "fig,ax = plt.subplots(1,1, figsize=(18, 6))\n",
        "ax.set_title('Monthly publications in cond-mat')\n",
        "ax.set_xlabel('months since March 1992')\n",
        "ax.set_ylabel('publications')\n",
        "\n",
        "_ = sns.lineplot(x=range(len(monthly_publications)), y=monthly_publications, ax=ax, linewidth=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4966367-9fe7-495c-9649-e99e5eb9f7d4",
      "metadata": {
        "id": "c4966367-9fe7-495c-9649-e99e5eb9f7d4"
      },
      "source": [
        "The data for the last month is incomplete, so we can drop it from the dataframe. We see a roughly linear growth in the number of publications since the first cond-mat publication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3410f795-ae18-4d92-b324-397b0a3b7c29",
      "metadata": {
        "id": "3410f795-ae18-4d92-b324-397b0a3b7c29"
      },
      "outputs": [],
      "source": [
        "cond_mat.drop(cond_mat[cond_mat['months_since_March_1992']==385].index, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003efe22-cc3c-4b52-974d-dac44f8af715",
      "metadata": {
        "id": "003efe22-cc3c-4b52-974d-dac44f8af715",
        "outputId": "3fbbf1a1-61c5-43c4-ae4f-beb38d69bf5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Growth rate: +4.7197 papers per month\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import linregress\n",
        "m, b, r_value, p_value, std = linregress(range(385), monthly_publications[:385])\n",
        "print(f\"Growth rate: +{m:.4f} papers per month\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55544620-4ff0-4b96-a585-26c9e9a74905",
      "metadata": {
        "id": "55544620-4ff0-4b96-a585-26c9e9a74905",
        "outputId": "80d57b68-f381-4f2c-b54e-af9cfeae0230"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig,ax = plt.subplots(1,1, figsize=(18, 6))\n",
        "ax.set_title('Monthly publications in cond-mat')\n",
        "ax.set_xlabel('months since March 1992')\n",
        "ax.set_ylabel('publications')\n",
        "\n",
        "_ = sns.lineplot(x=range(385), y=monthly_publications[:385], ax=ax, linewidth=1)\n",
        "_ = sns.lineplot(x=range(385), y=[m*x+b for x in range(385)], ax=ax, linewidth=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a43fe7bb-0978-4970-ac63-ab18d6364021",
      "metadata": {
        "id": "a43fe7bb-0978-4970-ac63-ab18d6364021"
      },
      "source": [
        "Next, let's look at the weekly publication numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4980c220-88fa-40fc-9828-f98f455c9e02",
      "metadata": {
        "id": "4980c220-88fa-40fc-9828-f98f455c9e02",
        "outputId": "a2bd560b-c2a5-43ab-83f6-470aa1949354"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "weekly_publications = [len(cond_mat[cond_mat['weeks_since_March_1992']==week]) for week in range(3,1676)]\n",
        "\n",
        "fig,ax = plt.subplots(1,1, figsize=(18, 6))\n",
        "ax.set_title('Weekly publications in cond-mat')\n",
        "ax.set_xlabel('weeks since March 1992')\n",
        "ax.set_ylabel('publications')\n",
        "\n",
        "_ = sns.lineplot(x=range(len(weekly_publications)), y=weekly_publications, ax=ax, linewidth=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88abb750-813b-4f0b-b7e4-8cdc9220eb10",
      "metadata": {
        "id": "88abb750-813b-4f0b-b7e4-8cdc9220eb10"
      },
      "source": [
        "If the arxiv database is accurate, it seems like there is a regular pattern of sharp dips and spikes in the number of publications over time, which is quite an interesting phenomenon. However, for the purpose of analysing trends, we will be using the monthly data as it is much smoother."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a4d346a-1339-440b-91c5-0e6317e1f4f1",
      "metadata": {
        "id": "6a4d346a-1339-440b-91c5-0e6317e1f4f1"
      },
      "source": [
        "## Natural language processing\n",
        "\n",
        "### Generating the vocabulary\n",
        "\n",
        "We will now create a vocabulary of keywords from the titles and abstracts. I'll use NER (Named Entity Recognition) from the transformers library to identify special words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "955e0d1e-28c1-45fe-9264-5cd341e131f2",
      "metadata": {
        "id": "955e0d1e-28c1-45fe-9264-5cd341e131f2",
        "outputId": "bfa92476-d507-4b89-8693-40733159ca61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tommy/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2024-05-16 01:01:47.232641: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-05-16 01:01:47.350769: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-05-16 01:01:47.733558: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-16 01:01:49.168498: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "All PyTorch model weights were used when initializing TFDistilBertForTokenClassification.\n",
            "\n",
            "All the weights of TFDistilBertForTokenClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForTokenClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\")\n",
        "model = TFAutoModelForTokenClassification.from_pretrained(\"dslim/distilbert-NER\")\n",
        "\n",
        "ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a430dd4-5f24-484a-b39b-b7958ebdd5af",
      "metadata": {
        "id": "7a430dd4-5f24-484a-b39b-b7958ebdd5af"
      },
      "source": [
        "Note that NER is case sensitive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d7eae53-9040-4455-8173-0c2766dd1fb5",
      "metadata": {
        "id": "1d7eae53-9040-4455-8173-0c2766dd1fb5",
        "outputId": "a129e3b2-7d37-4d2b-8ebc-e5fafd972074"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity': 'LABEL_0',\n",
              "  'score': 0.9549229,\n",
              "  'index': 1,\n",
              "  'word': 'pet',\n",
              "  'start': 0,\n",
              "  'end': 3},\n",
              " {'entity': 'LABEL_0',\n",
              "  'score': 0.98883796,\n",
              "  'index': 2,\n",
              "  'word': '##er',\n",
              "  'start': 3,\n",
              "  'end': 5}]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ner('peter')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "616e6026-3eb4-4833-bf56-c681f35d1e14",
      "metadata": {
        "id": "616e6026-3eb4-4833-bf56-c681f35d1e14",
        "outputId": "80eeb828-6ea4-4f96-d026-610c46e5d803"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity': 'LABEL_1',\n",
              "  'score': 0.98770624,\n",
              "  'index': 1,\n",
              "  'word': 'Peter',\n",
              "  'start': 0,\n",
              "  'end': 5}]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ner('Peter')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f95b73d0-b87b-4d65-a361-f2a2985fae17",
      "metadata": {
        "id": "f95b73d0-b87b-4d65-a361-f2a2985fae17"
      },
      "source": [
        "Next, we'll tokenise the titles and abstracts, removing punctuation and numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5df6c0f-263d-4e49-8741-ae6b68cd14c7",
      "metadata": {
        "id": "c5df6c0f-263d-4e49-8741-ae6b68cd14c7"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"Processing rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "738cf3ec-3a18-481d-b7da-0490dd01278f",
      "metadata": {
        "id": "738cf3ec-3a18-481d-b7da-0490dd01278f",
        "outputId": "a4ad7b3a-7af7-4450-d06a-9a7f52032119"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing rows: 100%|███████████████| 368030/368030 [00:04<00:00, 80875.81it/s]\n",
            "Processing rows: 100%|███████████████| 368030/368030 [00:26<00:00, 13951.93it/s]\n"
          ]
        }
      ],
      "source": [
        "import regex\n",
        "import string\n",
        "\n",
        "def tokenize(text):\n",
        "    pattern = r'[\\s{}]+'.format(regex.escape(string.punctuation))\n",
        "    tokens = regex.split(pattern, text)\n",
        "    return [token for token in tokens if token and not token.isdigit()]\n",
        "\n",
        "cond_mat['title_tokenized'] = cond_mat['title'].progress_apply(tokenize)\n",
        "cond_mat['abstract_tokenized'] = cond_mat['abstract'].progress_apply(tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f713f2-c380-4966-896d-60e909092c92",
      "metadata": {
        "id": "a3f713f2-c380-4966-896d-60e909092c92",
        "outputId": "4305c35e-913b-4b80-cbfc-78c083c1fd32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'test', 'Potato']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenize('This! is! a! test! Potato!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b4a523c-45a6-4718-823d-d7c9059dbe3c",
      "metadata": {
        "id": "2b4a523c-45a6-4718-823d-d7c9059dbe3c"
      },
      "source": [
        "Next, we'll generate a vocabulary using scikit-learn's TfidfVectorizer. This is a powerful tool that performs a count of each token in a text sample, as well as a count of the token in the entire corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "985ddba7-ff8b-4482-9b04-9d4356fa87ce",
      "metadata": {
        "id": "985ddba7-ff8b-4482-9b04-9d4356fa87ce"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5082131-2504-406c-b06d-2dd6212ff18f",
      "metadata": {
        "id": "d5082131-2504-406c-b06d-2dd6212ff18f",
        "outputId": "d71d072f-5031-4381-c836-3bc299f75daf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created count vectorizer with vocabulary size =  191152\n",
            "CPU times: user 10.6 s, sys: 670 ms, total: 11.3 s\n",
            "Wall time: 11.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "count_vectorizer = CountVectorizer(preprocessor= lambda x:x, tokenizer=lambda x:x, binary=True, token_pattern=None, lowercase=False)\n",
        "vectorizer_train = pd.concat([cond_mat['abstract_tokenized'], cond_mat['title_tokenized']])\n",
        "vectorized = count_vectorizer.fit_transform(vectorizer_train)\n",
        "document_frequencies =  np.ravel(vectorized.sum(axis=0))\n",
        "\n",
        "vocabulary = count_vectorizer.get_feature_names_out()\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "print(\"Created count vectorizer with vocabulary size = \", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a32c5ee2-cd5a-44a1-9573-f21b00e4b2c2",
      "metadata": {
        "id": "a32c5ee2-cd5a-44a1-9573-f21b00e4b2c2"
      },
      "source": [
        "The vocabulary will contain variations of the same word, e.g. \"quantize\" and \"quantization\", \"magnetic,\" \"magnetism\", \"magnetize\", \"magnetization,\" etc. In order to identify the keywords, we will have to first merge these variations. I'll do this by introducing a function that removes suffixes from words and then matches their stems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a7effae-34fb-4a8c-91e7-51c6d7445ba7",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "4a7effae-34fb-4a8c-91e7-51c6d7445ba7"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def word_similarity(v, w):\n",
        "    if v == w:\n",
        "        return True\n",
        "    v = stemmer.stem(v)\n",
        "    w = stemmer.stem(w)\n",
        "    L = min(len(v),len(w))\n",
        "    if L < 5:\n",
        "        return False\n",
        "    return v[:L] == w[:L]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "661c4f29-a27d-4e18-b207-66d9fdf2f416",
      "metadata": {
        "id": "661c4f29-a27d-4e18-b207-66d9fdf2f416",
        "outputId": "253f17d3-3a29-4a24-a526-217c01a4f9ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_similarity('superconducting', 'superconductor')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29b4bd88-4728-48d6-92a1-382165b1880e",
      "metadata": {
        "id": "29b4bd88-4728-48d6-92a1-382165b1880e",
        "outputId": "1b6f0d1e-41b8-4662-fb8c-12ad84543d9c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_similarity('quantized', 'quantization')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "378060f5-dfa5-4f5d-9a9f-b5e6f5d7e540",
      "metadata": {
        "id": "378060f5-dfa5-4f5d-9a9f-b5e6f5d7e540",
        "outputId": "e1d35e1e-b77c-4668-d813-a6d533dde103"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_similarity('magnetism', 'magnetization')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ab9177f-3642-4b4c-ae95-0817f112304b",
      "metadata": {
        "id": "7ab9177f-3642-4b4c-ae95-0817f112304b",
        "outputId": "682dd4f4-25f5-4b9d-ca9c-2beef598ce2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_similarity('learning', 'leading')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d4faaf6-dee5-417e-8864-53d574efb75b",
      "metadata": {
        "id": "2d4faaf6-dee5-417e-8864-53d574efb75b"
      },
      "source": [
        "We'll create a \"base vocabulary\" consisting of words which have had their variations merged:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc814ec4-68aa-4a14-bb34-5411ec8f925d",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "bc814ec4-68aa-4a14-bb34-5411ec8f925d"
      },
      "outputs": [],
      "source": [
        "stop_words = ['the', 'of', 'and', 'in', 'a', 'to', 'we', 'with', 'is', 'that', 'for', 'by', 'this', 'on', 'are', 'as', 'an', 'which', 'from', 'be', 'at', 'can', 'our', 'where']\n",
        "\n",
        "def create_base_vocabulary(vocabulary):\n",
        "    base_vocabulary_unnamed = []\n",
        "    vocabulary_named = []\n",
        "    base_word_dict = {}\n",
        "\n",
        "    for word in tqdm(vocabulary):\n",
        "        if word.lower() in stop_words:\n",
        "            continue\n",
        "\n",
        "        if word in base_vocabulary_unnamed or word in vocabulary_named:\n",
        "            continue\n",
        "\n",
        "        items = ner(word)\n",
        "        if items[0]['entity'] != 'LABEL_0':\n",
        "            vocabulary_named.append(word)\n",
        "            continue\n",
        "\n",
        "        found_match = False\n",
        "        word = word.lower()\n",
        "        for base_word in base_vocabulary_unnamed:\n",
        "            if word_similarity(word, base_word):\n",
        "                base_word_dict[word] = base_word\n",
        "                found_match = True\n",
        "        if not found_match:\n",
        "            base_vocabulary_unnamed.append(word)\n",
        "\n",
        "    return base_vocabulary_unnamed, vocabulary_named, base_word_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d2bdb4f-8953-4c07-820e-4d2a56ad2fe4",
      "metadata": {
        "id": "6d2bdb4f-8953-4c07-820e-4d2a56ad2fe4"
      },
      "source": [
        "Due to the large size of the vocabulary, creating a dictionary of variations of each word would take hours. We can simply select the most common words, which we can do by ranking the idf (inverse document frequency) scores from the TfidfVectorizer: the lower the idf score, the more common the word:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09c9f868-56d8-4e2d-9d1c-4ba5616a582f",
      "metadata": {
        "id": "09c9f868-56d8-4e2d-9d1c-4ba5616a582f",
        "outputId": "3d2710c3-51b8-401e-854a-2002cd144149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 109 ms, sys: 0 ns, total: 109 ms\n",
            "Wall time: 108 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "word_indices_ranked_by_frequency = np.argsort(document_frequencies)\n",
        "words_ranked_by_frequency = [vocabulary[word_indices_ranked_by_frequency[vocab_size - 1 - i]] for i in range(vocab_size)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "127d3cc9-fb61-48cd-a3bf-fb2dd138b961",
      "metadata": {
        "id": "127d3cc9-fb61-48cd-a3bf-fb2dd138b961",
        "outputId": "4374fd38-a16e-4d78-b98d-32265476e638"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['of', 'in', 'the', 'and', 'a', 'to', 'with', 'for', 'is', 'that', 'by', 'We', 'The', 'on', 'are', 'we', 'an', 'as', 'which', 'at', 'from', 'be', 'this', 'two', 'can', 'model', 'In', 'quantum', 'between', 'field', 'show', 'spin', 'results', 'phase', 'study', 'state', 'temperature', 'energy', 'magnetic', 'This', 'system', 'properties', 'also', 'dimensional', 'systems', 'one', 'states', 'using', 'transition', 'A', 'theory', 'or', 'have', 'non', 'density', 'order', 'these', 'time', 'high', 'has', 'based', 'structure', 'it', 'effect', 'different', 'such', 'both', 'dynamics', 'low', 'well', 'find', 'single', 'electron', 'Our', 'not', 'lattice', 'large', 'its', 'present', 'where', 'their', 'been', 'function', 'range', 'Here', 'coupling', 'into', 'interaction', 'first', 'observed', 'found', 'our', 'due', 'critical', 'experimental', 'effects', 'behavior', 'potential', 'method', 'than']\n"
          ]
        }
      ],
      "source": [
        "print(words_ranked_by_frequency[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53e94f6f-3c94-420d-98e1-291e0aeb7302",
      "metadata": {
        "id": "53e94f6f-3c94-420d-98e1-291e0aeb7302",
        "outputId": "cf8addfb-7936-4d32-a6c8-0a831d5ceb1d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████| 10000/10000 [13:06<00:00, 12.72it/s]\n"
          ]
        }
      ],
      "source": [
        "base_vocabulary_unnamed, vocabulary_named, base_word_dict = create_base_vocabulary(words_ranked_by_frequency[:10000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96b42471-6447-4b13-b72a-6c10e6341b10",
      "metadata": {
        "scrolled": true,
        "id": "96b42471-6447-4b13-b72a-6c10e6341b10",
        "outputId": "b528eca6-853e-4740-ec0e-0ccf1b81cfe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['two', 'model', 'quantum', 'between', 'field', 'show', 'spin', 'results', 'phase', 'study', 'state', 'temperature', 'energy', 'magnetic', 'system', 'properties', 'also', 'dimensional', 'one', 'using', 'transition', 'theory', 'or', 'have', 'non', 'density', 'order', 'these', 'time', 'high', 'has', 'based', 'structure', 'it', 'effect', 'different', 'such', 'both', 'dynamics', 'low', 'well', 'find', 'single', 'electron', 'not', 'lattice', 'large', 'its', 'present', 'their', 'been', 'function', 'range', 'here', 'coupling', 'into', 'interaction', 'first', 'observed', 'found', 'due', 'critical', 'experimental', 'behavior', 'potential', 'method', 'than', 'approach', 'surface', 'new', 'induced', 'but', 's', 'strong', 'when', 'transport', 'materials', 'used', 'demonstrate', 'through', 'only', 'point', 'finite', 'charge', 'simulations', 'other', 'symmetry', 'three', 'obtained', 'wave', 'work', 'number', 'calculations', 'local', 'case', 'all', 'analysis', 'band', 'under', 'dependence', 'small', 'topological', 'investigate', 'current', 'optical', 'long', 'limit', 't', 'within', 'measurements', 'presence', 'like', 'superconducting', 'shown', 'scattering', 'particle', 'many', 'regime', 'zero', 'data', 'size', 'numerical', 'e', 'more', 'up', 'parameters', 'space', 'may', 'out', 'while', 'use', 'gap', 'distribution', 'k', 'particular', 'metal', 'type', 'free', 'theoretical', 'via', 'ground', 'simple', 'level', 'thermal', 'provide', 'linear', 'fluctuations', 'equilibrium', 'x', 'graphene', 'was', 'terms', 'liquid', 'equation', 'however', 'how', 'applied', 'discuss', 'near', 'form', 'correlation', 'self', 'report', 'very', 'random', 'over', 'frequency', 'scale', 'even', 'mean', 'hall', 'bulk', 'crystal', 'possible', 'agreement', 'conditions', 'general', 'driven', 'recent', 'same', 'process', 'value', 'molecular', 'leads', 'some', 'length', 'i', 'including', 'role', 'known', 'response', 'values', 'approximation', 'propose', 'consider', 'body', 'formation', 'allows', 'plane', 'modes', 'shows', 'c', 'paper', 'layer', 'higher', 'atoms', 'mechanism', 'n', 'exact', 'atomic', 'various', 'complex', 'physical', 'ferromagnetic', 'along', 'gas', 'associated', 'important', 'spectrum', 'features', 'corresponding', 'each', 'classical', 'weak', 'compared', 'related', 'thin', 'consistent', 'power', 'external', 'matrix', 'insulator', 'were', 'spectroscopy', 'devices', 'problem', 'exchange', 'evolution', 'light', 'further', 'second', 'strength', 'described', '2d', 'relaxation', 'novel', 'd', 'without', 'nature', 'any', 'rate', 'there', 'several', 'exhibit', 'films', 'disorder', 'boundary', 'they', 'most', 'excitations', 'similar', 'below', 'way', 'diagram', 'phonon', 'group', 'then', 'control', 'no', 'heat', 'antiferromagnetic', 'specific', 'motion', 'quasi', 'o', 'those', 'framework', 'information', 'chain', 'change', 'determined', 'above', 'hole', 'mode', 'law', 'times', 'diffusion', 'doped', 'conductivity', 'edge', 'so', 'addition', 'orbit', 'increasing', 'region', 'principles', 'thus', 'given', 'constant', 'derive', 'matter', 'chemical', 'tunneling', 'solution', 'spatial', 'description', 'phenomena', 'short', 'momentum', 'analytical', 'contrast', 'highly', 'ising', 'close', 'pressure', 'about', 'thermodynamic', 'anisotropy', 'spectra', 'interface', 'larger', 'resonance', 'characterized', 'polarization', 'force', 'independent', 'direction', 'us', 'solid', 'lead', 'significant', 'good', 'normal', 'reveal', 'growth', 'wide', 'entropy', 'account', 'existence', 'set', 'around', 'transfer', 'stable', 'finally', 'b', 'periodic', 'means', 'stability', 'technique', 'flow', 'real', 'much', 'factor', 'double', 'doping', 'suggest', 'developed', 'nonlinear', 'lower', 'noise', 'does', 'network', 'analyze', 'ray', 'p', 'microscopic', 'leading', 'magnitude', 'cases', 'evidence', 'relevant', 'could', 'enhanced', 'universal', 'pair', 'if', 'predicted', 'angle', 'extended', 'fundamental', 'performed', 'conventional', 'open', 'influence', 'domain', 'g', 'findings', 'breaking', 'site', 'ratio', 'previous', 'example', 'them', 'fermions', 'respectively', 'either', 'elastic', 'renormalization', 'full', 'sample', 'moreover', 'statistical', 'anomalous', 'semiconductor', 'identify', 'give', 'geometry', 'distinct', 'will', 'continuous', 'multi', 'film', 'velocity', 'voltage', 'fe', 'mass', 'coherent', 'expected', 'fully', 'efficient', 'strain', 'becomes', 'square', 'j', 'occurs', 'component', 'standard', 'defined', 'during', 'line', 'resolved', 'shape', 'initial', 'whose', 'expansion', 'across', 'probability', 'bands', 'reduced', 'introduce', 'certain', 'defects', 'contact', 'half', 'chiral', 'arbitrary', 'origin', 'few', 'vortex', 'being', 'beyond', 'down', 'oscillations', 'h', 'atom', 'average', 'spins', 'latter', 'glass', 'design', 'dot', 'four', 'probe', 'far', 'phys', 'scheme', 'binding', 'negative', 'intrinsic', 'concentration', 'behaviour', 'key', 'rise', 'dispersion', 'peak', 'maximum', 'fixed', 'total', 'types', 'interplay', 'active', 'fluid', 'stochastic', 'remains', 'flux', 'made', 'infinite', 'towards', 'room', 'condensate', 'after', 'gives', 'v', 'static', 'explore', 'upon', 'entanglement', 'absorption', 'phenomenon', 'parallel', 'impurity', 'detailed', 'susceptibility', 'diffraction', 'amplitude', 'freedom', 'contribution', 'called', 'crossover', 'degree', 'coefficient', 'formalism', 'nm', 'basis', 'collective', 'substrate', 'promising', 'do', 'waves', 'indicate', 'confined', 'pairing', 'decay', 'compounds', 'ab', 'axis', 'laser', 'generated', 'separation', 'interest', 'combination', 'unique', 'term', 'al', 'absence', 'et', 'transverse', 'distance', 'carbon', 'sim', 'exponent', 'intermediate', 'series', 'cell', 'reversal', 'u', 'explain', 'configuration', 'because', 'bilayer', 'volume', 'carrier', 'dots', 'main', 'photon', 'transmission', 'superfluid', 'should', 'si', 'robust', 'alpha', 'bias', 'harmonic', 'sites', 'pairs', 'dielectric', 'algorithm', 'hard', 'step', 'compute', 'kinetic', 'junction', 'formed', 'part', 'rather', 'neutron', 'still', 'monolayer', 'thickness', 'bond', 'cluster', 'm', 'ion', 'focus', 'trivial', 'weakly', 'macroscopic', 'filling', 'uniform', 'resistance', '3d', 'rev', 'off', 'core', 'varying', 'perpendicular', 'resolution', 'less', 'shift', 'perturbation', 'steady', 'achieved', 'splitting', 'discrete', 'quantitative', 'appears', 'scanning', 'stress', 'decreases', 'variety', 'channel', 'pure', 'accurate', 'individual', 'rates', 'gate', 'instability', 'mu', 'r', 'dissipation', 'width', 'ions', 'nearest', 'l', 'able', 'sensitive', 'polymer', 'invariant', 'forces', 'often', 'rich', 'pi', 'tunable', 'van', 'environment', 'oxide', 'emission', 'typical', 'barrier', 'hybrid', 'patterns', 'gauge', 'path', 'crucial', 'attractive', 'among', 'realized', 'future', 'against', 'nonequilibrium', 'review', 'threshold', '1d', 'trapped', 'argue', 'hopping', 'broad', 'silicon', 'position', 'cubic', 'valence', 'whereas', 'gases', 'soft', 'useful', 'picture', 'available', 'fraction', 'connected', 'modified', 'elements', 'fast', 'isotropic', 'showing', 'wall', 'propagation', 'complete', 'grown', 'broken', 'mobility', 'shear', 'water', 'cannot', 'tuning', 'homogeneous', 'emergence', 'sign', 'vector', 'quality', 'expression', 'construct', 'internal', 'ensemble', 'optimal', 'global', 'mn', 'delta', 'switching', 'context', 'realistic', 'impact', 'spontaneous', 'orientation', 'established', 'generic', 'nearly', 'ideal', 'flat', 'enables', 'intensity', 'essential', 'f', 'caused', 'source', 'inside', 'research', 'tight', 'asymmetric', 'signal', 'dipole', 'suppressed', 'examine', 'excellent', 'cavity', 'lowest', 'variation', 'memory', 'estimate', 'special', 'make', 'iron', 'degenerate', 'center', 'stationary', 'tc', 'explicitly', 'rotation', 'sub', 'der', 'equal', 'qubit', 'corrections', 'opposite', 'area', 'mixed', 'heterostructures', 'clear', 'nanoscale', 'sharp', 'interference', 'beam', 'engineering', 'q', 'play', 'loss', 'detection', 'required', 'formula', 'bosonic', 'semi', 'common', 'almost', 'inelastic', 'central', 'deformation', 'neighbor', 'unit', 'inverse', 'ones', 'representation', 'angular', 'learning', 'planar', 'suitable', 'support', 'paramagnetic', 'test', 'cells', 'confirm', 'tuned', 'fact', 'integral', 'gradient', 'repulsive', 'peaks', 'inhomogeneous', 'nano', 'together', 'vortices', 'narrow', 'would', 'yields', 'tool', 'oxygen', 'ev', 'photoemission', 'dft', 'slow', 'hexagonal', 'operator', 'mediated', 'per', 'transformation', 'platform', 'finding', 'heavy', 'beta', 'nuclear', 'namely', 'cuprates', 'frustrated', 'containing', 'takes', 'yet', 'modulation', 'sufficiently', 'loop', 'hand', 'binary', 'family', 'consequence', 'tensor', 'trap', 'organic', 'medium', 'treatment', 'ii', 'although', 'rm', 'makes', 'upper', 'asymptotic', 'triangular', 'sizes', 'edges', 'z', 'involving', 'onset', 'cooling', 'attributed', 'implications', 'granular', 'taken', 'taking', 'equivalent', 'arising', 'microwave', 'ultracold', 'concept', 'unusual', 'prove', 'end', 'mesoscopic', 'minimum', 'longitudinal', 'protected', 'dominant', 'proportional', 'another', 'since', 'literature', 'appropriate', 'electromagnetic', 'bi', 'competition', 'better', 'cold', 'attention', 'theorem', 'illustrate', 'wavelength', 'subject', 'signatures', 'having', 'ring', 'adiabatic', 'composition', 'procedure', 'nontrivial', 'seen', 'hydrogen', 'nu', 'address', 'holes', 'gaps', 'y', 'operators', 'arise', 'radiation', 'employed', 'exotic', 'curvature', 'unconventional', 'vicinity', 'always', 'spherical', 'colloidal', 'reaction', 'screening', 'speed', 'zone', 'widely', 'infrared', 'forms', 'alternative', 'embedded', 'ferroelectric', 'gapped', 'cm', 'extensive', 'nematic', 'necessary', 'produce', 'take', 'hydrodynamic', 'profile', 'partial', 'whether', 'practical', 'alloys', 'lines', 'arises', 'next', 'imaging', 'plays', 'walls', 'especially', 'basic', 'onto', 'opens', 'deposition', 'anti', 'mapping', 'nanoparticles', 'interpretation', 'amorphous', 'proximity', 'scenario', 'sigma', 'gapless', 'place', 'making', 'isolated', 'population', 'rare', 'action', 'percolation', 'quantized', 'usual', 'triplet', 'forming', 'moving', 'insights', 'array', 'solve', 'weight', 'following', 'mixture', 'circuit', 'discovered', 'honeycomb', 'implementation', 'mev', 'extremely', 'top', 'depth', 'reflection', 'variable', 'bath', 'great', 'coexistence', 'aspects', 'instead', 'acoustic', 'epitaxial', 'nanotubes', 'mixing', 'deep', 'diamond', 'enough', 'candidate', 'display', 'improved', 'de', 'side', 'nodes', 'interlayer', 'w', 'challenge', 'damping', 'dilute', 'question', 'version', 'gamma', 'vacuum', 'grain', 'remarkable', 'valid', 'host', 'pm', 'extract', 'ability', 'earlier', 'view', 'governed', 'need', 'dense', 'nanowires', 'third', 'radius', 'accompanied', 'years', 'pulse', 'must', 'temporal', 'curves', 'mainly', 'what', 'transient', 'article', 'conclude', 'ballistic', 'before', 'electrostatic', 'least', 'machine', 'might', 'metastable', 'away', 'capacity', 'unstable', 'quite', 'unitary', 'error', 'constraints', 'rule', 'ionic', 'whole', 'biological', 'flows', 'pronounced', 'turn', 'semimetal', 'ranging', 'vacancy', 'sphere', 'circular', 'laws', 'amount', 'cost', 'neural', 'bf', 'evaluate', 'affect', 'valley', 'solved', 'artificial', 'thermoelectric', 'reproduce', 'canonical', 'diagonalization', 'nucleation', 'torque', 'chaotic', 'filled', 'vibrational', 'background', 'displacement', 'solving', 'hence', 'starting', 'omega', 'nanostructures', 'route', 'precise', 'odd', 'help', 'created', 'lambda', 'conformal', 'itself', 'shell', 'vanishing', 'despite', 'situ', 'map', 'overall', 'bonds', 'major', 'easily', 'sequence', 'agree', 'longer', 'according', 'matrices', 'sum', 'offers', 'successfully', 'quenched', 'neutral', 'fabrication', 'sets', 'dual', 'comprehensive', 'oxides', 'prepared', 'accessible', 'curve', 'synthesis', 'index', 'reach', 'phenomenological', 'mathrm', 'wires', 'se', 'search', 'perfect', 'showed', 'twisted', 'annealing', 'lifetime', 'hysteresis', 'versus', 'magnon', 'wire', 'adsorption', 'likely', 'assisted', 'pulses', 'difficult', 'scalar', 'trapping', 'entire', 'pinning', 'heating', 'easy', 'purely', 'friction', 'best', 'rapidly', 'explanation', 'rules', 'operation', 'regular', 'though', 'wells', 'needed', 'causes', 'growing', 'ba', 'steps', 'melting', 'ga', 'conversion', 'ca', 'perspective', 'works', 'capture', 'giving', 'smooth', 'saturation', 'pt', 'strategy', 'boron', 'arguments', 'electrodes', 'setup', 'giant', 'decoherence', 'dimer', 'intriguing', 'nitride', 'already', 'just', 'technology', 'nodal', 'tetragonal', 'te', 'choice', 'manipulation', 'knowledge', 'parity', 'input', 'turns', 'analogous', 'dna', 'former', 'quadratic', 'textit', 'distortion', 'pseudogap', 'plasma', 'back', 'slightly', 'earth', 'plasmon', 'deviation', 'pumping', 'uniaxial', 'ansatz', 'itinerant', 'eigenstates', 'moderate', 'height', 'science', 'ultrafast', 'viscosity', 'helical', 'ti', 'ambient', 'collapse', 'assumed', 'pump', 'gain', 'content', 'trajectories', 'now', 'li', 'situation', 'cause', 'link', 'motivated', 'logarithmic', 'spintronics', 'located', 'fold', 'highest', 'quantify', 'art', 'excess', 'atomistic', 'tested', 'faster', 'bonding', 'reference', 'coarse', 'photoluminescence', 'manner', 'floquet', 'conservation', 'compact', 'mathematical', 'solvable', 'events', 'lack', 'dc', 'lateral', 'membrane', 'assembly', 'loops', 'selective', 'substantial', 'partition', 'diameter', 'incommensurate', 'rho', 'master', 'gross', 'empirical', 'substitution', 'opening', 'storage', 'balance', 'flexible', 'issue', 'largely', 'heterogeneous', 'tension', 'sensing', 'tau', 'negligible', 'landscape', 'progress', 'highlight', 'clean', 'building', 'fine', 'protocol', 'polycrystalline', 'advantage', 'fractal', 'possess', 'vertex', 'solitons', 'morphology', 'ladder', 'serve', 'ac', 'infty', 'phi', 'traditional', 'semiclassical', 'right', 'rigid', 'forward', 'tools', 'kind', 'jump', 'nonzero', 'fourier', 'stripe', 'epsilon', 'angles', 'protein', 'true', 'vapor', 'visible', 'unified', 'classification', 'cr', 'dark', 'dichalcogenides', 'tune', 'dephasing', 'briefly', 'absent', 'peculiar', 'na', 'compatible', 'done', 'nitrogen', 'varies', 'transistors', 'hidden', 'capable', 'rings', 'overlap', 'sqrt', 'synthetic', 'convergence', 'parts', 'paths', 'imaginary', 'injection', 'theta', 'intra', 'nb', 'five', 'net', 'kagome', 'stacking', 'left', 'last', 'image', 'restricted', 'co', 'bending', 'tip', 'walk', 'proof', 'hot', 'occupation', 'reliable', 'grows', 'monotonic', 'chosen', 'compression', 'elucidate', 'ways', 'eigenvalues', 'sound', 'verified', 'world', 'move', 'lying', 'stiffness', 'idea', 'reconstruction', 'alignment', 'gates', 'built', 'vs', 'ordinary', 'agrees', 'black', 'rigorous', 'images', 'zigzag', 'largest', 'block', 'six', 'translational', 'mathbb', 'target', 'superlattices', 'holds', 'decomposition', 'residual', 'linked', 'iii', 'singularity', 'sense', 'chemistry', 'extent', 'penetration', 'acting', 'text', 'cycle', 'varied', 'chaos', 'powder', 'distinguish', 'inherent', 'he', 'irradiation', 'fidelity', 'apparent', 'solvent', 'objects', 'nonlocal', 'gravity', 'dramatically', 'fit', 'copper', 'criterion', 'persistent', 'droplets', 'sheet', 'subsequent', 'orthorhombic', 'pulsed', 'adjacent', 'skyrmion', 're', 'gold', 'purpose', 'sn', 'mutual', 'viscous', 'collisions', 'solar', 'output', 'indeed', 'kappa', 'vary', 'maps', 'bandgap', 'radial', 'ag', 'variance', 'act', 'sb', 'parent', 'indirect', 'recombination', 'overcome', 'massless', 'past', 'graphite', 'stage', 'optoelectronic', 'operating', 'replica', 'surrounding', 'biased', 'walks', 'node', 'fitting', 'pb', 'leq', 'plateau', 'paradigm', 'clarify', 'packing', 'diverse', 'flip', 'hold', 'incoherent', 'comment', 'unexpected', 'collinear', 'code', 'counterpart', 'propto', 'commensurate', 'kinds', 'roughness', 'massive', 'dislocation', 'atomically', 'suspensions', 'desired', 'cylindrical', 'units', 'contrary', 'relies', 'vertical', 'primary', 'uses', 'too', 'advanced', 'plastic', 'etc', 'algebraic', 'sublattice', 'incident', 'areas', 'setting', 'microstructure', 'utilizing', 'orthogonal', 'bottom', 'exceeds', 'sized', 'ice', 'ranged', 'interval', 'aim', 'mapped', 'conjecture', 'striking', 'why', 'decades', 'issues', 'traps', 'owing', 'little', 'divergence', 'emph', 'adsorbed', 'terminal', 'drop', 'frac', 'porous', 'complicated', 'shot', 'besides', 'inclusion', 'get', 'complementary', 'unlike', 'slowly', 'tensile', 'textures', 'ce', 'cellular', 'elusive', 'air', 'benchmark', 'wavefunction', 'pre', 'section', 'nanoribbons', 'surprisingly', 'duality', 'hypothesis', 'zn', 'nonmagnetic', 'mostly', 'staggered', 'ultrathin', 'absolute', 'eta', 'discontinuous', 'string', 'readily', 'acts', 'grow', 'mg', 'exclusion', 'slip', 'reservoir', 'accumulation', 'except', 'minima', 'imposed', 'reciprocal', 'drift', 'evolves', 'links', 'feedback', 'turbulence', 'shed', 'bar', 'helium', 'believed', 'superposition', 'ideas', 'failure', 'prominent', 'exploiting', 'operations', 'bandwidth', 'outside', 'hyperfine', 'notion', 'em', 'waveguide', 'unknown', 'scalable', 'spreading', 'versatile', 'regarding', 'reasonable', 'cone', 'losses', 'behind', 'instance', 'added', 'drastically', 'prototypical', 'bare', 'architecture', 'momenta', 'tends', 'hierarchical', 'log', 'ranges', 'odinger', 'irreversible', 'window', 'tilted', 'coordination', 'spinless', 'newly', 'emphasis', 'solely', 'goes', 'axial', 'masses', 'tail', 'opportunities', 'conclusion', 'goal', 'arrangement', 'passage', 'mathcal', 'until', 'put', 'suspended', 'chi', 'matching', 'interatomic', 'actual', 'anharmonic', 'transparent', 'post', 'ubiquitous', 'utilized', 'manifold', 'chip', 'argued', 'unprecedented', 'loading', 'call', 'lies', 'mk', 'projection', 'nanometer', 'tests', 'paves', 'satisfy', 'holographic', 'ln', 'concerning', 'eel', 'seems', 'interband', 'feasible', 'recovered', 'teller', 'uncertainty', 'drag', 'sudden', 'branches', 'sensors', 'nd', 'pave', 'curved', 'oxidation', 'circ', 'manifests', 'mirror', 'prior', 'aa', 'gradually', 'cycles', 'go', 'incorporating', 'aging', 'assess', 'pristine', 'pinned', 'energetic', 'efforts', 'weaker', 'spiral', 'favorable', 'base', 'pd', 'inspired', 'nor', 'precession', 'fracture', 'task', 'freezing', 'pyrochlore', 'metric', 'rb', 'rely', 'passive', 'simeq', 'xi', 'root', 'alone', 'adding', 'date', 'pr', 'jumps', 'obey', 'moir', 'published', 'parabolic', 'pathways', 'abrupt', 'lived', 'uncover', 'arxiv', 'tend', 'born', 'photoelectron', 'working', 'letter', 'front', 'qed', 'corner', 'viewed', 'axes', 'delocalized', 'tube', 'salt', 'training', 'emitters', 'electrochemical', 'trends', 'supercooled', 'nuclei', 'tendency', 'deg', 'aqueous', 'attached', 'undoped', 'color', 'ends', 'nevertheless', 'annihilation', 'softening', 'absorbing', 'mgb2', 'preserving', 'quadrupole', 'inner', 'every', 'bright', 'ferrimagnetic', 'slope', 'damage', 'overview', 'drude', 'violation', 'cut', 'mismatch', 'facilitate', 'disk', 'metamaterials', 'played', 'straightforward', 'readout', 'disappears', 'segregation', 'reminiscent', 'face', 'electrolyte', 'authors', 'impedance', 'comes', 'depletion', 'synchronization', 'mid', 'avoiding', 'communication', 'donor', 'cooperative', 'logic', 'polynomial', 'joint', 'femtosecond', 'linewidth', 'viscoelastic', 'themselves', 'bipartite', 'armchair', 'led', 'cloud', 'wetting', 'manganites', 'load', 'subtle', 'moves', 'subsystem', 'gated', 'emitted', 'coincides', 'pore', 'folding', 'piezoelectric', 'box', 'see', 'scientific', 'plus', 'ergodic', 'insensitive', 'valuable', 'silica', 'serves', 'micron', 'supercurrent', 'decoupling', 'hbar', 'ternary', 'superior', 'winding', 'isothermal', 'immersed', 'sliding', 'flowing', 'xy', 'dichroism', 'aggregation', 'unambiguously', 'cyclic', 'islands', 'once', 'capillary', 'synchrotron', 'note', 'living', 'bridge', 'community', 'roles', 'rectangular', 'slower', 'gel', 'timescales', 'deduced', 'neglected', 'jamming', 'replaced', 'migration', 'guide', 'noninteracting', 'fails', 'diode', 'stretching', 'sector', 'muon', 'amplification', 'unity', 'micromagnetic', 'sequential', 'imbalance', 'cryogenic', 'frozen', 'spinor', 'hydrostatic', 'rg', 'comprising', 'packed', 'kernel', 'saddle', 'neither', 'written', 'had', 'exposed', 'beams', 'evidenced', 'constitute', 'intuitive', 'drops', 'concrete', 'carefully', 'gd', 'criteria', 'fluorescence', 'textbf', 'delay', 'noisy', 'incompressible', 'interstitial', 'cooled', 'spite', 'reactive', 'lacking', 'lithium', 'tailored', 'utilize', 'tilt', 'needs', 'coated', 'monoclinic', 'trace', 'duration', 'damped', 'tails', 'virtual', 'walled', 'geq', 'triggered', 'poor', 'social', 'extrinsic', 'cdot', 'bifurcation', 'human', 'leg', 'unexplored', 'agents', 'fascinating', 'evaporation', 'history', 'uncorrelated', 'cascade', 'inorganic', 'suited', 'cation', 'life', 'precursor', 'tens', 'pseudospin', 'dopant', 'degradation', 'neighbour', 'cs', 'testing', 'stoichiometric', 'ir', 'emitting', 'covalent', 'grand', 'alkali', 'rheology', 'centrosymmetric', 'merit', 'fashion', 'aluminum', 'own', 'strictly', 'propelled', 'autocorrelation', 'red', 'auxiliary', 'magic', 'melt', 'cl', 'cones', 'disks', 'shallow', 'photovoltaic', 'rod', 'pnictides', 'inferred', 'pockets', 'hosts', 'escape', 'crack', 'intricate', 'nanocrystals', 'outer', 'instantaneous', 'careful', 'phosphorus', 'encoded', '1t', 'dissociation', 'dressed', 'discrepancy', 'prime', 'tree', 'missing', 'belongs', 'af', 'revisit', 'overdoped', 'fitted', 'fiber', 'kink', 'truncated', 'fourth', 'laboratory', 'poly', 'herein', 'spanning', 'huge', 'outstanding', 'fcc', 'bubble', 'check', 'obeys', 'star', 'mhz', 'variant', 'sides', 'acid', 'marginal', 'locking', 'figure', 'exp', 'hosting', 'nesting', 'thermopower', 'cutoff', 'predominantly', 'demand', 'von', 'fluxes', 'unveil', 'permittivity', 'radio', 'unclear', 'adapted', 'going', 'white', 'bonded', '2o', 'removed', 'hyperbolic', 'nearby', 'covered', 'marked', 'fusion', 'rods', 'wavevector', 'backscattering', 'preferred', 'ultimately', 'halide', 'causing', 'pairwise', 'biaxial', 'echo', 'drawn', 'baths', 'microcavity', 'monomers', 'flakes', 'forbidden', 'slab', 'iv', 'twice', 'happens', 'sparse', 'overdamped', 'eu', 'blue', 'hydrophobic', 'ultraviolet', 'le', 'resembles', 'unchanged', 'inertial', 'gating', 'nv', 'adhesion', '4f', 'trilayer', 'ingredient', 'meaning', 'freely', 'cores', 'lie', 'come', 'tomography', 'interparticle', 'breathing', 'fail', 'cyclotron', 'sodium', 'luminescence', 'eight', 'stimulated', 'inverted', 'pumped', 'anyons', 'technical', 'filaments', 'cite', 'reorientation', 'refractive', 'cobalt', 'irrespective', 'poorly', 'depinning', 'll', 'c2', 'segments', 'passing', 'conjunction', 'wider', 'filtering', 'mathbf', 'attempt', 'deeper', 'monitoring', 'redistribution', 'convenient', 'sputtering', 'deal', 'signs', 'rest', 'avalanches', 'rectification', 'release', 'quickly', 'ionization', 'frequently', 'elevated', 'ascribed', 'operate', 'elongated', 'avenue', 'diodes', 'yb', 'subspace', 'ps', 'packings', 'acceleration', 'codes', 'detuning', 'octahedral', 'tubes', 'coating', 'probabilistic', 'viable', 'augmented', 'unidirectional', 'maxima', 'native', 'drain', 'valve', 'gg', 'catalytic', 'sine', 'slowing', 'reentrant', 'mentioned', 'adopted', 'bring', 'digital', 'gets', 'monopole', 'ago', 'share', 'topic', 'toy', 'record', 'notably', 'resources', 'silver', 'torus', 'run', 'late', 'minority', 'spring', 'ns', 'expanding', 'mm', 'track', 'convex', 'photocurrent', 'radii', 'sea', 'fits', 'vdw', 'maintaining', 'hundreds', 'paired', 'hz', 'hbn', 'arcs', 'keeping', 'utility', 'superexchange', 'lipid', 'acceptor', 'prospects', 'supersymmetric', 'sizable', 'propulsion', 'ten', 'irreducible', 'debate', 'destructive', 'plaquette', 'illumination', 'software', 'routes', 'adatoms', 'hysteretic', 'destroyed', 'rhombohedral', 'antisymmetric', 'batteries', 'seem', 'br', 'brought', 'qd', 'pores', 'packet', 'ttf', 'settings', 'empty', 'isotope', 'helix', 'irrelevant', 'fault', 'evidences', 'intercalation', 'optomechanical', 'bcc', 'chem', 'purity', 'movement', '5d', 'locked', 'shock', 'outline', 'noncentrosymmetric', 'rank', 'diamagnetic', 'tetrahedral', '2g', 'mat', 'flight', 'slave', 'spinon', 'loaded', 'tricritical', 'compensation', 'excluded', 'read', 'nickel', 'meta', 'cond', 'squeezing', 'supersolid', 'rh', 'unbiased', 'acquire', 'bit', 'fall', 'iterative', 'buckling', 'cal', 'microcanonical', 'prevent', 'cm2', 'proton', 'valued', 'tasks', 'aid', 'rangle', 'th', 'neurons', 'industrial', 'abelian', 'trial', 'subset', 'middle', 'qcd', 'sm', 'thanks', 'profound', 'inequality', 'permanent', 'helps', 'creep', 'transmitted', 'look', 'encountered', 'promote', 'whereby', 'devoted', 'eff', 'tio2', 'jammed', 'trigonal', 'arc', 'ratchet', 'divided', 'mimic', 'repeated', 'smectic', 'fragile', 'turned', 'ligand', 'interior', 'carbide', 'thorough', 'claim', 'language', 'wse', 'coming', 'tungsten', 'bismuth', 'genuine', 'gels', 'fillings', '2p', 'contour', 'micrometer', 'apart', 'assigned', 'fragmentation', 'wse2', 'commercial', 'ribbon', 'survive', 'aims', 'suffer', 'stems', 'ge', '2h', 'titanium', 'nanomechanical', 'coplanar', 'nonperturbative', 'motor', 'vanadium', 'forced', 'contraction', 'gained', 'extraordinary', 'dip', 'pseudopotential', 'pl', 'ads', 'virial', 'dry', 'zeta', 'hardware', 'guided', 'zinc', 'adjustable', 'attenuation', 'dioxide', 'skin', 'stored', 'mono', 'return', 'horizontal', 'traveling', '2e', 'waiting', 'cumulant', 'dataset', 'cd', 'zeros', 'dome', 'bedt', 'seemingly', 'market', 'melts', 'isostructural', 'lasing', 'markedly', 'obstacle', 'harvesting', 'exfoliated', 'percent', 'lithography', 'door', 'received', 'heuristic', 'columnar', 'lesssim', 'enters', 'modest', 'sheds', 'tin', 'antiparallel', 'enriched', 'traffic', 'decorated', 'albeit', 'solver', 'corroborated', 'manufacturing', 'evident', 'etching', 'fairly', 'akin', 'outcome', 'bacteria', 'leakage', 'linking', 'whenever', 'spinel', 'opened', 'viewpoint', 'grid', 'bistable', 'braiding', 'el', 'nonuniform', 'bodies', 'answer', 'circle', 'bi2se3', 'vital', 'wherein', 'spots', 'spot', 'ever', 'sometimes', 'spatio', '3he', 'twin', 'affine', 'write', 'quarter', 'brain', 'subband', 'beneficial', 'controversial', 'unaffected', 'benefit', 'vast', 'clock', 'lost', 'trade', 'incomplete', 'sandwiched', 'finds', 'ensure', 'volatile', 'colossal', 'relying', 'desorption', 'qds', 'quartic', 'silicene', 'ceramics', 'perp', 'max', 'extrapolation', 'sought', 'hitherto', 'floating', 'basal', 'leaving', 'forcing', 'cohesive', 'brittle', 'scope', 'sustained', 'falls', 'encapsulated', 'paid', 'spinodal', 'intermolecular', 'tilting', 'noncollinear', 'intermetallic', 'conjugated', 'backbone', 'enyi', 'sinusoidal', 'obeying', 'survey', 'proceeds', 'choosing', 'tiny', 'termed', 'raised', '4d', 'canted', 'expensive', 'metamagnetic', 'eigenvectors', 'database', 'supercell', 'coalescence', 'ell', 'tractable', 'eigenfunctions', 'concomitant', 'picosecond', '2cu', 'chalcogenide', 'electromechanical', 'catalyst', 'la0', 'motility', 'evy', 'langle', 'consumption', 'footing', 'atmosphere', 'add', 'phosphorene', 'sp', 'consecutive', 'swimming', 'distant', 'widespread', 'cmos', 'refined', 'mol', 'hardness', 'metrology', 'hat', 'did', 'permeability', 'running', 'tb', 'package', 'quanta', 'hallmark', 'program', 'meet', 'toroidal', 'somewhat', 'backward', 'course', 'garnet', 'inconsistent', 'deeply', 'intermittent', 'deconfined', 'operational', 'appealing', 'dirty', 'aimed', 'plausible', 'conical', 'partly', 'genetic', 'yttrium', 'tn', 'unoccupied', 'catalysis', 'detrimental', 'guiding', 'vice', 'plot', 'fs', 'satisfactory', 'meanwhile', 'textrm', 'management', 'molybdenum', 'strange', 'onsite', 'kept', 'germanium', 'nanomaterials', 'wurtzite', 'closure', 'held', 'autonomous', 'game', 'nonmonotonic', 'recognized', 'nominal', 'obvious', 'triangle', 'anion', 'begin', 'stemming', 'renders', 'hindered', 'enormous', 'insertion', 'hardening', 'sophisticated', 'preliminary', 'keep', '3o', 'bits', 'intimately', 'wet', 'puzzling', 'imperfections', 'immediately', 'steel', 'heterojunction', 'interchain', 'dinger', 'members', 'vi', 'kinks', 'fullerene', 'coil', 'remote', 'anticipated', 'rf', 'helpful', 'summarize', 'heated', 'bottleneck', 'interpolation', 'polydisperse', 'nonextensive', 'noble', 'ws2', 'nested', 'rearrangements', 'dictated', 'abundant', 'buffer', 'ultrahigh', 'xps', 'tolerant', 'regression', 'photoexcited', 'participation', 'irregular', 'oil', 'seven', 'vesicles', 'cage', 'cycling', 'mesoscale', 'eliminate', 'automatically', 'converted', 'tissue', 'paving', 'costs', 'fluence', 'ongoing', 'mc', 'restored', 'visualization', 'adequate', 'bimodal', 'hgte', 'porosity', 'primitive', 'indium', 'flavor', 'indistinguishable', 'nucleus', 'beads', 'tm', 'bundle', 'usefulness', 'supplemented', 'draw', 'update', 'envelope', 'amenable', 'bilinear', 'offset', 'meaningful', 'eigenmodes', 'inaccessible', 'gallium', 'enthalpy', 'neq', 'named', 'circumstances', 'niobium', 'portion', 'trees', 'pass', '4he', 'financial', 'c60', 'lifted', 'incorrect', 'admits', 'leaves', 'unequal', 'extinction', 'ws', 'turning', 'fingerprint', 'handle', 'poses', 'unpolarized', 'rotor', 'nonreciprocal', 'elaborate', 'implantation', 'wealth', 'insufficient', 'guidance', 'richer', 'merge', 'unravel', 'al2o3', 'subgap', 'opto', 'microfluidic', 'demagnetization', 'semiflexible', 'stoner', 'athermal', 'lineshape', 'fate', 'khz', 'elliptic', 'bio', 'recursive', 'er', 'slabs', 'coercive', 'potassium', 'retains', 'yig', 'sapphire', 'cnt', 'merging', 'summation', 'subwavelength', 'epidemic', 'auto', 'tips', 'truly', 'joule', 'raises', 'operated', 'becs', 'monodisperse', 'delicate', 'photoinduced', 'sc', 'big', 'fuel', 'span', 'manuscript', 'wafer', 'pole', 'lens', 'coatings', 'nonadiabatic', 'lot', 'butterfly', 'supercritical', 'celebrated', 'exerted', 'ar', 'inp', 'flop', 'spurious', 'walker', 'copolymer', 'aperiodic', 'min', 'tremendous', 'utilizes', 'lamellar', 'realm', 'grating', 'hardly', 'arsenide', 'thousands', 'auger', 'bead', 'quark', 'norm', 'compelling', 'incompatible', 'words', 'consensus', 'tackle', 'price', 'attained', 'hcp', 'plethora', 'lastly', 'noted', 'insb', 'manganese', 'psi', 'chapter', 'belief', 'century', 'rna', 'covariance', 'rarely', 'opinion', 'who', 'screw', 'hours', 'accomplished', 'ignored', 'heart', 'prefactor', 'sulfur', 'ye', 'fe1', 'anchoring', 'easier', 'salts', 'exemplified', 'zones', 'nanodevices', 'folded', 'nanomagnets', 'trimer', 'neuromorphic', 'entrant', 'ph', 'retarded', 'serious', 'pivotal', 'rays', 'platinum', 'axion', 'biases', 'chromium', 'solvation', 'prescribed', 'latent', 'cp', 'gtrsim', 'antivortex', 'martensitic', 'resetting', 'plots', 'quartz', 'sweep', 'topography', 'cusp', 'xx', 'calibration', 'gene', 'spt', 'vec', 'stranded', 'contradiction', 'cancellation', 'evanescent', 'gaussian', 'times10', 'precipitation', 'magnesium', 'unphysical', 'dealing', 'stars', 'paradox', 'koe', 'swimmers', '1g', 'drug', 'unfolding', 'doing', 'symplectic', 'biquadratic', 'user', 'unless', 'motifs', 'ex', 'paraelectric', 'ensuing', 'solubility', 'bend', 'nanoelectronics', 'ultrashort', 'gr', 'vapour', 'template', 'million', 'noticeable', 'synaptic', 'status', 'noises', 'innovative', 'operates', 'peaked', 'burgers', 'boost', 'kev', 'justified', 'ferrite', 'sizeable', 'ohmic', 'fets', 'unpaired', 'trion', 'dubbed', 'thinning', 'usage', 'tis', 'old', 'nanocomposites', 'twofold', 'overlooked', 'patches', 'ad', 'naive', 'bases', 'bed', 'aforementioned', 'transmon', 'ms', 'subdiffusive', 'putative', 'bipolar', 'benzene', 'rutile', 'hydration', 'torsion', 'um', 'his', 'devise', 'osmotic', 'hope', 'prerequisite', 'rupture', 'specimen', 'mt', 'lift', 'intersite', 'prescription', 'convection', 'weakening', 'table', 'commuting', 'playing', 'hf', 'intertwined', 'stem', 'guidelines', 'hydride', 'lifting', 'equilibria', 'valves', 'graded', 'tl', 'facets', 'gyration', 'handed', 'pedagogical', 'unsupervised', 'viz', 'hc2', 'speculate', 'disulfide', 'hollow', 'sandpile', 'bind', 'microscale', 'strontium', 'o2', 'roton', 'fill', 'healing', 'photodetectors', 'tri', 'toric', 'swelling', 'wise', 'respond', 'o3', 'famous', 'flips', 'cantilever', 'unperturbed', 'macromolecules', 'superimposed', '2n', 'nanopores', 'facts', 'nanoscopic', 'green', 'jc', 'oxidized', 'sts', 'disconnected', 'likelihood', 'interconnected', 'delivery', 'sintering', 'salient', 'stark', 'presumably', 'nbse', 'shielding', 'bearing', 'sums', 'kicked', 'hydrophilic', 'stick', 'slit', 'cfts', 'dangling', 'nanosecond', 'experienced', 'deals', 'epr', 'advection', 'cast', 'skew', 'proliferation', 'day', 'unfortunately', 'isolation', 'none', 'appreciable', 'bandstructure', 'oe', 'h2', 'bkt', 'superradiant', 'dedicated', 'rises', 'intact', 'la1', 'nonrelativistic', 'arrest', 'dye', 'deficient', 'unbounded', 'death', 'lock', 'hamiltonian', 'buried', 'icosahedral', 'arrive', 'poles', 'playground', 'discriminate', 'axisymmetric', 'fitness', 'nanoclusters', 'convolutional', 'ball', 'hypothetical', 'constriction', 'annular', 'ch', 'touching', 'supposed', 'perhaps', 'la2', 'category', 'superstructure', 'prl', 'zt', 'instanton', 'corrugated', 'refrigerator', 'wavenumber', 'reconfigurable', 'utilization', 'cathode', 'unable', 'tilde', 'thermometry', 'yz', 'departure', 'intersection', 'outperforms', 'disentangle', 'catastrophe', 'ultralow', 'bent', 'steric', 'z2', 'confocal', 'actin', 'tweezers', 'emulsions', 'thesis', 'kinematic', 'polyelectrolyte', 'fet', 'sl', '300k', 'hoppings', 'depolarization', 'null', 'stray', 'void', 'disagreement', 'amino', 'immiscible', 'decisive', 'looking', 'care', 'exit', 'hawking', 'fewer', 'invoking', 'diatomic', 'dose', 'indispensable', 'nanorods', 'tangent', 'head', 'incipient', 'staircase', 'iteration', 'rock', 'mv', 'inevitably', 'anvil', 'mesh', 'corrosion', 'pulling', 'lacks', 'economic', 'voids', 'immune', '1s', 'message', 'aiming', 'zeroth', 'risk', 'supply', 'roots', 'cutting', 'varphi', 'ref', 'foams', 'antenna', 'zag', 'oblique', 'bi2te3', 'cosmological', 'stays', 'faces', 'reinforcement', 'showcase', 'age', 'translocation', 'pm0', 'zig', 'leave', 'nanophotonic', 'list', 'invasive', 'recurrent', 'pertinent', 'misfit', 'whilst', 'keeps', 'unbinding', 'immobile', 'stock', 'ramp', 'soon', 'counterions', 'know', 'densely', 'np', 'tiling', 'drying', 'bootstrap', 'discharge', 'friendly', 'wt', 'biochemical', 'enlarged', 'biexciton', 'intended', 'c1', 'demixing', 'satellite', 'ruled', 'rescaled', 'polystyrene', 'border', 'passes', 'enclosed', 'ellipsometry', 'submitted', 'ductile', 'sol', 'rooted', 'scrambling', 'bafe2as2', 'labeled', 'ip', 'causal', 'blende', 'prone', 'histogram', 'adds', 'mix', 'wedge', 'cnts', 'entities', 'scan', 'birth', 'redox', 'solitary', 'diagnostic', 'calls', 'flexural', '2as2', 'slips', 'marks', 'spirit', 'ripples', 'microswimmers', 'cos', 'ultrasonic', 'nonetheless', 'merely', 'calcium', 'bi2sr2cacu2o8', 'cotunneling', 'dissimilar', 'dissolution', 'foam', 'efficacy', 'iridates', 'pdf', 'push', 'rising', '4h', 'interspecies', 'dopings', 'mj', 'tailed', 'flipping', 'subleading', 'bad', 'ultrasound', 'bunching', 'db', 'anode', 'colliding', 'worked', 'archetypal', 'steep', '2k', '3mno3', 'nanotechnology', 'accommodate', 'canting', 'id', 'nanosheets', 'arms', 't1', 'biomedical', 'fourfold', 'biasing', 'raising', 'raise', 'try', 'diminishes', 'acids', 'cube', 'sr2ruo4', 'pitch', 'enforced', 'actuation', 'division', 'minutes', 'superparamagnetic', 'para', 'today', 'tumble', 'aluminium', 'freestanding', 'permalloy', 'slows', 'chamber', 'warping', 'isolate', 'supervised', 'acquisition', 'unresolved', 'hinge', 'fr', 'thinner', 'beating', 'toughness', 'counterintuitive', 'mps', 'glide', 'loads', 'days', 'abstract', 'imaged', 'photocatalytic', 'xz', 'reality', 'perimeter', 'pumps', 'upward', 'quaternary', 'notes', 'seed', '5k', 'argon', 'arm', 'antilocalization', 'smart', 'vitro', 'micelles', 'games', 'spike', 'piecewise', 'solves', 'circumvent', 'ease', 'remnant', 'retrieval', 'runs', 'disease', 'nat', 'rung', 'squids', 'wrong', 'spectrometer', 'became', 'guarantee', 'sink', 'radical', 'hoc', 'cool', 'unavoidable', 'vibronic', 'spanned', 'detachment', 'shrinks', 'j1', 'descent', 'row', 'bars', 'capping', 'selenide', 'revival', 'permutation', 'ill', 'hump', 'printing', 'monochromatic', 'concurrence', 'hermiticity', 'sideband', 'ageing', 'n2', 'j2', 'methyl', 'disc', 'nodeless', 'denotes', 'serving', 'calorimetry', 'submicron', 'micromechanical', 'inactive', 'warm', 'routinely', 'uranium', 'fulfilled', 'chloride', 'spinful', 'mitigate', 'library', 'equipped', 'molten', 'forth', 'concave', 'ij', 'birefringence', 'billiard', 'thermostat', 'name', 'sand', 'bears', 'vivo', 'withdrawn', 'graphical', 'pose', 'hg', 'pushed', 'preceding', 'tapes', 'alumina', 'bi2212', 'divalent', 'vo2', 'intelligence', 'metasurfaces', 'nine', 'endowed', 'tuneable', 'decimation', 'aromatic', 'prevalent', 'grafted', 'reconcile', 'downward', 'paramount', 'fqh', 'coli', 'spinning', 'photoconductivity', 'web', 'lubrication', 'antidot', 'renewed', 'fictitious', 'scientists', 'tied', 'scars', 'iridate', 'etched', 'rolling', 'golden', 'year', 'trivalent', 'latest', 'essence', 'instrument', 'legs', 'cat', 'file', 'leverage', 'contamination', 'tddft', 'dissolved', 'stay', 'fatigue', 'contemporary', 'h2o', 'ellipsoidal', 'enumeration', 'intersubband', 'hts', 'holon', 'conflicting', 'pairings', 'piece', 'superdiffusive', 'copies', 'outgoing', 'pin', 'ask', 'restitution', 'upturn', 'shedding', 'fracton', 'blood', '2sr', 'unrestricted', 'hints', 'exhaustive', 'lay', 'rouse', 'remanent', 'crowding', 'pbte', 'medical', 'transduction', 'uncoupled', 'flory', 'gains', 'pieces', 'memristive', 'disparate', 'say', 'cds', 'smeared', 'publication', 'kbar', 'favourable', 'inexpensive', 'stop', 'encompassing', 'subcritical', 'interdot', 'round', 'overlayer', 'resilience', 'trans', 'monatomic', 'bear', 'co2', 'posed', 'banding', 'lab', 'unsolved', 'longest', 'anatase', 'mineral', 'propensity', 'gelation', 'globular', 'increment', 'appl', 'outlook', 'road', 'biomolecules', 'bigger', 'disciplines', 'physiological', 'apex', 'raw', 'millimeter', 'reorganization', 'antiferroelectric', 'elegant', 'deflection', 'falling', 'bursts', 'modal', 'worm', 'wavepacket', 'harnessing', 'fastest', 'downarrow', '5f', 'mild', 'confidence', 'milling', 'knots', 'unrelated', 'ambiguity', 'accidental', 'fluoride', 'tmr', 'disclination', 'ast', 'vastly', 'fixing', 'gaseous', 'hexatic', 'twinning', 'sort', 'substance', 'lifts', 'impenetrable', 'nonsymmorphic', 'p4', 'molar', 'hop', 'pulled', 'dendritic', 'ethylene', 'shadow', 'retention', 'resorting', 'shunted', 'indentation', 'ambipolar', 'un', 'voter', 'timing', 'uparrow', 'polyethylene', 'synapses', 'resummation', 'fluorine', 'emit', 'pc', 'cages', 'polygons', 'tubular', 'viability', 'apical', 'speckle', 'adjoint', 'pertaining', 'nondegenerate', 'parasitic', 'valent', 'comb', 'gpu', 'mutation', 'cleaved', 'hampered', 'hypercubic', 'sot', 'rubber', 'fly', 'su', 'puts', 'longstanding', 'wte2', '2nd', 'dt', 'allotropes', 'gaining', 'faithfully', 'artifacts', 'stopping', 'sin', 'lanthanide', 'ultrastrong', 'workflow', 'lose', 'mappings', 'monolithic', 'lossy', 'drawing', 'disproportionation', 'online', 'electroluminescence', 'seminal', 'nowadays', 'unexplained', 'miscible', 'ablation', 'soc', 'nonuniversal', 'cholesteric', 'mask', 'fringes', 'everywhere', 'nonvanishing', 'moved', 'ma', 'pancake', 'pillars', 'threading', 'scarce', 'grids', 'till', 'dp', 'nbse2', 'tilings', 'regulatory', 'locomotion', 'plotted', 'monovalent', 'fe3o4', 'dips', 'fission', 'abnormal', 'harness', 'imprinted', 'nrg', 'humidity', 'grazing', 'cosmic', 'antinodal', 'toolbox', 'wte', 'sulfide', 'unscreened', 'advent', 'twenty', 'antibonding', 'jellium', 'gave', 'live', 'mosfets', 'nonclassical', 'notoriously', 'repetition', 'photogalvanic', 'i4', 'preformed', 'dv', 'obeyed', 'diblock', 'infected', 'mmm', 'option', 'capped', 'amphiphilic', 'pentacene', 'brane', 'overline', 'seek', 'postulate', 'hardcore', 'concise', 'nps', 'cancer', 'teleportation', 'math', 'lanthanum', 'aerogel', 'purification', 'users', 'uneisen', 'admixture', 'rt', 'holding', 'tagged', 'knot', 'telecom', 'coding', 'clamped', 'isospin', 'era', 'inclined', 'mind', 'inhibition', 'steering', 'disjoint', 'achiral', 'improper', 'ecological', 'b2', 'telluride', 'gp', 'photoresponse', 'fresh', 'pile', 'nonpolar', 'nonvolatile', 'oblate', 'beneath', 'companion', 'yba2cu3o6', 'sedimentation', 'food', 'cerium', 'int', 'sp2', 'sawtooth', 'animal', 'cap', 'succeeded', 'sided', 'entails', 'loose', 'coils', '1k', 'isovalent', 'sitter', 'visited', 'iso', 'nanoelectromechanical', 'unwanted', 'iterations', 'spans', 'moire', 'wind', 'arsenic', 'prevailing', 'minus', 'disturbance', 'entries', 'society', 'finely', 'folds', 'compromise', 'keywords', 'entrance', 'guest', 'miniaturization', 'subsurface', 'ins', 'flattening', 'sorting', 'handedness', 'di', 'explosive', 'fix', 'custom', 't2', 'laminar', 'denaturation', 'really', 'stream', 'backaction', 'lays', 'dewetting', 'iodide', 'chips', 'byproduct', 'feed', 'met', 'ne', 'testable', 'unbroken', 'discs', 'sg', 'eutectic', 'hydrocarbon', 'inadequate', 'reading', 'atypical', 'trick', 'ferroelastic', 'tunes', 'qpc', 'kg', 'ohm', 'isoelectronic', 'tethered', 'kv', 'boldsymbol', 'pursuit', 'reactor', 'fire', 'harnessed', 'fair', 'https', 'cuts', 'logistic', 'binder', 'interrupted', 'gyrotropic', 'depression', 'aperture', 'softer', 'wettability', 'supramolecular', 'hydrogels', 'basin', 'nonintegrable', 'convincing', 'tape', 'inefficient', 'hint', 'cigar', 'fat', 'people', 'pacs', 'virus', 'costly', 'nonthermal', 'methane', 'softness', 'pool', 'hosted', 'wrinkles', 'puddles', 'prolate', 'bidisperse', 'telegraph', 'hinges', 'fugacity', 'counterflow', 'uru2si2', 'barium', 'immense', 'ai', 'cytoskeleton', 'supersonic', 'loses', 'mct', 'cosine', 'exemplary', 'wisdom', 'pay', 'bis', 'ba0', 'eigenenergies', 'genome', 'jet', '4k', 'gauging', 'goals', 'fused', 'rows', 'pushing', 'spectacular', 'meso', '6h', 'muffin', 't2g', 'mysterious', 'perceptron', 'uncompensated', 'microorganisms', 'brush', 'failed', 'bidirectional', 'astrophysical', 'dichotomy', 'uptake', 'metabolic', 'hyperscaling', 'ridge', 'lasting', 'tls', 'gauged', 'rubidium', 'stepwise', 'lecture', 'pm1', 'tbg', 'anticrossing', 'vision', 'animals', 'enigmatic', 'macroscale', '3m', 'fe3', 'quadruple', 'chemisorption', 'fifth', 'heteronuclear', 'false', 'let', 'compliance', 'zincblende', 'kt', 'genus', 'antisite', 'destabilize', 'upto', 'video', 'surrogate', 'unfavorable', 'reconsider', 'necessitates', 'glycerol', 'dilemma', 'sr2iro4', 'focal', 'subgroup', 'fibre', 'midgap', 'swnt', 'p2', 'inspection', 'pinch', 'microtubules', 'nws', 'slender', 'ethanol', 'swnts', 'aided', 'diselenide', 'said', 'isostatic', 'impressive', 'daily', 'endpoint', 'platelets', 'pull', 'bse', 'dumbbell', 'antiphase', 'waste', 'heats', 'm2', 'photogenerated', 'vol', 'semilocal', 'unconstrained', 'putting', 'getting', 'downstream', 'nanosystems', 'witnessed', 'iteratively', 'drawbacks', 'interconversion', 'nanofabrication', 'talk', 'com', 'firing', 'scans', 'l1', 'skewness', 'schematic', 'la', 'receptor', 'xxx', 'mini', 'relied', 'synergistic', 'pixel', 'phosphate', 'ppm', 'sole', 'lag', 'tantalum', 'eg', '4e', 'biophysical', 'ga2o3', 'superflow', 'subthreshold', 'erasure', 'formidable', 'ohlich', 'gluon', 'germanene', 'equidistant', 'swept', 'flexoelectric', 'wormlike', 'warming', 'zirconium', 'xe', 'garnered', 'dead', 'ending', 'dwell', 'smb6', 'joined', 'fp', 'palladium', 'players', 'inaccurate', 'deuterium', 'macrospin', 'documented', 'stat', 'microrheology', 'nanodiamonds', 'dws', 'forest', 'homopolymer', 'austenite', 'recoil', 'clay', '87rb', 'emulate', 'qpt', 'book', 'wear', 'noncommutative', 'enzyme', 'prismatic', 'invisible', 'mae', 'ruling', 'ep', 'climate', 'seismic', 'elsewhere', 'mw', 'qws', 'millikelvin', 'reformulation', 'serial', 'forefront', 'pioneering', 'paying', 'helimagnetic', 'misleading', 'ferroic', 'sp3', 'dhva', 'summing', 'intermixing', 'flocking', 'harder', 'sonic', 'shaking', 'biopolymers', 'fertile', 'redshift', 'hydrothermal', 'glue', 'nicely', 'quiescent', 'fe2', 'facing', 'misorientation', 'xsrxcuo4', 'niobate', 'aberration', 'piston', 'nanoplatelets', 'sw', 'knob', 'nonradiative', '2m', 'undirected', 'my', 'monoatomic', 'looks', 'odds', 'overestimates', 'sessile', 'nanopillars', 'needle', 'nothing', 'phenyl', 'casting', 'man', 'trs', 'iridium', 'nanoindentation', 'ba1', 'hyperuniform', 'uncontrolled', 'reentrance', 'textbook', 'circumference', 'meets', 'lumped', 'stainless', 'collaboration', 'recast', 'crosslinked', 'parton', 'undamped', 'coaxial', 'lenses', 'csv', 'hydroxyl', 'cpu', 'students', 'casher', 'gun', 'hull', 'ahead', 'lesser', 'mbox', 'l2', 'compass', 'barely', 'denominator', 'secular', 'hour', 'tilts', 'mark', 'traversing', 'fruitful', 'finesse', 'attack', 'glycol', 'tutorial', 'isomers', 'interdisciplinary', 'terraces', 'pentagonal', 'pn', 'qft', 'oligomers', '2as', 'org', 'routing', 'cycloidal', 'reactants', 'peptide', 'levitated', 'partners', 'gbs', 'swim', 'cri3', 'encouraging', 'phthalocyanine', 'upstream', 'fan', 'richness', 'irrational', 'undesirable', 'boxes', 'msd', 'smb', 'geodesic', 'splay', 'symmorphic', 'transcription', 'impulse', 'viral', 'mnbi2te4', 'mere', 'verge', 'footprint', 'spheroidal', 'unstrained', 'im', 'thermionic', 'dust', 'oc', 'fe2o3', 'decoding', 'unobserved', 'zirconia', 'ammonia', 'erosion', 'misalignment', 'hs', 'bdg', 'colonies', 'tandem', 'nonresonant', 'skutterudite', 'gallery', '2s', 'uni', 'cohomology', 'mixes', 'clathrate', 'nonergodic', 'ices', 'mosaic', 'camera', 'plain', 'lone', 'cesium', 'pr0', 'pf', 'cortex', 'witness', 'rocksalt', 'hops', 'swap', 'symbolic', 'overhead', 'denser', 'microparticles', 'rungs', 'ftir', 'penalty', 'nonstationary', 'skeleton', 'flatness', '40k', 'mn12', 'finer', 'chemotaxis', 'hfo2', 'prey', 'sat', 'epithelial', 'slowdown', 'knotted', 'ceases', '2x', 'firm', 'quintuple', 'subtraction', '10k', 'nanospheres', 'voigt', 'leaky', 'filtration', 'plant', 'uckel', 'things', '1st', 'microsecond', 'locus', 'pendulum', 'envisioned', 'joining', 'turing', 'jt', '2cuo', 'xc', 'wake', 'ruthenate', 'pyramidal', 'quartet', 'backflow', 'colour', 'exterior', 'antimony', 'borophene', 'acetate', 'compliant', 'otimes', 'twinned', 'unreported', 'homotopy', 'http', 'loosely', 'inability', 'heterointerface', 'antibunching', 'qp', 'nanodots', 'erroneous', 'tellurium', 'ascertain', 'slowed', 'cortical', 'clues', 'gs', 'bichromatic', 'soil', 'electromigration', 'pyrolytic', '1h', 'polyhedra', 'swarm', 'epilayers', 'thirring', 'permeation', 'recall', 'binds']\n"
          ]
        }
      ],
      "source": [
        "print(base_vocabulary_unnamed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8df77413-0c51-4a45-b762-14474faaf2ea",
      "metadata": {
        "id": "8df77413-0c51-4a45-b762-14474faaf2ea"
      },
      "source": [
        "We will now replace each token variation in the title and abstract with its base form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3accf105-8e1f-4448-8bcd-9b059e328aaa",
      "metadata": {
        "id": "3accf105-8e1f-4448-8bcd-9b059e328aaa",
        "outputId": "7cd4483a-c4ad-4d49-bd7e-9e7682b1e2d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'superconducting'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_word_dict['superconductor']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de6e5188-6c32-4c25-b9ab-c570da03f056",
      "metadata": {
        "id": "de6e5188-6c32-4c25-b9ab-c570da03f056"
      },
      "outputs": [],
      "source": [
        "def replace_words(text):\n",
        "    new_text = []\n",
        "    for word in text:\n",
        "        if word.lower() in stop_words:\n",
        "            continue\n",
        "        if word.lower() in base_word_dict:\n",
        "            new_text.append(base_word_dict[word.lower()])\n",
        "        else:\n",
        "            new_text.append(word.lower())\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3810ecd5-2ad3-40b0-a3c7-3c270116db99",
      "metadata": {
        "id": "3810ecd5-2ad3-40b0-a3c7-3c270116db99",
        "outputId": "f0fede44-ff46-42de-e4b9-b90cb06831aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing rows: 100%|██████████████| 368030/368030 [00:03<00:00, 104255.91it/s]\n",
            "Processing rows: 100%|███████████████| 368030/368030 [00:12<00:00, 30378.83it/s]\n"
          ]
        }
      ],
      "source": [
        "cond_mat.loc[:,'title_base_replaced'] = cond_mat['title_tokenized'].progress_apply(replace_words)\n",
        "cond_mat.loc[:,'abstract_base_replaced'] = cond_mat['abstract_tokenized'].progress_apply(replace_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c8e489e-c46d-4633-9f49-640368ea83cf",
      "metadata": {
        "id": "1c8e489e-c46d-4633-9f49-640368ea83cf",
        "outputId": "37f7e98d-6097-4e45-f8d1-09ed5722e3a9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>categories</th>\n",
              "      <th>id</th>\n",
              "      <th>authors</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>versions</th>\n",
              "      <th>semesters_since_March_1992</th>\n",
              "      <th>weeks_since_March_1992</th>\n",
              "      <th>months_since_March_1992</th>\n",
              "      <th>title_tokenized</th>\n",
              "      <th>abstract_tokenized</th>\n",
              "      <th>title_base_replaced</th>\n",
              "      <th>abstract_base_replaced</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cond-mat.mes-hall</td>\n",
              "      <td>0704.0006</td>\n",
              "      <td>Y. H. Pong and C. K. Law</td>\n",
              "      <td>Bosonic characters of atomic Cooper pairs acro...</td>\n",
              "      <td>We study the two-particle wave function of p...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
              "      <td>30</td>\n",
              "      <td>786</td>\n",
              "      <td>180</td>\n",
              "      <td>[Bosonic, characters, of, atomic, Cooper, pair...</td>\n",
              "      <td>[We, study, the, two, particle, wave, function...</td>\n",
              "      <td>[bosonic, characterized, atomic, cooper, pairs...</td>\n",
              "      <td>[study, two, particle, wave, function, paired,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cond-mat.mtrl-sci</td>\n",
              "      <td>0704.0008</td>\n",
              "      <td>Damian C. Swift</td>\n",
              "      <td>Numerical solution of shock and ramp compressi...</td>\n",
              "      <td>A general formulation was developed to repre...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
              "      <td>30</td>\n",
              "      <td>786</td>\n",
              "      <td>180</td>\n",
              "      <td>[Numerical, solution, of, shock, and, ramp, co...</td>\n",
              "      <td>[A, general, formulation, was, developed, to, ...</td>\n",
              "      <td>[numerical, solution, shock, ramp, compression...</td>\n",
              "      <td>[general, formula, was, developed, representat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cond-mat.str-el cond-mat.stat-mech</td>\n",
              "      <td>0704.0025</td>\n",
              "      <td>A. S. Mishchenko (1 and 2) and N. Nagaosa (1 a...</td>\n",
              "      <td>Spectroscopic Properties of Polarons in Strong...</td>\n",
              "      <td>We present recent advances in understanding ...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
              "      <td>30</td>\n",
              "      <td>787</td>\n",
              "      <td>181</td>\n",
              "      <td>[Spectroscopic, Properties, of, Polarons, in, ...</td>\n",
              "      <td>[We, present, recent, advances, in, understand...</td>\n",
              "      <td>[spectroscopy, properties, polarization, stron...</td>\n",
              "      <td>[present, recent, advanced, under, ground, exc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cond-mat.mes-hall</td>\n",
              "      <td>0704.0027</td>\n",
              "      <td>M. O. Goerbig, J.-N. Fuchs, K. Kechedzhi, Vlad...</td>\n",
              "      <td>Filling-Factor-Dependent Magnetophonon Resonan...</td>\n",
              "      <td>We describe a peculiar fine structure acquir...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
              "      <td>30</td>\n",
              "      <td>787</td>\n",
              "      <td>181</td>\n",
              "      <td>[Filling, Factor, Dependent, Magnetophonon, Re...</td>\n",
              "      <td>[We, describe, a, peculiar, fine, structure, a...</td>\n",
              "      <td>[filling, factor, dependence, magnetophonon, r...</td>\n",
              "      <td>[described, peculiar, fine, structure, acquire...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>cond-mat.str-el</td>\n",
              "      <td>0704.0030</td>\n",
              "      <td>J.P.Hague and N.d'Ambrumenil</td>\n",
              "      <td>Tuning correlation effects with electron-phono...</td>\n",
              "      <td>We investigate the effect of tuning the phon...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
              "      <td>30</td>\n",
              "      <td>786</td>\n",
              "      <td>180</td>\n",
              "      <td>[Tuning, correlation, effects, with, electron,...</td>\n",
              "      <td>[We, investigate, the, effect, of, tuning, the...</td>\n",
              "      <td>[tuning, correlation, effect, electron, phonon...</td>\n",
              "      <td>[investigate, effect, tuning, phonon, energy, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369013</th>\n",
              "      <td>supr-con cond-mat.supr-con</td>\n",
              "      <td>supr-con/9608008</td>\n",
              "      <td>R. Prozorov, M. Konczykowski, B. Schmidt, Y. Y...</td>\n",
              "      <td>On the origin of the irreversibility line in t...</td>\n",
              "      <td>We report on measurements of the angular dep...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Mon, 26 Aug 199...</td>\n",
              "      <td>9</td>\n",
              "      <td>234</td>\n",
              "      <td>53</td>\n",
              "      <td>[On, the, origin, of, the, irreversibility, li...</td>\n",
              "      <td>[We, report, on, measurements, of, the, angula...</td>\n",
              "      <td>[origin, irreversible, line, thin, ybacuo7, fi...</td>\n",
              "      <td>[report, measurements, angular, dependence, ir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369014</th>\n",
              "      <td>supr-con cond-mat.supr-con</td>\n",
              "      <td>supr-con/9609001</td>\n",
              "      <td>Durga P. Choudhury, Balam A. Willemsen, John S...</td>\n",
              "      <td>Nonlinear Response of HTSC Thin Film Microwave...</td>\n",
              "      <td>The non-linear microwave surface impedance o...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Sat, 31 Aug 199...</td>\n",
              "      <td>9</td>\n",
              "      <td>234</td>\n",
              "      <td>53</td>\n",
              "      <td>[Nonlinear, Response, of, HTSC, Thin, Film, Mi...</td>\n",
              "      <td>[The, non, linear, microwave, surface, impedan...</td>\n",
              "      <td>[nonlinear, response, htsc, thin, film, microw...</td>\n",
              "      <td>[non, linear, microwave, surface, impedance, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369015</th>\n",
              "      <td>supr-con cond-mat.supr-con</td>\n",
              "      <td>supr-con/9609002</td>\n",
              "      <td>Balam A. Willemsen, J. S. Derov and S.Sridhar ...</td>\n",
              "      <td>Critical State Flux Penetration and Linear Mic...</td>\n",
              "      <td>The vortex contribution to the dc field (H) ...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Tue, 3 Sep 1996...</td>\n",
              "      <td>9</td>\n",
              "      <td>235</td>\n",
              "      <td>54</td>\n",
              "      <td>[Critical, State, Flux, Penetration, and, Line...</td>\n",
              "      <td>[The, vortex, contribution, to, the, dc, field...</td>\n",
              "      <td>[critical, state, flux, penetration, linear, m...</td>\n",
              "      <td>[vortex, contribution, dc, field, h, dependenc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369016</th>\n",
              "      <td>supr-con cond-mat.supr-con</td>\n",
              "      <td>supr-con/9609003</td>\n",
              "      <td>Yasumasa Hasegawa (Himeji Institute of Technol...</td>\n",
              "      <td>Density of States and NMR Relaxation Rate in A...</td>\n",
              "      <td>We show that the density of states in an ani...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Wed, 18 Sep 199...</td>\n",
              "      <td>9</td>\n",
              "      <td>237</td>\n",
              "      <td>54</td>\n",
              "      <td>[Density, of, States, and, NMR, Relaxation, Ra...</td>\n",
              "      <td>[We, show, that, the, density, of, states, in,...</td>\n",
              "      <td>[density, state, nmr, relaxation, rate, anisot...</td>\n",
              "      <td>[show, density, state, anisotropy, superconduc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369017</th>\n",
              "      <td>supr-con cond-mat.supr-con</td>\n",
              "      <td>supr-con/9609004</td>\n",
              "      <td>Naoki Enomoto, Masanori Ichioka and Kazushige ...</td>\n",
              "      <td>Ginzburg Landau theory for d-wave pairing and ...</td>\n",
              "      <td>The Ginzburg Landau theory for d_{x^2-y^2}-w...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Wed, 25 Sep 199...</td>\n",
              "      <td>9</td>\n",
              "      <td>238</td>\n",
              "      <td>54</td>\n",
              "      <td>[Ginzburg, Landau, theory, for, d, wave, pairi...</td>\n",
              "      <td>[The, Ginzburg, Landau, theory, for, d, x, y, ...</td>\n",
              "      <td>[ginzburg, landau, theory, d, wave, pairing, f...</td>\n",
              "      <td>[ginzburg, landau, theory, d, x, y, wave, supe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>368030 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                categories                id  \\\n",
              "0                        cond-mat.mes-hall         0704.0006   \n",
              "1                        cond-mat.mtrl-sci         0704.0008   \n",
              "2       cond-mat.str-el cond-mat.stat-mech         0704.0025   \n",
              "3                        cond-mat.mes-hall         0704.0027   \n",
              "4                          cond-mat.str-el         0704.0030   \n",
              "...                                    ...               ...   \n",
              "369013          supr-con cond-mat.supr-con  supr-con/9608008   \n",
              "369014          supr-con cond-mat.supr-con  supr-con/9609001   \n",
              "369015          supr-con cond-mat.supr-con  supr-con/9609002   \n",
              "369016          supr-con cond-mat.supr-con  supr-con/9609003   \n",
              "369017          supr-con cond-mat.supr-con  supr-con/9609004   \n",
              "\n",
              "                                                  authors  \\\n",
              "0                                Y. H. Pong and C. K. Law   \n",
              "1                                         Damian C. Swift   \n",
              "2       A. S. Mishchenko (1 and 2) and N. Nagaosa (1 a...   \n",
              "3       M. O. Goerbig, J.-N. Fuchs, K. Kechedzhi, Vlad...   \n",
              "4                            J.P.Hague and N.d'Ambrumenil   \n",
              "...                                                   ...   \n",
              "369013  R. Prozorov, M. Konczykowski, B. Schmidt, Y. Y...   \n",
              "369014  Durga P. Choudhury, Balam A. Willemsen, John S...   \n",
              "369015  Balam A. Willemsen, J. S. Derov and S.Sridhar ...   \n",
              "369016  Yasumasa Hasegawa (Himeji Institute of Technol...   \n",
              "369017  Naoki Enomoto, Masanori Ichioka and Kazushige ...   \n",
              "\n",
              "                                                    title  \\\n",
              "0       Bosonic characters of atomic Cooper pairs acro...   \n",
              "1       Numerical solution of shock and ramp compressi...   \n",
              "2       Spectroscopic Properties of Polarons in Strong...   \n",
              "3       Filling-Factor-Dependent Magnetophonon Resonan...   \n",
              "4       Tuning correlation effects with electron-phono...   \n",
              "...                                                   ...   \n",
              "369013  On the origin of the irreversibility line in t...   \n",
              "369014  Nonlinear Response of HTSC Thin Film Microwave...   \n",
              "369015  Critical State Flux Penetration and Linear Mic...   \n",
              "369016  Density of States and NMR Relaxation Rate in A...   \n",
              "369017  Ginzburg Landau theory for d-wave pairing and ...   \n",
              "\n",
              "                                                 abstract  \\\n",
              "0         We study the two-particle wave function of p...   \n",
              "1         A general formulation was developed to repre...   \n",
              "2         We present recent advances in understanding ...   \n",
              "3         We describe a peculiar fine structure acquir...   \n",
              "4         We investigate the effect of tuning the phon...   \n",
              "...                                                   ...   \n",
              "369013    We report on measurements of the angular dep...   \n",
              "369014    The non-linear microwave surface impedance o...   \n",
              "369015    The vortex contribution to the dc field (H) ...   \n",
              "369016    We show that the density of states in an ani...   \n",
              "369017    The Ginzburg Landau theory for d_{x^2-y^2}-w...   \n",
              "\n",
              "                                                 versions  \\\n",
              "0       [{'version': 'v1', 'created': 'Sat, 31 Mar 200...   \n",
              "1       [{'version': 'v1', 'created': 'Sat, 31 Mar 200...   \n",
              "2       [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...   \n",
              "3       [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...   \n",
              "4       [{'version': 'v1', 'created': 'Sat, 31 Mar 200...   \n",
              "...                                                   ...   \n",
              "369013  [{'version': 'v1', 'created': 'Mon, 26 Aug 199...   \n",
              "369014  [{'version': 'v1', 'created': 'Sat, 31 Aug 199...   \n",
              "369015  [{'version': 'v1', 'created': 'Tue, 3 Sep 1996...   \n",
              "369016  [{'version': 'v1', 'created': 'Wed, 18 Sep 199...   \n",
              "369017  [{'version': 'v1', 'created': 'Wed, 25 Sep 199...   \n",
              "\n",
              "        semesters_since_March_1992  weeks_since_March_1992  \\\n",
              "0                               30                     786   \n",
              "1                               30                     786   \n",
              "2                               30                     787   \n",
              "3                               30                     787   \n",
              "4                               30                     786   \n",
              "...                            ...                     ...   \n",
              "369013                           9                     234   \n",
              "369014                           9                     234   \n",
              "369015                           9                     235   \n",
              "369016                           9                     237   \n",
              "369017                           9                     238   \n",
              "\n",
              "        months_since_March_1992  \\\n",
              "0                           180   \n",
              "1                           180   \n",
              "2                           181   \n",
              "3                           181   \n",
              "4                           180   \n",
              "...                         ...   \n",
              "369013                       53   \n",
              "369014                       53   \n",
              "369015                       54   \n",
              "369016                       54   \n",
              "369017                       54   \n",
              "\n",
              "                                          title_tokenized  \\\n",
              "0       [Bosonic, characters, of, atomic, Cooper, pair...   \n",
              "1       [Numerical, solution, of, shock, and, ramp, co...   \n",
              "2       [Spectroscopic, Properties, of, Polarons, in, ...   \n",
              "3       [Filling, Factor, Dependent, Magnetophonon, Re...   \n",
              "4       [Tuning, correlation, effects, with, electron,...   \n",
              "...                                                   ...   \n",
              "369013  [On, the, origin, of, the, irreversibility, li...   \n",
              "369014  [Nonlinear, Response, of, HTSC, Thin, Film, Mi...   \n",
              "369015  [Critical, State, Flux, Penetration, and, Line...   \n",
              "369016  [Density, of, States, and, NMR, Relaxation, Ra...   \n",
              "369017  [Ginzburg, Landau, theory, for, d, wave, pairi...   \n",
              "\n",
              "                                       abstract_tokenized  \\\n",
              "0       [We, study, the, two, particle, wave, function...   \n",
              "1       [A, general, formulation, was, developed, to, ...   \n",
              "2       [We, present, recent, advances, in, understand...   \n",
              "3       [We, describe, a, peculiar, fine, structure, a...   \n",
              "4       [We, investigate, the, effect, of, tuning, the...   \n",
              "...                                                   ...   \n",
              "369013  [We, report, on, measurements, of, the, angula...   \n",
              "369014  [The, non, linear, microwave, surface, impedan...   \n",
              "369015  [The, vortex, contribution, to, the, dc, field...   \n",
              "369016  [We, show, that, the, density, of, states, in,...   \n",
              "369017  [The, Ginzburg, Landau, theory, for, d, x, y, ...   \n",
              "\n",
              "                                      title_base_replaced  \\\n",
              "0       [bosonic, characterized, atomic, cooper, pairs...   \n",
              "1       [numerical, solution, shock, ramp, compression...   \n",
              "2       [spectroscopy, properties, polarization, stron...   \n",
              "3       [filling, factor, dependence, magnetophonon, r...   \n",
              "4       [tuning, correlation, effect, electron, phonon...   \n",
              "...                                                   ...   \n",
              "369013  [origin, irreversible, line, thin, ybacuo7, fi...   \n",
              "369014  [nonlinear, response, htsc, thin, film, microw...   \n",
              "369015  [critical, state, flux, penetration, linear, m...   \n",
              "369016  [density, state, nmr, relaxation, rate, anisot...   \n",
              "369017  [ginzburg, landau, theory, d, wave, pairing, f...   \n",
              "\n",
              "                                   abstract_base_replaced  \n",
              "0       [study, two, particle, wave, function, paired,...  \n",
              "1       [general, formula, was, developed, representat...  \n",
              "2       [present, recent, advanced, under, ground, exc...  \n",
              "3       [described, peculiar, fine, structure, acquire...  \n",
              "4       [investigate, effect, tuning, phonon, energy, ...  \n",
              "...                                                   ...  \n",
              "369013  [report, measurements, angular, dependence, ir...  \n",
              "369014  [non, linear, microwave, surface, impedance, p...  \n",
              "369015  [vortex, contribution, dc, field, h, dependenc...  \n",
              "369016  [show, density, state, anisotropy, superconduc...  \n",
              "369017  [ginzburg, landau, theory, d, x, y, wave, supe...  \n",
              "\n",
              "[368030 rows x 13 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cond_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "239b7406-13a6-4660-80d4-572f1061ba50",
      "metadata": {
        "id": "239b7406-13a6-4660-80d4-572f1061ba50"
      },
      "source": [
        "We can now perform a count vectorisation of the modified text. In addition to individual tokens, we can include bigrams and trigrams to capture terms consisting of several words, e.g. \"quantum computing\" or \"machine learning.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f86f702-474d-4234-8698-7457c8465a14",
      "metadata": {
        "id": "1f86f702-474d-4234-8698-7457c8465a14",
        "outputId": "6fbac923-9597-41c6-92b6-55fc268d62d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created new count vectorizer with vocabulary size = 28875927\n",
            "CPU times: user 2min 58s, sys: 8.47 s, total: 3min 7s\n",
            "Wall time: 3min 6s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "count_vectorizer = CountVectorizer(preprocessor= lambda x:x, tokenizer=lambda x:x, ngram_range=(1,3), binary=True, token_pattern=None, lowercase=False)\n",
        "vectorizer_train = pd.concat([cond_mat['abstract_base_replaced'], cond_mat['title_base_replaced']])\n",
        "vectorized = count_vectorizer.fit_transform(vectorizer_train)\n",
        "document_frequencies =  np.ravel(vectorized.sum(axis=0))\n",
        "\n",
        "vocabulary = count_vectorizer.get_feature_names_out()\n",
        "vocab_size = len(vocabulary)\n",
        "print(f\"Created new count vectorizer with vocabulary size = {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b35b7b4c-2c46-42cc-aed0-f063672a4ac9",
      "metadata": {
        "id": "b35b7b4c-2c46-42cc-aed0-f063672a4ac9"
      },
      "source": [
        "The count vectorizer can give us the number of documents in which each term has appeared:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc4759df-4116-4cc8-8ba7-0536912f4d42",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "cc4759df-4116-4cc8-8ba7-0536912f4d42",
        "outputId": "6159b347-7c47-4edc-f13a-0fd8e1c3354a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 8.78 s, sys: 161 ms, total: 8.94 s\n",
            "Wall time: 8.94 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "term_indices_ranked_by_frequency = np.argsort(document_frequencies)\n",
        "terms_ranked_by_frequency = [vocabulary[term_indices_ranked_by_frequency[vocab_size - 1 - i]] for i in range(vocab_size)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75cf746f-1648-4be5-8373-ed351ce9de7f",
      "metadata": {
        "id": "75cf746f-1648-4be5-8373-ed351ce9de7f",
        "outputId": "94932d7a-7ece-499f-cd58-06d311a39821"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['model', 'system', 'state', 'study', 'two', 'effect', 'quantum', 'results', 'field', 'electron', 'magnetic', 'phase', 'spin', 'between', 'temperature', 'dimensional', 'show', 'interaction', 'structure', 'transition', 'energy', 'dynamics', 'using', 'function', 'properties', 'it', 'these', 'dependence', 'order', 'experimental', 'theory', 'different', 'one', 'also', 'observed', 'density', 'applied', 'coupling', 'under', 'non', 'calculations', 'or', 'have', 'high', 'based', 'strong', 'time', 'superconducting', 'present', 'investigate', 'such', 'single', 'measurements', 'has', 'both', 'lattice', 'simulations', 'low', 'related', 'local', 'here', 'well', 'method', 'correlation', 'find', 'characterized', 'materials', 'parameters', 'not', 'induced', 'demonstrate', 'large', 'scale', 'approach', 'first', 'its', 'fermions', 's', 'surface', 'obtained', 'potential', 'provide', 'their', 'critical', 'symmetry', 'been', 'range', 'mechanism', 'limit', 'point', 'compared', 'crystal', 'general', 'due', 'consider', 'metal', 'charge', 'particle', 'behavior', 'into']\n"
          ]
        }
      ],
      "source": [
        "print(terms_ranked_by_frequency[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f061530-cfe0-43de-ab20-eb82038d2d5f",
      "metadata": {
        "id": "1f061530-cfe0-43de-ab20-eb82038d2d5f"
      },
      "outputs": [],
      "source": [
        "Y = [document_frequencies[term_indices_ranked_by_frequency[i]] for i in range(vocab_size)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cef290b-0e54-4d25-9b11-396e68d7bfbe",
      "metadata": {
        "id": "0cef290b-0e54-4d25-9b11-396e68d7bfbe"
      },
      "outputs": [],
      "source": [
        "X = [vocab_size - i for i in range(vocab_size)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2acfebe-32b2-4006-875a-ded4f6f34543",
      "metadata": {
        "id": "c2acfebe-32b2-4006-875a-ded4f6f34543",
        "outputId": "66ecf51e-0646-4962-e01d-f7882de3872e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1600x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "sns.scatterplot(x=X, y=Y)\n",
        "ax.set_xscale('log')\n",
        "ax.set_yscale('log')\n",
        "fig.suptitle('Number of documents containing a term')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0113d9d1-736a-4cb5-af3e-d60d15b6c97c",
      "metadata": {
        "id": "0113d9d1-736a-4cb5-af3e-d60d15b6c97c"
      },
      "source": [
        "We can see that, of the 5.6 million terms in the vocabulary, less than a million appear in at least 10 documents. We'll create a new vocabulary which only includes terms that occur in at least 10 documents, which allows us to reduce the vocabulary size substantially:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c5d5e9d-3d31-421a-bb6e-e4d0ea1b225f",
      "metadata": {
        "id": "0c5d5e9d-3d31-421a-bb6e-e4d0ea1b225f",
        "outputId": "004e9611-2f93-4b20-9ca6-3a62dddfecc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "669306"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_vocabulary = [vocabulary[i] for i in range(vocab_size) if document_frequencies[i] > 10]\n",
        "len(new_vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23138d26-a2f7-49e1-aa2a-368c1dba8116",
      "metadata": {
        "id": "23138d26-a2f7-49e1-aa2a-368c1dba8116",
        "outputId": "8878d7a7-d6df-42c7-bab3-e0d085d3e74e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['2d optical lattice', '2d or', '2d or 3d', '2d order', '2d organic', '2d organic superconducting', '2d p', '2d pancake', '2d parabolic', '2d particle', '2d patterns', '2d peak', '2d percolation', '2d periodic', '2d perovskite', '2d perovskites', '2d phase', '2d phase transition', '2d phonon', '2d photon', '2d photon crystal', '2d physical', '2d planar', '2d plane', '2d plasmon', '2d platform', '2d point', '2d poisson', '2d polarization', '2d polymer', '2d potential', '2d potts', '2d potts model', '2d problem', '2d projection', '2d q', '2d q state', '2d quantum', '2d quantum antiferromagnetic', '2d quantum critical', '2d quantum dot', '2d quantum gravity', '2d quantum hall', '2d quantum heisenberg', '2d quantum ising', '2d quantum magnetic', '2d quantum materials', '2d quantum spin', '2d quantum system', '2d quantum turbulence', '2d quantum wells', '2d quasi', '2d quasi 1d', '2d quasi 2d', '2d raman', '2d random', '2d random bond', '2d random field', '2d rashba', '2d rectangular', '2d regime', '2d resistance', '2d resonance', '2d results', '2d ring', '2d rotation', '2d ruddlesden', '2d ruddlesden popper', '2d s', '2d sample', '2d sc', '2d scale', '2d scattering', '2d semi', '2d semiconductor', '2d semiconductor have', '2d semiconductor heterostructures', '2d semiconductor materials', '2d semiconductor structure', '2d semiconductor such', '2d semiconductor system', '2d semiconductor transition', '2d semimetal', '2d sheet', '2d show', '2d silicon', '2d simulations', '2d single', '2d single layer', '2d skyrmion', '2d so', '2d soft', '2d solid', '2d solitons', '2d space', '2d spatial', '2d spectra', '2d spectroscopy', '2d spin', '2d spin glass']\n"
          ]
        }
      ],
      "source": [
        "print(new_vocabulary[1000:1100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82d5ad22-1d8b-42c3-bccd-e6f1837dab44",
      "metadata": {
        "id": "82d5ad22-1d8b-42c3-bccd-e6f1837dab44"
      },
      "outputs": [],
      "source": [
        "vocabulary = new_vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9d2b076-8d72-4e25-94eb-118c1e5ccd9f",
      "metadata": {
        "id": "b9d2b076-8d72-4e25-94eb-118c1e5ccd9f"
      },
      "source": [
        "### Time series analysis\n",
        "\n",
        "Now that we have our new vectorizer vocabulary, we are going to calculate statistics for each feature by month and semester. I'll first calculate the document frequencies of each feature grouped by time period. For a number of years, the number of monthly publications was too low to perform a statistical analysis. Let's look at the number of monthly publications in January 2000:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8544b051-3242-4d84-8430-6857cbda096c",
      "metadata": {
        "id": "8544b051-3242-4d84-8430-6857cbda096c",
        "outputId": "e47a45ac-bd4d-47af-e334-c0b389742cab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 506 publications in January 2000.\n"
          ]
        }
      ],
      "source": [
        "earliest_month = (2000 - 1992)*12 + 1-3\n",
        "print(f\"Found {len(cond_mat[cond_mat['months_since_March_1992']==earliest_month])} publications in January 2000.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f437cdde-131a-4295-9a30-f434cdd0d35c",
      "metadata": {
        "id": "f437cdde-131a-4295-9a30-f434cdd0d35c"
      },
      "source": [
        "We will choose January 2000 as the start range for analysing historical trends in the cond-mat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f5c830d-9f1f-4a3e-a2b0-0bfe924ff8b8",
      "metadata": {
        "id": "9f5c830d-9f1f-4a3e-a2b0-0bfe924ff8b8"
      },
      "outputs": [],
      "source": [
        "latest_month = 385\n",
        "monthly_publication_numbers = [len(cond_mat[cond_mat['months_since_March_1992'] == month]) for month in range(earliest_month,latest_month)]\n",
        "earliest_semester = 100//6\n",
        "latest_semester = 385//6\n",
        "biannual_publication_numbers = [len(cond_mat[cond_mat['semesters_since_March_1992']== semester]) for semester in range(earliest_semester,latest_semester)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01548e14-5bf8-473b-91c8-a72871662753",
      "metadata": {
        "id": "01548e14-5bf8-473b-91c8-a72871662753"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "def get_document_frequencies(texts):\n",
        "    vectorizer = CountVectorizer(preprocessor= lambda x:x, tokenizer=lambda x:x, ngram_range = (1,3), binary=True, token_pattern=None, vocabulary=vocabulary, lowercase=False)\n",
        "    document_frequencies = (np.ravel(vectorizer.fit_transform(texts).sum(axis=0))+1)/len(texts) # add 1 to avoid divide by zero errors later\n",
        "    del(vectorizer)\n",
        "    gc.collect()\n",
        "    return document_frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c733c8b9-521e-4720-a7e6-ef65037452bc",
      "metadata": {
        "id": "c733c8b9-521e-4720-a7e6-ef65037452bc",
        "outputId": "4d75eecf-be5a-4671-f933-d8fcb16c517f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████| 291/291 [12:41<00:00,  2.62s/it]\n"
          ]
        }
      ],
      "source": [
        "monthly_titles_df = np.array([get_document_frequencies(cond_mat[cond_mat['months_since_March_1992']==month]['title_base_replaced']) for month in tqdm(range(earliest_month, latest_month))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "628db8e6-3bc8-488a-bdb2-0efc7b8951f3",
      "metadata": {
        "id": "628db8e6-3bc8-488a-bdb2-0efc7b8951f3",
        "outputId": "b99c88e7-777b-433f-e9d0-0d73b8409d09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████| 291/291 [13:18<00:00,  2.74s/it]\n"
          ]
        }
      ],
      "source": [
        "monthly_abstracts_df = np.array([get_document_frequencies(cond_mat[cond_mat['months_since_March_1992']==month]['abstract_base_replaced']) for month in tqdm(range(earliest_month, latest_month))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f26ed867-a9cf-4f38-bcfc-299b84d1cab9",
      "metadata": {
        "id": "f26ed867-a9cf-4f38-bcfc-299b84d1cab9",
        "outputId": "f5de0fe0-98c5-4b99-a375-e81e897b8c98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████| 48/48 [02:10<00:00,  2.71s/it]\n"
          ]
        }
      ],
      "source": [
        "semesterly_titles_df = np.array([get_document_frequencies(cond_mat[cond_mat['semesters_since_March_1992']==semester]['title_base_replaced']) for semester in tqdm(range(earliest_semester, latest_semester))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "540c1643-fe58-4db1-824f-26fe608d0ffe",
      "metadata": {
        "id": "540c1643-fe58-4db1-824f-26fe608d0ffe",
        "outputId": "d040255b-fddb-4833-ef7d-35cfe74b0a91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████| 48/48 [02:46<00:00,  3.47s/it]\n"
          ]
        }
      ],
      "source": [
        "semesterly_abstracts_df = np.array([get_document_frequencies(cond_mat[cond_mat['semesters_since_March_1992']==semester]['abstract_base_replaced']) for semester in tqdm(range(earliest_semester, latest_semester))])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our time series data, we can look at the maximum and minimum values of the document frequencies in the abstract and title, counted monthly and semesterly, over the period since January 2000:"
      ],
      "metadata": {
        "id": "T8698P4o9rYn"
      },
      "id": "T8698P4o9rYn"
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "65768887-9bb5-4e4d-96a5-3cda5875041d",
      "metadata": {
        "scrolled": true,
        "id": "65768887-9bb5-4e4d-96a5-3cda5875041d"
      },
      "outputs": [],
      "source": [
        "df_data = pd.DataFrame()\n",
        "df_data['term'] = vocabulary\n",
        "df_data['max_monthly_abstract_df'] = monthly_abstracts_df.max(axis=0)\n",
        "df_data['min_monthly_abstract_df'] = monthly_abstracts_df.min(axis=0)\n",
        "df_data['max_monthly_title_df'] = monthly_titles_df.max(axis=0)\n",
        "df_data['min_monthly_title_df'] = monthly_titles_df.min(axis=0)\n",
        "df_data['min_semesterly_abstract_df'] = semesterly_abstracts_df.min(axis=0)\n",
        "df_data['max_semesterly_abstract_df'] = semesterly_abstracts_df.max(axis=0)\n",
        "df_data['min_semesterly_title_df'] = semesterly_titles_df.min(axis=0)\n",
        "df_data['max_semesterly_title_df'] = semesterly_titles_df.max(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the terms that have occurred most frequently in the title for some six-month period:"
      ],
      "metadata": {
        "id": "fJlpVOxX-Kfj"
      },
      "id": "fJlpVOxX-Kfj"
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.sort_values('max_semesterly_title_df', ascending=False).head(200)"
      ],
      "metadata": {
        "id": "toiOFVD7-LkR",
        "outputId": "bea71ad2-b264-40fe-9dbd-41fe4b1ffa72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "toiOFVD7-LkR",
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            term  max_monthly_abstract_df  \\\n",
              "470000                   quantum                 0.304857   \n",
              "363672                     model                 0.400000   \n",
              "572860           superconducting                 0.207018   \n",
              "545060                      spin                 0.253118   \n",
              "339603                  magnetic                 0.286458   \n",
              "171649                    effect                 0.344311   \n",
              "177772                  electron                 0.322727   \n",
              "148096               dimensional                 0.265378   \n",
              "431641                     phase                 0.311976   \n",
              "581391                    system                 0.429949   \n",
              "552448                     state                 0.401413   \n",
              "254798                  graphene                 0.096935   \n",
              "217650                     field                 0.324774   \n",
              "166646                  dynamics                 0.259036   \n",
              "617153                transition                 0.251664   \n",
              "613300               topological                 0.165800   \n",
              "625254                       two                 0.383643   \n",
              "598345                    theory                 0.245714   \n",
              "562725                 structure                 0.289587   \n",
              "293613               interaction                 0.272303   \n",
              "319894                   lattice                 0.176149   \n",
              "57362                       bose                 0.072039   \n",
              "565058                     study                 0.408824   \n",
              "214271                  fermions                 0.149269   \n",
              "388856                       non                 0.237646   \n",
              "411269                     order                 0.260417   \n",
              "286248                   induced                 0.161248   \n",
              "113900                  coupling                 0.223912   \n",
              "95807                 condensate                 0.084630   \n",
              "588769               temperature                 0.326892   \n",
              "530583                    single                 0.199349   \n",
              "460732                properties                 0.297605   \n",
              "620038                 transport                 0.123399   \n",
              "356225                     metal                 0.141848   \n",
              "119188                   crystal                 0.152033   \n",
              "116929                  critical                 0.166359   \n",
              "576695                   surface                 0.163150   \n",
              "625862           two dimensional                 0.124907   \n",
              "383990                   network                 0.082732   \n",
              "110424               correlation                 0.171481   \n",
              "291685                 insulator                 0.104665   \n",
              "268471                      high                 0.232991   \n",
              "406971                   optical                 0.120430   \n",
              "78834                     charge                 0.142534   \n",
              "375687                     multi                 0.141343   \n",
              "175898                  einstein                 0.061050   \n",
              "240069                  function                 0.269294   \n",
              "57383              bose einstein                 0.054945   \n",
              "199483               excitations                 0.145130   \n",
              "475693                    random                 0.133803   \n",
              "664553                         x                 0.109467   \n",
              "175899       einstein condensate                 0.052503   \n",
              "57384   bose einstein condensate                 0.052503   \n",
              "347238                 materials                 0.267216   \n",
              "42563                      based                 0.264249   \n",
              "331710                    liquid                 0.103873   \n",
              "333092                     local                 0.185207   \n",
              "154452                  disorder                 0.097606   \n",
              "216227             ferromagnetic                 0.092000   \n",
              "252512                     glass                 0.066869   \n",
              "340366            magnetic field                 0.135531   \n",
              "131582                   density                 0.228639   \n",
              "227781              fluctuations                 0.122857   \n",
              "579495                  symmetry                 0.174902   \n",
              "97275               conductivity                 0.127747   \n",
              "369657                 molecular                 0.110104   \n",
              "359326                      mgb2                 0.053140   \n",
              "184547                    energy                 0.280183   \n",
              "638237                     using                 0.313472   \n",
              "402830                       one                 0.245448   \n",
              "509507                     scale                 0.166779   \n",
              "410314                     orbit                 0.106159   \n",
              "652417                      wave                 0.119008   \n",
              "494631                 resonance                 0.099746   \n",
              "163485                    driven                 0.121597   \n",
              "324559                  learning                 0.055689   \n",
              "133227                dependence                 0.281184   \n",
              "259637                      hall                 0.070896   \n",
              "74776                      chain                 0.079190   \n",
              "601242                   thermal                 0.107256   \n",
              "121454                   current                 0.125277   \n",
              "443669              polarization                 0.108564   \n",
              "221611                     films                 0.058716   \n",
              "24816          antiferromagnetic                 0.096118   \n",
              "649853                    vortex                 0.061192   \n",
              "246032                       gas                 0.074502   \n",
              "403096           one dimensional                 0.095238   \n",
              "561252                    strong                 0.211509   \n",
              "528846               simulations                 0.230289   \n",
              "610020                      time                 0.212486   \n",
              "424002                  particle                 0.144181   \n",
              "123440                         d                 0.116412   \n",
              "624264                 tunneling                 0.081031   \n",
              "353709                 mechanism                 0.197006   \n",
              "472777                     quasi                 0.095714   \n",
              "511010                scattering                 0.119005   \n",
              "434364          phase transition                 0.097547   \n",
              "107338                   control                 0.138257   \n",
              "556797               statistical                 0.084302   \n",
              "28462                   approach                 0.193757   \n",
              "321990                     layer                 0.130932   \n",
              "647191                       via                 0.104651   \n",
              "50537                    between                 0.330394   \n",
              "605781                      thin                 0.068611   \n",
              "7337                      active                 0.080588   \n",
              "121155                  cuprates                 0.054386   \n",
              "160957                     doped                 0.054628   \n",
              "59730                   boundary                 0.129012   \n",
              "22619                 anisotropy                 0.086181   \n",
              "436150                    phonon                 0.076230   \n",
              "311087                  junction                 0.056034   \n",
              "147138                 diffusion                 0.074713   \n",
              "336305                       low                 0.185945   \n",
              "630917                     under                 0.315665   \n",
              "514544                      self                 0.085648   \n",
              "162280                      dots                 0.037775   \n",
              "296852                 interface                 0.078086   \n",
              "396876                         o                 0.067769   \n",
              "515333             semiconductor                 0.067974   \n",
              "197289                     exact                 0.125954   \n",
              "224426                    finite                 0.147664   \n",
              "351906              measurements                 0.205567   \n",
              "23788                  anomalous                 0.066510   \n",
              "157686              distribution                 0.141369   \n",
              "470722              quantum dots                 0.034627   \n",
              "543378              spectroscopy                 0.081004   \n",
              "546786                spin orbit                 0.065758   \n",
              "80772                     chiral                 0.054765   \n",
              "515852                 semimetal                 0.037464   \n",
              "442317                     point                 0.159298   \n",
              "379981                 nanotubes                 0.035264   \n",
              "445139                   polymer                 0.043893   \n",
              "471204              quantum hall                 0.034783   \n",
              "47236                   behavior                 0.154623   \n",
              "447672                 potential                 0.202994   \n",
              "505988                         s                 0.166667   \n",
              "54284                    bilayer                 0.044785   \n",
              "415214              oscillations                 0.085253   \n",
              "338981                   machine                 0.046707   \n",
              "69538                     carbon                 0.045340   \n",
              "190806                  equation                 0.138922   \n",
              "608072                   through                 0.174323   \n",
              "613674     topological insulator                 0.040268   \n",
              "542628                   spectra                 0.091969   \n",
              "187258                  enhanced                 0.111307   \n",
              "56030                       body                 0.089798   \n",
              "40461                       band                 0.126374   \n",
              "488269                relaxation                 0.082437   \n",
              "225288                     first                 0.177764   \n",
              "188550              entanglement                 0.050907   \n",
              "161908                       dot                 0.048913   \n",
              "305122                      iron                 0.036145   \n",
              "281402                  impurity                 0.059150   \n",
              "89015                    comment                 0.024730   \n",
              "212869                        fe                 0.059081   \n",
              "345427                      many                 0.134069   \n",
              "359315                       mgb                 0.042735   \n",
              "305495                     ising                 0.053286   \n",
              "151140                     dirac                 0.055019   \n",
              "454043                  pressure                 0.057681   \n",
              "603                           2d                 0.078843   \n",
              "602145             thermodynamic                 0.077571   \n",
              "162525                    double                 0.056462   \n",
              "470528               quantum dot                 0.034884   \n",
              "371771                 monolayer                 0.052239   \n",
              "254679                  granular                 0.036626   \n",
              "91326                    complex                 0.116366   \n",
              "26813                    applied                 0.298107   \n",
              "274294                   hubbard                 0.045775   \n",
              "57517                    bosonic                 0.057797   \n",
              "606952                     three                 0.127907   \n",
              "84302                    cluster                 0.061538   \n",
              "584901                         t                 0.175523   \n",
              "575103                superfluid                 0.047149   \n",
              "191949               equilibrium                 0.090000   \n",
              "388319                     noise                 0.046985   \n",
              "66248               calculations                 0.247036   \n",
              "537911                  solution                 0.093913   \n",
              "437446                    photon                 0.059723   \n",
              "338994          machine learning                 0.041916   \n",
              "69636           carbon nanotubes                 0.034005   \n",
              "189307                   entropy                 0.063017   \n",
              "455301                principles                 0.094849   \n",
              "127192                   defects                 0.061632   \n",
              "456325                     probe                 0.068109   \n",
              "390945                 nonlinear                 0.058140   \n",
              "559091                    strain                 0.059758   \n",
              "486692                   related                 0.201739   \n",
              "345464                 many body                 0.074389   \n",
              "357537                    method                 0.201796   \n",
              "561036                    stripe                 0.035088   \n",
              "442211                 pnictides                 0.027563   \n",
              "310720                 josephson                 0.036364   \n",
              "182129                 emergence                 0.139760   \n",
              "14955                     alloys                 0.043004   \n",
              "247932                   general                 0.152850   \n",
              "120524                        cu                 0.054545   \n",
              "397658                  observed                 0.259233   \n",
              "317444                     large                 0.180777   \n",
              "657402                      weyl                 0.031700   \n",
              "\n",
              "        min_monthly_abstract_df  max_monthly_title_df  min_monthly_title_df  \\\n",
              "470000                 0.149096              0.152326              0.086093   \n",
              "363672                 0.272823              0.147708              0.062338   \n",
              "572860                 0.083924              0.164251              0.045223   \n",
              "545060                 0.144762              0.123958              0.041905   \n",
              "339603                 0.188571              0.122318              0.069930   \n",
              "171649                 0.210721              0.104167              0.054711   \n",
              "177772                 0.176570              0.109783              0.047280   \n",
              "148096                 0.162928              0.100935              0.040570   \n",
              "431641                 0.195157              0.104497              0.049236   \n",
              "581391                 0.268934              0.095420              0.039423   \n",
              "552448                 0.264052              0.092421              0.036514   \n",
              "254798                 0.001109              0.085336              0.001078   \n",
              "217650                 0.220899              0.090000              0.038255   \n",
              "166646                 0.130568              0.088790              0.043810   \n",
              "617153                 0.154545              0.091787              0.041599   \n",
              "613300                 0.016822              0.075839              0.003326   \n",
              "625254                 0.269421              0.080153              0.029762   \n",
              "598345                 0.148982              0.083810              0.024829   \n",
              "562725                 0.124521              0.069034              0.024226   \n",
              "293613                 0.170093              0.065141              0.027097   \n",
              "319894                 0.096831              0.063542              0.021053   \n",
              "57362                  0.017943              0.064336              0.009679   \n",
              "565058                 0.282456              0.056673              0.014763   \n",
              "214271                 0.069421              0.057411              0.018135   \n",
              "388856                 0.109501              0.051961              0.017143   \n",
              "411269                 0.173963              0.052724              0.015116   \n",
              "286248                 0.059593              0.050426              0.013834   \n",
              "113900                 0.122241              0.058442              0.012000   \n",
              "95807                  0.036830              0.055453              0.008103   \n",
              "588769                 0.218025              0.061192              0.018041   \n",
              "530583                 0.095070              0.054187              0.012704   \n",
              "460732                 0.133978              0.062215              0.021314   \n",
              "620038                 0.040665              0.058050              0.011050   \n",
              "356225                 0.055556              0.047330              0.013245   \n",
              "119188                 0.041739              0.045643              0.011628   \n",
              "116929                 0.091677              0.049407              0.011152   \n",
              "576695                 0.053286              0.048762              0.007728   \n",
              "625862                 0.054371              0.053435              0.014137   \n",
              "383990                 0.017606              0.050465              0.007018   \n",
              "110424                 0.097074              0.049251              0.018257   \n",
              "291685                 0.030418              0.046335              0.007112   \n",
              "268471                 0.089376              0.057971              0.016336   \n",
              "406971                 0.026643              0.048587              0.005272   \n",
              "78834                  0.077108              0.048571              0.014646   \n",
              "375687                 0.041502              0.045064              0.007905   \n",
              "175898                 0.012422              0.044362              0.003778   \n",
              "240069                 0.193809              0.046154              0.012422   \n",
              "57383                  0.011042              0.044362              0.003778   \n",
              "199483                 0.064081              0.042735              0.011885   \n",
              "475693                 0.044669              0.049296              0.008922   \n",
              "664553                 0.050467              0.054371              0.010944   \n",
              "175899                 0.010172              0.044362              0.003778   \n",
              "57384                  0.010172              0.044362              0.003778   \n",
              "347238                 0.036398              0.043250              0.003221   \n",
              "42563                  0.082667              0.043530              0.003641   \n",
              "331710                 0.039669              0.041845              0.010417   \n",
              "333092                 0.088567              0.045840              0.010884   \n",
              "154452                 0.037513              0.053147              0.006431   \n",
              "216227                 0.044944              0.042824              0.009130   \n",
              "252512                 0.016337              0.042179              0.005917   \n",
              "340366                 0.073120              0.040665              0.005729   \n",
              "131582                 0.144366              0.041841              0.012431   \n",
              "227781                 0.060677              0.042857              0.008328   \n",
              "579495                 0.066345              0.038717              0.004121   \n",
              "97275                  0.068702              0.040179              0.011095   \n",
              "369657                 0.035120              0.041801              0.009346   \n",
              "359326                 0.000505              0.053140              0.000505   \n",
              "184547                 0.189385              0.039258              0.011043   \n",
              "638237                 0.166384              0.035660              0.003040   \n",
              "402830                 0.162287              0.041322              0.008794   \n",
              "509507                 0.097561              0.041262              0.009615   \n",
              "410314                 0.027451              0.038462              0.007366   \n",
              "652417                 0.061911              0.037500              0.009272   \n",
              "494631                 0.036827              0.040791              0.005272   \n",
              "163485                 0.023256              0.036442              0.005282   \n",
              "324559                 0.000823              0.032204              0.000673   \n",
              "133227                 0.194737              0.041620              0.011429   \n",
              "259637                 0.015385              0.038462              0.008816   \n",
              "74776                  0.028634              0.036735              0.007947   \n",
              "601242                 0.036474              0.034483              0.007417   \n",
              "121454                 0.062847              0.034611              0.005282   \n",
              "443669                 0.033956              0.033040              0.004249   \n",
              "221611                 0.014060              0.041868              0.007030   \n",
              "24816                  0.036137              0.042514              0.006070   \n",
              "649853                 0.009422              0.035427              0.002706   \n",
              "246032                 0.027696              0.039829              0.006418   \n",
              "403096                 0.037164              0.036842              0.007063   \n",
              "561252                 0.114943              0.034158              0.007084   \n",
              "528846                 0.092522              0.035411              0.008876   \n",
              "610020                 0.116190              0.032110              0.010110   \n",
              "424002                 0.077844              0.033451              0.007186   \n",
              "123440                 0.039289              0.038817              0.002230   \n",
              "624264                 0.030120              0.033956              0.004450   \n",
              "353709                 0.070476              0.028956              0.006135   \n",
              "472777                 0.052427              0.037143              0.006608   \n",
              "511010                 0.057092              0.034483              0.008344   \n",
              "434364                 0.046099              0.034394              0.011450   \n",
              "107338                 0.022099              0.031274              0.001488   \n",
              "556797                 0.034004              0.033987              0.005705   \n",
              "28462                  0.083736              0.031116              0.008547   \n",
              "321990                 0.030560              0.031746              0.004854   \n",
              "647191                 0.022378              0.027340              0.001290   \n",
              "50537                  0.199605              0.031712              0.008547   \n",
              "605781                 0.017787              0.030541              0.005059   \n",
              "7337                   0.007105              0.028522              0.001122   \n",
              "121155                 0.006653              0.031480              0.001331   \n",
              "160957                 0.016420              0.027778              0.003284   \n",
              "59730                  0.043845              0.032919              0.002535   \n",
              "22619                  0.031972              0.032689              0.005217   \n",
              "436150                 0.021127              0.029977              0.005545   \n",
              "311087                 0.012882              0.033215              0.005312   \n",
              "147138                 0.033215              0.030534              0.006832   \n",
              "336305                 0.124046              0.033384              0.007982   \n",
              "630917                 0.109155              0.033577              0.007042   \n",
              "514544                 0.038732              0.032578              0.004559   \n",
              "162280                 0.006281              0.031169              0.003081   \n",
              "296852                 0.019324              0.025819              0.003610   \n",
              "396876                 0.020343              0.033549              0.004283   \n",
              "515333                 0.018895              0.030624              0.005525   \n",
              "197289                 0.050542              0.028626              0.003209   \n",
              "224426                 0.067143              0.031776              0.005291   \n",
              "351906                 0.112754              0.026490              0.004926   \n",
              "23788                  0.016490              0.027675              0.003534   \n",
              "157686                 0.065089              0.028419              0.003819   \n",
              "470722                 0.005653              0.029381              0.003081   \n",
              "543378                 0.011189              0.028470              0.003284   \n",
              "546786                 0.003738              0.028045              0.001754   \n",
              "80772                  0.001642              0.023810              0.001195   \n",
              "515852                 0.000895              0.026756              0.000806   \n",
              "442317                 0.082645              0.028881              0.006912   \n",
              "379981                 0.001766              0.033696              0.000589   \n",
              "445139                 0.013575              0.028169              0.003759   \n",
              "471204                 0.007345              0.028419              0.002423   \n",
              "47236                  0.089333              0.026490              0.004058   \n",
              "447672                 0.085714              0.025184              0.002833   \n",
              "505988                 0.101504              0.031776              0.004274   \n",
              "54284                  0.003396              0.026741              0.001908   \n",
              "415214                 0.035088              0.029677              0.005217   \n",
              "338981                 0.000672              0.021099              0.000672   \n",
              "69538                  0.001908              0.032609              0.002221   \n",
              "190806                 0.072267              0.033647              0.005495   \n",
              "608072                 0.049236              0.026265              0.004608   \n",
              "613674                 0.000986              0.026116              0.000914   \n",
              "542628                 0.041333              0.029557              0.003958   \n",
              "187258                 0.020333              0.024825              0.001399   \n",
              "56030                  0.017831              0.026420              0.001323   \n",
              "40461                  0.038750              0.023632              0.003552   \n",
              "488269                 0.033201              0.027826              0.003943   \n",
              "225288                 0.085648              0.023725              0.002692   \n",
              "188550                 0.003515              0.026042              0.001425   \n",
              "161908                 0.007156              0.023913              0.003769   \n",
              "305122                 0.001110              0.023346              0.001049   \n",
              "281402                 0.017365              0.022609              0.003029   \n",
              "89015                  0.001078              0.026408              0.000702   \n",
              "212869                 0.002762              0.026042              0.001328   \n",
              "345427                 0.036832              0.023190              0.002361   \n",
              "359315                 0.000505              0.031339              0.000505   \n",
              "305495                 0.015404              0.026455              0.004184   \n",
              "151140                 0.001167              0.021665              0.001104   \n",
              "454043                 0.011091              0.025717              0.001848   \n",
              "603                    0.016478              0.026718              0.003617   \n",
              "602145                 0.028261              0.031304              0.004651   \n",
              "162525                 0.021761              0.029644              0.001761   \n",
              "470528                 0.006086              0.022826              0.002513   \n",
              "371771                 0.001453              0.024902              0.001057   \n",
              "254679                 0.003534              0.033296              0.002282   \n",
              "91326                  0.025806              0.023307              0.003364   \n",
              "26813                  0.113027              0.025157              0.005019   \n",
              "274294                 0.015990              0.026273              0.003924   \n",
              "57517                  0.024431              0.023613              0.002912   \n",
              "606952                 0.057229              0.024969              0.002099   \n",
              "84302                  0.022430              0.027223              0.003578   \n",
              "584901                 0.073133              0.027668              0.001691   \n",
              "575103                 0.009881              0.026316              0.001976   \n",
              "191949                 0.036866              0.024882              0.002364   \n",
              "388319                 0.018764              0.025166              0.003975   \n",
              "66248                  0.153197              0.029565              0.002395   \n",
              "537911                 0.043860              0.022430              0.004637   \n",
              "437446                 0.003630              0.023256              0.001761   \n",
              "338994                 0.000672              0.018878              0.000672   \n",
              "69636                  0.001076              0.030435              0.000589   \n",
              "189307                 0.015581              0.022915              0.001109   \n",
              "455301                 0.017713              0.021352              0.003306   \n",
              "127192                 0.009242              0.020025              0.001905   \n",
              "456325                 0.007634              0.022539              0.001416   \n",
              "390945                 0.015581              0.022989              0.001869   \n",
              "559091                 0.005272              0.020915              0.001221   \n",
              "486692                 0.106569              0.021223              0.004844   \n",
              "345464                 0.008489              0.020476              0.001078   \n",
              "357537                 0.105360              0.023601              0.003810   \n",
              "561036                 0.005690              0.022857              0.000583   \n",
              "442211                 0.000526              0.022051              0.000505   \n",
              "310720                 0.008137              0.023715              0.003238   \n",
              "182129                 0.011189              0.019668              0.001049   \n",
              "14955                  0.004458              0.019897              0.000928   \n",
              "247932                 0.094479              0.023774              0.005332   \n",
              "120524                 0.009579              0.025898              0.002094   \n",
              "397658                 0.149648              0.020177              0.002660   \n",
              "317444                 0.081905              0.020290              0.001263   \n",
              "657402                 0.000814              0.024496              0.000814   \n",
              "\n",
              "        min_semesterly_abstract_df  max_semesterly_abstract_df  \\\n",
              "470000                    0.183220                    0.288148   \n",
              "363672                    0.313569                    0.380952   \n",
              "572860                    0.095539                    0.164715   \n",
              "545060                    0.169072                    0.231972   \n",
              "339603                    0.216762                    0.267757   \n",
              "171649                    0.253823                    0.330031   \n",
              "177772                    0.204587                    0.294694   \n",
              "148096                    0.186013                    0.244037   \n",
              "431641                    0.218930                    0.288241   \n",
              "581391                    0.296124                    0.405975   \n",
              "552448                    0.289323                    0.390278   \n",
              "254798                    0.000696                    0.082962   \n",
              "217650                    0.256925                    0.303415   \n",
              "166646                    0.166818                    0.237766   \n",
              "617153                    0.185060                    0.229372   \n",
              "613300                    0.022492                    0.151506   \n",
              "625254                    0.303765                    0.356778   \n",
              "598345                    0.177571                    0.212870   \n",
              "562725                    0.155046                    0.276389   \n",
              "293613                    0.188749                    0.255336   \n",
              "319894                    0.111617                    0.152911   \n",
              "57362                     0.023241                    0.054487   \n",
              "565058                    0.313153                    0.384722   \n",
              "214271                    0.089812                    0.131125   \n",
              "388856                    0.132017                    0.220741   \n",
              "411269                    0.190608                    0.234578   \n",
              "286248                    0.071604                    0.149004   \n",
              "113900                    0.139011                    0.205014   \n",
              "95807                     0.048706                    0.066212   \n",
              "588769                    0.232399                    0.284997   \n",
              "530583                    0.127692                    0.176877   \n",
              "460732                    0.155256                    0.278395   \n",
              "620038                    0.052169                    0.112641   \n",
              "356225                    0.071840                    0.128825   \n",
              "119188                    0.064301                    0.137866   \n",
              "116929                    0.107326                    0.147081   \n",
              "576695                    0.074596                    0.142907   \n",
              "625862                    0.072298                    0.111574   \n",
              "383990                    0.024231                    0.071667   \n",
              "110424                    0.112642                    0.150926   \n",
              "291685                    0.040290                    0.092111   \n",
              "268471                    0.119503                    0.211019   \n",
              "406971                    0.035780                    0.109176   \n",
              "78834                     0.087926                    0.127389   \n",
              "375687                    0.050652                    0.126667   \n",
              "175898                    0.017593                    0.039246   \n",
              "240069                    0.214925                    0.242038   \n",
              "57383                     0.014259                    0.036240   \n",
              "199483                    0.073315                    0.123716   \n",
              "475693                    0.054471                    0.096774   \n",
              "664553                    0.067031                    0.092599   \n",
              "175899                    0.013519                    0.034747   \n",
              "57384                     0.013519                    0.034747   \n",
              "347238                    0.041590                    0.250075   \n",
              "42563                     0.098054                    0.237944   \n",
              "331710                    0.053500                    0.074369   \n",
              "333092                    0.111081                    0.172990   \n",
              "154452                    0.055586                    0.079205   \n",
              "216227                    0.055352                    0.074171   \n",
              "252512                    0.022081                    0.050459   \n",
              "340366                    0.084259                    0.106134   \n",
              "131582                    0.169690                    0.211734   \n",
              "227781                    0.069649                    0.108257   \n",
              "579495                    0.079104                    0.162130   \n",
              "97275                     0.087830                    0.108691   \n",
              "369657                    0.052294                    0.094806   \n",
              "359326                    0.000187                    0.030493   \n",
              "184547                    0.204133                    0.268166   \n",
              "638237                    0.184709                    0.291104   \n",
              "402830                    0.186452                    0.216515   \n",
              "509507                    0.113569                    0.149772   \n",
              "410314                    0.036397                    0.092634   \n",
              "652417                    0.077280                    0.100636   \n",
              "494631                    0.045886                    0.085394   \n",
              "163485                    0.035128                    0.105823   \n",
              "324559                    0.000793                    0.052870   \n",
              "133227                    0.218077                    0.246362   \n",
              "259637                    0.026911                    0.058189   \n",
              "74776                     0.041166                    0.066121   \n",
              "601242                    0.047579                    0.098269   \n",
              "121454                    0.071560                    0.112559   \n",
              "443669                    0.043017                    0.099392   \n",
              "221611                    0.020489                    0.047787   \n",
              "24816                     0.043714                    0.072477   \n",
              "649853                    0.014259                    0.044650   \n",
              "246032                    0.033176                    0.060329   \n",
              "403096                    0.050842                    0.073089   \n",
              "561252                    0.141759                    0.199056   \n",
              "528846                    0.117197                    0.204543   \n",
              "610020                    0.136120                    0.198384   \n",
              "424002                    0.094100                    0.129505   \n",
              "123440                    0.049718                    0.094801   \n",
              "624264                    0.037671                    0.061192   \n",
              "353709                    0.082803                    0.170000   \n",
              "472777                    0.063224                    0.078410   \n",
              "511010                    0.066944                    0.102816   \n",
              "434364                    0.056667                    0.078582   \n",
              "107338                    0.030275                    0.124537   \n",
              "556797                    0.045359                    0.064748   \n",
              "28462                     0.100735                    0.170370   \n",
              "321990                    0.045193                    0.112297   \n",
              "647191                    0.032348                    0.093262   \n",
              "50537                     0.229358                    0.308056   \n",
              "605781                    0.023840                    0.057038   \n",
              "7337                      0.020251                    0.068152   \n",
              "121155                    0.012082                    0.037003   \n",
              "160957                    0.027230                    0.042029   \n",
              "59730                     0.064294                    0.113079   \n",
              "22619                     0.046103                    0.070387   \n",
              "436150                    0.033060                    0.059573   \n",
              "311087                    0.024699                    0.035786   \n",
              "147138                    0.045557                    0.060245   \n",
              "336305                    0.141676                    0.174343   \n",
              "630917                    0.125265                    0.288056   \n",
              "514544                    0.053685                    0.069668   \n",
              "162280                    0.011520                    0.027509   \n",
              "296852                    0.026911                    0.067528   \n",
              "396876                    0.030411                    0.049537   \n",
              "515333                    0.024503                    0.057881   \n",
              "197289                    0.059033                    0.101529   \n",
              "224426                    0.087122                    0.112527   \n",
              "351906                    0.130422                    0.184330   \n",
              "23788                     0.033760                    0.056506   \n",
              "157686                    0.081105                    0.121062   \n",
              "470722                    0.010771                    0.023606   \n",
              "543378                    0.018200                    0.073148   \n",
              "546786                    0.008984                    0.057085   \n",
              "80772                     0.010150                    0.047633   \n",
              "515852                    0.000545                    0.030730   \n",
              "442317                    0.111064                    0.135942   \n",
              "379981                    0.005743                    0.024907   \n",
              "445139                    0.018609                    0.030331   \n",
              "471204                    0.011408                    0.025174   \n",
              "47236                     0.113560                    0.131742   \n",
              "447672                    0.099646                    0.181204   \n",
              "505988                    0.116893                    0.151389   \n",
              "54284                     0.005810                    0.037500   \n",
              "415214                    0.041928                    0.068587   \n",
              "338981                    0.000808                    0.042130   \n",
              "69538                     0.010346                    0.029740   \n",
              "190806                    0.082407                    0.119249   \n",
              "608072                    0.065454                    0.154352   \n",
              "613674                    0.000191                    0.034364   \n",
              "542628                    0.054305                    0.076245   \n",
              "187258                    0.032454                    0.095833   \n",
              "56030                     0.023686                    0.075253   \n",
              "40461                     0.045799                    0.114626   \n",
              "488269                    0.045278                    0.062679   \n",
              "225288                    0.101258                    0.164115   \n",
              "188550                    0.005199                    0.042029   \n",
              "161908                    0.014523                    0.033643   \n",
              "305122                    0.001517                    0.029246   \n",
              "281402                    0.022416                    0.049541   \n",
              "89015                     0.003772                    0.014559   \n",
              "212869                    0.008037                    0.048225   \n",
              "345427                    0.046789                    0.122986   \n",
              "359315                    0.000299                    0.020692   \n",
              "305495                    0.024654                    0.040979   \n",
              "151140                    0.003674                    0.046899   \n",
              "454043                    0.021101                    0.048056   \n",
              "603                       0.022191                    0.068354   \n",
              "602145                    0.041192                    0.063083   \n",
              "162525                    0.031296                    0.044078   \n",
              "470528                    0.011759                    0.027695   \n",
              "371771                    0.004084                    0.040145   \n",
              "254679                    0.006004                    0.021909   \n",
              "91326                     0.036700                    0.096852   \n",
              "26813                     0.123853                    0.278582   \n",
              "274294                    0.023916                    0.036106   \n",
              "57517                     0.030300                    0.041760   \n",
              "606952                    0.077518                    0.115684   \n",
              "84302                     0.028990                    0.046655   \n",
              "584901                    0.081858                    0.158104   \n",
              "575103                    0.013761                    0.034554   \n",
              "191949                    0.049534                    0.077507   \n",
              "388319                    0.024325                    0.036160   \n",
              "66248                     0.177627                    0.208257   \n",
              "537911                    0.055760                    0.074917   \n",
              "437446                    0.011222                    0.051939   \n",
              "338994                    0.000140                    0.036852   \n",
              "69636                     0.003747                    0.021747   \n",
              "189307                    0.024231                    0.051236   \n",
              "455301                    0.028135                    0.086642   \n",
              "127192                    0.017288                    0.052315   \n",
              "456325                    0.016013                    0.057540   \n",
              "390945                    0.026580                    0.044253   \n",
              "559091                    0.009480                    0.047024   \n",
              "486692                    0.139500                    0.181189   \n",
              "345464                    0.013613                    0.060242   \n",
              "357537                    0.124583                    0.185068   \n",
              "561036                    0.009211                    0.025994   \n",
              "442211                    0.000186                    0.019800   \n",
              "310720                    0.012597                    0.024261   \n",
              "182129                    0.012252                    0.127593   \n",
              "14955                     0.008764                    0.026733   \n",
              "247932                    0.110082                    0.143291   \n",
              "120524                    0.014029                    0.031590   \n",
              "397658                    0.175229                    0.240444   \n",
              "317444                    0.114733                    0.169729   \n",
              "657402                    0.000335                    0.022274   \n",
              "\n",
              "        min_semesterly_title_df  max_semesterly_title_df  \n",
              "470000                 0.106775                 0.139431  \n",
              "363672                 0.073472                 0.128135  \n",
              "572860                 0.051981                 0.119249  \n",
              "545060                 0.077593                 0.109524  \n",
              "339603                 0.078556                 0.108265  \n",
              "171649                 0.066835                 0.088532  \n",
              "177772                 0.058228                 0.084139  \n",
              "148096                 0.053769                 0.079205  \n",
              "431641                 0.062540                 0.074917  \n",
              "581391                 0.048834                 0.074613  \n",
              "552448                 0.052538                 0.074596  \n",
              "254798                 0.000230                 0.073019  \n",
              "217650                 0.047190                 0.071560  \n",
              "166646                 0.059628                 0.070632  \n",
              "617153                 0.049935                 0.067890  \n",
              "613300                 0.006889                 0.066992  \n",
              "625254                 0.041574                 0.061802  \n",
              "598345                 0.033573                 0.059448  \n",
              "562725                 0.039430                 0.052861  \n",
              "293613                 0.037027                 0.052141  \n",
              "319894                 0.031582                 0.050500  \n",
              "57362                  0.011667                 0.046753  \n",
              "565058                 0.024817                 0.046242  \n",
              "214271                 0.025475                 0.046051  \n",
              "388856                 0.023449                 0.045741  \n",
              "411269                 0.025560                 0.044954  \n",
              "286248                 0.019572                 0.044074  \n",
              "113900                 0.021583                 0.043181  \n",
              "95807                  0.012694                 0.042962  \n",
              "588769                 0.027761                 0.042810  \n",
              "530583                 0.020018                 0.042699  \n",
              "460732                 0.029339                 0.042269  \n",
              "620038                 0.019412                 0.040780  \n",
              "356225                 0.024353                 0.039824  \n",
              "119188                 0.017895                 0.038925  \n",
              "116929                 0.016812                 0.038532  \n",
              "576695                 0.022558                 0.038414  \n",
              "625862                 0.020274                 0.038116  \n",
              "383990                 0.010890                 0.037867  \n",
              "110424                 0.022833                 0.036912  \n",
              "291685                 0.012383                 0.036153  \n",
              "268471                 0.022430                 0.035393  \n",
              "406971                 0.013150                 0.034777  \n",
              "78834                  0.019145                 0.034115  \n",
              "375687                 0.017405                 0.033519  \n",
              "175898                 0.007037                 0.033359  \n",
              "240069                 0.019912                 0.033325  \n",
              "57383                  0.006111                 0.033106  \n",
              "199483                 0.018200                 0.032898  \n",
              "475693                 0.013109                 0.032757  \n",
              "664553                 0.018198                 0.032485  \n",
              "175899                 0.006019                 0.032095  \n",
              "57384                  0.006019                 0.032095  \n",
              "347238                 0.005560                 0.032082  \n",
              "42563                  0.006409                 0.032009  \n",
              "331710                 0.019958                 0.031854  \n",
              "333092                 0.016662                 0.031791  \n",
              "154452                 0.016336                 0.031590  \n",
              "216227                 0.015573                 0.031528  \n",
              "252512                 0.008912                 0.031498  \n",
              "340366                 0.008304                 0.031498  \n",
              "131582                 0.019539                 0.031399  \n",
              "227781                 0.012860                 0.031337  \n",
              "579495                 0.009727                 0.031190  \n",
              "97275                  0.014358                 0.031124  \n",
              "369657                 0.017431                 0.031005  \n",
              "359326                 0.000094                 0.030221  \n",
              "184547                 0.020556                 0.030027  \n",
              "638237                 0.007076                 0.029815  \n",
              "402830                 0.013704                 0.029358  \n",
              "509507                 0.016148                 0.029223  \n",
              "410314                 0.009402                 0.028459  \n",
              "652417                 0.014014                 0.027829  \n",
              "494631                 0.013981                 0.027739  \n",
              "163485                 0.010659                 0.027443  \n",
              "324559                 0.000198                 0.027222  \n",
              "133227                 0.016688                 0.027180  \n",
              "259637                 0.015437                 0.026692  \n",
              "74776                  0.014646                 0.026691  \n",
              "601242                 0.010288                 0.026616  \n",
              "121454                 0.012870                 0.026451  \n",
              "443669                 0.007951                 0.026208  \n",
              "221611                 0.010703                 0.024775  \n",
              "24816                  0.013197                 0.024771  \n",
              "649853                 0.005367                 0.024771  \n",
              "246032                 0.009620                 0.024767  \n",
              "403096                 0.010833                 0.024465  \n",
              "561252                 0.013004                 0.024462  \n",
              "528846                 0.013228                 0.024217  \n",
              "610020                 0.014942                 0.024205  \n",
              "424002                 0.013945                 0.024169  \n",
              "123440                 0.004928                 0.023853  \n",
              "624264                 0.007525                 0.023777  \n",
              "353709                 0.012887                 0.023741  \n",
              "472777                 0.011444                 0.023547  \n",
              "511010                 0.011574                 0.023547  \n",
              "434364                 0.016941                 0.023414  \n",
              "107338                 0.004628                 0.023016  \n",
              "556797                 0.009209                 0.022962  \n",
              "28462                  0.014010                 0.022940  \n",
              "321990                 0.009283                 0.022759  \n",
              "647191                 0.001835                 0.022416  \n",
              "50537                  0.012436                 0.022298  \n",
              "605781                 0.008257                 0.021996  \n",
              "7337                   0.001769                 0.021574  \n",
              "121155                 0.003603                 0.021407  \n",
              "160957                 0.010430                 0.021266  \n",
              "59730                  0.008266                 0.021247  \n",
              "22619                  0.012132                 0.021185  \n",
              "436150                 0.009786                 0.020944  \n",
              "311087                 0.009039                 0.020929  \n",
              "147138                 0.012203                 0.020817  \n",
              "336305                 0.010833                 0.020795  \n",
              "630917                 0.009706                 0.020552  \n",
              "514544                 0.012132                 0.020545  \n",
              "162280                 0.005063                 0.020545  \n",
              "296852                 0.008101                 0.020333  \n",
              "396876                 0.007785                 0.020185  \n",
              "515333                 0.011081                 0.020024  \n",
              "197289                 0.005807                 0.020018  \n",
              "224426                 0.009074                 0.020018  \n",
              "351906                 0.010312                 0.019825  \n",
              "23788                  0.008525                 0.019759  \n",
              "157686                 0.006679                 0.019726  \n",
              "470722                 0.005050                 0.019612  \n",
              "543378                 0.006665                 0.019424  \n",
              "546786                 0.002730                 0.019342  \n",
              "80772                  0.003170                 0.019314  \n",
              "515852                 0.000213                 0.019192  \n",
              "442317                 0.009205                 0.019089  \n",
              "379981                 0.002674                 0.019051  \n",
              "445139                 0.008074                 0.018805  \n",
              "471204                 0.005577                 0.018805  \n",
              "47236                  0.007724                 0.018786  \n",
              "447672                 0.010361                 0.018698  \n",
              "505988                 0.012375                 0.018502  \n",
              "54284                  0.002141                 0.018426  \n",
              "415214                 0.008981                 0.018338  \n",
              "338981                 0.000186                 0.018333  \n",
              "69538                  0.005248                 0.018216  \n",
              "190806                 0.008714                 0.017943  \n",
              "608072                 0.006116                 0.017931  \n",
              "613674                 0.000191                 0.017812  \n",
              "542628                 0.007052                 0.017737  \n",
              "187258                 0.005156                 0.017626  \n",
              "56030                  0.003383                 0.017617  \n",
              "40461                  0.006152                 0.017617  \n",
              "488269                 0.007500                 0.017592  \n",
              "225288                 0.008087                 0.017524  \n",
              "188550                 0.002752                 0.017492  \n",
              "161908                 0.006224                 0.017472  \n",
              "305122                 0.000612                 0.017439  \n",
              "281402                 0.005468                 0.017431  \n",
              "89015                  0.002001                 0.017405  \n",
              "212869                 0.003058                 0.017257  \n",
              "345427                 0.004689                 0.017247  \n",
              "359315                 0.000099                 0.017152  \n",
              "305495                 0.008021                 0.017125  \n",
              "151140                 0.000696                 0.017014  \n",
              "454043                 0.004893                 0.016894  \n",
              "603                    0.005148                 0.016880  \n",
              "602145                 0.008916                 0.016820  \n",
              "162525                 0.008283                 0.016774  \n",
              "470528                 0.005928                 0.016729  \n",
              "371771                 0.000817                 0.016648  \n",
              "254679                 0.003703                 0.016575  \n",
              "91326                  0.005460                 0.016455  \n",
              "26813                  0.008869                 0.016427  \n",
              "274294                 0.008368                 0.016379  \n",
              "57517                  0.007308                 0.016283  \n",
              "606952                 0.007097                 0.016088  \n",
              "84302                  0.006154                 0.016059  \n",
              "584901                 0.003493                 0.015902  \n",
              "575103                 0.006004                 0.015876  \n",
              "191949                 0.008916                 0.015866  \n",
              "388319                 0.004951                 0.015860  \n",
              "66248                  0.007805                 0.015799  \n",
              "537911                 0.006883                 0.015772  \n",
              "437446                 0.002426                 0.015744  \n",
              "338994                 0.000140                 0.015741  \n",
              "69636                  0.001782                 0.015547  \n",
              "189307                 0.005307                 0.015511  \n",
              "455301                 0.005173                 0.015496  \n",
              "127192                 0.004550                 0.015463  \n",
              "456325                 0.003538                 0.015441  \n",
              "390945                 0.008670                 0.015291  \n",
              "559091                 0.002123                 0.015268  \n",
              "486692                 0.008716                 0.015230  \n",
              "345464                 0.002538                 0.015153  \n",
              "357537                 0.009705                 0.015063  \n",
              "561036                 0.001664                 0.014985  \n",
              "442211                 0.000187                 0.014896  \n",
              "310720                 0.006183                 0.014658  \n",
              "182129                 0.001089                 0.014630  \n",
              "14955                  0.003538                 0.014622  \n",
              "247932                 0.007544                 0.014430  \n",
              "120524                 0.003903                 0.014405  \n",
              "397658                 0.005383                 0.014311  \n",
              "317444                 0.006555                 0.014302  \n",
              "657402                 0.000167                 0.014302  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ad49b9de-5f52-4472-9766-6c6a705c2af8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>term</th>\n",
              "      <th>max_monthly_abstract_df</th>\n",
              "      <th>min_monthly_abstract_df</th>\n",
              "      <th>max_monthly_title_df</th>\n",
              "      <th>min_monthly_title_df</th>\n",
              "      <th>min_semesterly_abstract_df</th>\n",
              "      <th>max_semesterly_abstract_df</th>\n",
              "      <th>min_semesterly_title_df</th>\n",
              "      <th>max_semesterly_title_df</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>470000</th>\n",
              "      <td>quantum</td>\n",
              "      <td>0.304857</td>\n",
              "      <td>0.149096</td>\n",
              "      <td>0.152326</td>\n",
              "      <td>0.086093</td>\n",
              "      <td>0.183220</td>\n",
              "      <td>0.288148</td>\n",
              "      <td>0.106775</td>\n",
              "      <td>0.139431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363672</th>\n",
              "      <td>model</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.272823</td>\n",
              "      <td>0.147708</td>\n",
              "      <td>0.062338</td>\n",
              "      <td>0.313569</td>\n",
              "      <td>0.380952</td>\n",
              "      <td>0.073472</td>\n",
              "      <td>0.128135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>572860</th>\n",
              "      <td>superconducting</td>\n",
              "      <td>0.207018</td>\n",
              "      <td>0.083924</td>\n",
              "      <td>0.164251</td>\n",
              "      <td>0.045223</td>\n",
              "      <td>0.095539</td>\n",
              "      <td>0.164715</td>\n",
              "      <td>0.051981</td>\n",
              "      <td>0.119249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>545060</th>\n",
              "      <td>spin</td>\n",
              "      <td>0.253118</td>\n",
              "      <td>0.144762</td>\n",
              "      <td>0.123958</td>\n",
              "      <td>0.041905</td>\n",
              "      <td>0.169072</td>\n",
              "      <td>0.231972</td>\n",
              "      <td>0.077593</td>\n",
              "      <td>0.109524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339603</th>\n",
              "      <td>magnetic</td>\n",
              "      <td>0.286458</td>\n",
              "      <td>0.188571</td>\n",
              "      <td>0.122318</td>\n",
              "      <td>0.069930</td>\n",
              "      <td>0.216762</td>\n",
              "      <td>0.267757</td>\n",
              "      <td>0.078556</td>\n",
              "      <td>0.108265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171649</th>\n",
              "      <td>effect</td>\n",
              "      <td>0.344311</td>\n",
              "      <td>0.210721</td>\n",
              "      <td>0.104167</td>\n",
              "      <td>0.054711</td>\n",
              "      <td>0.253823</td>\n",
              "      <td>0.330031</td>\n",
              "      <td>0.066835</td>\n",
              "      <td>0.088532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177772</th>\n",
              "      <td>electron</td>\n",
              "      <td>0.322727</td>\n",
              "      <td>0.176570</td>\n",
              "      <td>0.109783</td>\n",
              "      <td>0.047280</td>\n",
              "      <td>0.204587</td>\n",
              "      <td>0.294694</td>\n",
              "      <td>0.058228</td>\n",
              "      <td>0.084139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148096</th>\n",
              "      <td>dimensional</td>\n",
              "      <td>0.265378</td>\n",
              "      <td>0.162928</td>\n",
              "      <td>0.100935</td>\n",
              "      <td>0.040570</td>\n",
              "      <td>0.186013</td>\n",
              "      <td>0.244037</td>\n",
              "      <td>0.053769</td>\n",
              "      <td>0.079205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431641</th>\n",
              "      <td>phase</td>\n",
              "      <td>0.311976</td>\n",
              "      <td>0.195157</td>\n",
              "      <td>0.104497</td>\n",
              "      <td>0.049236</td>\n",
              "      <td>0.218930</td>\n",
              "      <td>0.288241</td>\n",
              "      <td>0.062540</td>\n",
              "      <td>0.074917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581391</th>\n",
              "      <td>system</td>\n",
              "      <td>0.429949</td>\n",
              "      <td>0.268934</td>\n",
              "      <td>0.095420</td>\n",
              "      <td>0.039423</td>\n",
              "      <td>0.296124</td>\n",
              "      <td>0.405975</td>\n",
              "      <td>0.048834</td>\n",
              "      <td>0.074613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>552448</th>\n",
              "      <td>state</td>\n",
              "      <td>0.401413</td>\n",
              "      <td>0.264052</td>\n",
              "      <td>0.092421</td>\n",
              "      <td>0.036514</td>\n",
              "      <td>0.289323</td>\n",
              "      <td>0.390278</td>\n",
              "      <td>0.052538</td>\n",
              "      <td>0.074596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254798</th>\n",
              "      <td>graphene</td>\n",
              "      <td>0.096935</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.085336</td>\n",
              "      <td>0.001078</td>\n",
              "      <td>0.000696</td>\n",
              "      <td>0.082962</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.073019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217650</th>\n",
              "      <td>field</td>\n",
              "      <td>0.324774</td>\n",
              "      <td>0.220899</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.038255</td>\n",
              "      <td>0.256925</td>\n",
              "      <td>0.303415</td>\n",
              "      <td>0.047190</td>\n",
              "      <td>0.071560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166646</th>\n",
              "      <td>dynamics</td>\n",
              "      <td>0.259036</td>\n",
              "      <td>0.130568</td>\n",
              "      <td>0.088790</td>\n",
              "      <td>0.043810</td>\n",
              "      <td>0.166818</td>\n",
              "      <td>0.237766</td>\n",
              "      <td>0.059628</td>\n",
              "      <td>0.070632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>617153</th>\n",
              "      <td>transition</td>\n",
              "      <td>0.251664</td>\n",
              "      <td>0.154545</td>\n",
              "      <td>0.091787</td>\n",
              "      <td>0.041599</td>\n",
              "      <td>0.185060</td>\n",
              "      <td>0.229372</td>\n",
              "      <td>0.049935</td>\n",
              "      <td>0.067890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>613300</th>\n",
              "      <td>topological</td>\n",
              "      <td>0.165800</td>\n",
              "      <td>0.016822</td>\n",
              "      <td>0.075839</td>\n",
              "      <td>0.003326</td>\n",
              "      <td>0.022492</td>\n",
              "      <td>0.151506</td>\n",
              "      <td>0.006889</td>\n",
              "      <td>0.066992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625254</th>\n",
              "      <td>two</td>\n",
              "      <td>0.383643</td>\n",
              "      <td>0.269421</td>\n",
              "      <td>0.080153</td>\n",
              "      <td>0.029762</td>\n",
              "      <td>0.303765</td>\n",
              "      <td>0.356778</td>\n",
              "      <td>0.041574</td>\n",
              "      <td>0.061802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>598345</th>\n",
              "      <td>theory</td>\n",
              "      <td>0.245714</td>\n",
              "      <td>0.148982</td>\n",
              "      <td>0.083810</td>\n",
              "      <td>0.024829</td>\n",
              "      <td>0.177571</td>\n",
              "      <td>0.212870</td>\n",
              "      <td>0.033573</td>\n",
              "      <td>0.059448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>562725</th>\n",
              "      <td>structure</td>\n",
              "      <td>0.289587</td>\n",
              "      <td>0.124521</td>\n",
              "      <td>0.069034</td>\n",
              "      <td>0.024226</td>\n",
              "      <td>0.155046</td>\n",
              "      <td>0.276389</td>\n",
              "      <td>0.039430</td>\n",
              "      <td>0.052861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293613</th>\n",
              "      <td>interaction</td>\n",
              "      <td>0.272303</td>\n",
              "      <td>0.170093</td>\n",
              "      <td>0.065141</td>\n",
              "      <td>0.027097</td>\n",
              "      <td>0.188749</td>\n",
              "      <td>0.255336</td>\n",
              "      <td>0.037027</td>\n",
              "      <td>0.052141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319894</th>\n",
              "      <td>lattice</td>\n",
              "      <td>0.176149</td>\n",
              "      <td>0.096831</td>\n",
              "      <td>0.063542</td>\n",
              "      <td>0.021053</td>\n",
              "      <td>0.111617</td>\n",
              "      <td>0.152911</td>\n",
              "      <td>0.031582</td>\n",
              "      <td>0.050500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57362</th>\n",
              "      <td>bose</td>\n",
              "      <td>0.072039</td>\n",
              "      <td>0.017943</td>\n",
              "      <td>0.064336</td>\n",
              "      <td>0.009679</td>\n",
              "      <td>0.023241</td>\n",
              "      <td>0.054487</td>\n",
              "      <td>0.011667</td>\n",
              "      <td>0.046753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565058</th>\n",
              "      <td>study</td>\n",
              "      <td>0.408824</td>\n",
              "      <td>0.282456</td>\n",
              "      <td>0.056673</td>\n",
              "      <td>0.014763</td>\n",
              "      <td>0.313153</td>\n",
              "      <td>0.384722</td>\n",
              "      <td>0.024817</td>\n",
              "      <td>0.046242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214271</th>\n",
              "      <td>fermions</td>\n",
              "      <td>0.149269</td>\n",
              "      <td>0.069421</td>\n",
              "      <td>0.057411</td>\n",
              "      <td>0.018135</td>\n",
              "      <td>0.089812</td>\n",
              "      <td>0.131125</td>\n",
              "      <td>0.025475</td>\n",
              "      <td>0.046051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388856</th>\n",
              "      <td>non</td>\n",
              "      <td>0.237646</td>\n",
              "      <td>0.109501</td>\n",
              "      <td>0.051961</td>\n",
              "      <td>0.017143</td>\n",
              "      <td>0.132017</td>\n",
              "      <td>0.220741</td>\n",
              "      <td>0.023449</td>\n",
              "      <td>0.045741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411269</th>\n",
              "      <td>order</td>\n",
              "      <td>0.260417</td>\n",
              "      <td>0.173963</td>\n",
              "      <td>0.052724</td>\n",
              "      <td>0.015116</td>\n",
              "      <td>0.190608</td>\n",
              "      <td>0.234578</td>\n",
              "      <td>0.025560</td>\n",
              "      <td>0.044954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286248</th>\n",
              "      <td>induced</td>\n",
              "      <td>0.161248</td>\n",
              "      <td>0.059593</td>\n",
              "      <td>0.050426</td>\n",
              "      <td>0.013834</td>\n",
              "      <td>0.071604</td>\n",
              "      <td>0.149004</td>\n",
              "      <td>0.019572</td>\n",
              "      <td>0.044074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113900</th>\n",
              "      <td>coupling</td>\n",
              "      <td>0.223912</td>\n",
              "      <td>0.122241</td>\n",
              "      <td>0.058442</td>\n",
              "      <td>0.012000</td>\n",
              "      <td>0.139011</td>\n",
              "      <td>0.205014</td>\n",
              "      <td>0.021583</td>\n",
              "      <td>0.043181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95807</th>\n",
              "      <td>condensate</td>\n",
              "      <td>0.084630</td>\n",
              "      <td>0.036830</td>\n",
              "      <td>0.055453</td>\n",
              "      <td>0.008103</td>\n",
              "      <td>0.048706</td>\n",
              "      <td>0.066212</td>\n",
              "      <td>0.012694</td>\n",
              "      <td>0.042962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>588769</th>\n",
              "      <td>temperature</td>\n",
              "      <td>0.326892</td>\n",
              "      <td>0.218025</td>\n",
              "      <td>0.061192</td>\n",
              "      <td>0.018041</td>\n",
              "      <td>0.232399</td>\n",
              "      <td>0.284997</td>\n",
              "      <td>0.027761</td>\n",
              "      <td>0.042810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>530583</th>\n",
              "      <td>single</td>\n",
              "      <td>0.199349</td>\n",
              "      <td>0.095070</td>\n",
              "      <td>0.054187</td>\n",
              "      <td>0.012704</td>\n",
              "      <td>0.127692</td>\n",
              "      <td>0.176877</td>\n",
              "      <td>0.020018</td>\n",
              "      <td>0.042699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>460732</th>\n",
              "      <td>properties</td>\n",
              "      <td>0.297605</td>\n",
              "      <td>0.133978</td>\n",
              "      <td>0.062215</td>\n",
              "      <td>0.021314</td>\n",
              "      <td>0.155256</td>\n",
              "      <td>0.278395</td>\n",
              "      <td>0.029339</td>\n",
              "      <td>0.042269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>620038</th>\n",
              "      <td>transport</td>\n",
              "      <td>0.123399</td>\n",
              "      <td>0.040665</td>\n",
              "      <td>0.058050</td>\n",
              "      <td>0.011050</td>\n",
              "      <td>0.052169</td>\n",
              "      <td>0.112641</td>\n",
              "      <td>0.019412</td>\n",
              "      <td>0.040780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356225</th>\n",
              "      <td>metal</td>\n",
              "      <td>0.141848</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.047330</td>\n",
              "      <td>0.013245</td>\n",
              "      <td>0.071840</td>\n",
              "      <td>0.128825</td>\n",
              "      <td>0.024353</td>\n",
              "      <td>0.039824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119188</th>\n",
              "      <td>crystal</td>\n",
              "      <td>0.152033</td>\n",
              "      <td>0.041739</td>\n",
              "      <td>0.045643</td>\n",
              "      <td>0.011628</td>\n",
              "      <td>0.064301</td>\n",
              "      <td>0.137866</td>\n",
              "      <td>0.017895</td>\n",
              "      <td>0.038925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116929</th>\n",
              "      <td>critical</td>\n",
              "      <td>0.166359</td>\n",
              "      <td>0.091677</td>\n",
              "      <td>0.049407</td>\n",
              "      <td>0.011152</td>\n",
              "      <td>0.107326</td>\n",
              "      <td>0.147081</td>\n",
              "      <td>0.016812</td>\n",
              "      <td>0.038532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>576695</th>\n",
              "      <td>surface</td>\n",
              "      <td>0.163150</td>\n",
              "      <td>0.053286</td>\n",
              "      <td>0.048762</td>\n",
              "      <td>0.007728</td>\n",
              "      <td>0.074596</td>\n",
              "      <td>0.142907</td>\n",
              "      <td>0.022558</td>\n",
              "      <td>0.038414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625862</th>\n",
              "      <td>two dimensional</td>\n",
              "      <td>0.124907</td>\n",
              "      <td>0.054371</td>\n",
              "      <td>0.053435</td>\n",
              "      <td>0.014137</td>\n",
              "      <td>0.072298</td>\n",
              "      <td>0.111574</td>\n",
              "      <td>0.020274</td>\n",
              "      <td>0.038116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383990</th>\n",
              "      <td>network</td>\n",
              "      <td>0.082732</td>\n",
              "      <td>0.017606</td>\n",
              "      <td>0.050465</td>\n",
              "      <td>0.007018</td>\n",
              "      <td>0.024231</td>\n",
              "      <td>0.071667</td>\n",
              "      <td>0.010890</td>\n",
              "      <td>0.037867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110424</th>\n",
              "      <td>correlation</td>\n",
              "      <td>0.171481</td>\n",
              "      <td>0.097074</td>\n",
              "      <td>0.049251</td>\n",
              "      <td>0.018257</td>\n",
              "      <td>0.112642</td>\n",
              "      <td>0.150926</td>\n",
              "      <td>0.022833</td>\n",
              "      <td>0.036912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291685</th>\n",
              "      <td>insulator</td>\n",
              "      <td>0.104665</td>\n",
              "      <td>0.030418</td>\n",
              "      <td>0.046335</td>\n",
              "      <td>0.007112</td>\n",
              "      <td>0.040290</td>\n",
              "      <td>0.092111</td>\n",
              "      <td>0.012383</td>\n",
              "      <td>0.036153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268471</th>\n",
              "      <td>high</td>\n",
              "      <td>0.232991</td>\n",
              "      <td>0.089376</td>\n",
              "      <td>0.057971</td>\n",
              "      <td>0.016336</td>\n",
              "      <td>0.119503</td>\n",
              "      <td>0.211019</td>\n",
              "      <td>0.022430</td>\n",
              "      <td>0.035393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>406971</th>\n",
              "      <td>optical</td>\n",
              "      <td>0.120430</td>\n",
              "      <td>0.026643</td>\n",
              "      <td>0.048587</td>\n",
              "      <td>0.005272</td>\n",
              "      <td>0.035780</td>\n",
              "      <td>0.109176</td>\n",
              "      <td>0.013150</td>\n",
              "      <td>0.034777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78834</th>\n",
              "      <td>charge</td>\n",
              "      <td>0.142534</td>\n",
              "      <td>0.077108</td>\n",
              "      <td>0.048571</td>\n",
              "      <td>0.014646</td>\n",
              "      <td>0.087926</td>\n",
              "      <td>0.127389</td>\n",
              "      <td>0.019145</td>\n",
              "      <td>0.034115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375687</th>\n",
              "      <td>multi</td>\n",
              "      <td>0.141343</td>\n",
              "      <td>0.041502</td>\n",
              "      <td>0.045064</td>\n",
              "      <td>0.007905</td>\n",
              "      <td>0.050652</td>\n",
              "      <td>0.126667</td>\n",
              "      <td>0.017405</td>\n",
              "      <td>0.033519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175898</th>\n",
              "      <td>einstein</td>\n",
              "      <td>0.061050</td>\n",
              "      <td>0.012422</td>\n",
              "      <td>0.044362</td>\n",
              "      <td>0.003778</td>\n",
              "      <td>0.017593</td>\n",
              "      <td>0.039246</td>\n",
              "      <td>0.007037</td>\n",
              "      <td>0.033359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240069</th>\n",
              "      <td>function</td>\n",
              "      <td>0.269294</td>\n",
              "      <td>0.193809</td>\n",
              "      <td>0.046154</td>\n",
              "      <td>0.012422</td>\n",
              "      <td>0.214925</td>\n",
              "      <td>0.242038</td>\n",
              "      <td>0.019912</td>\n",
              "      <td>0.033325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57383</th>\n",
              "      <td>bose einstein</td>\n",
              "      <td>0.054945</td>\n",
              "      <td>0.011042</td>\n",
              "      <td>0.044362</td>\n",
              "      <td>0.003778</td>\n",
              "      <td>0.014259</td>\n",
              "      <td>0.036240</td>\n",
              "      <td>0.006111</td>\n",
              "      <td>0.033106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199483</th>\n",
              "      <td>excitations</td>\n",
              "      <td>0.145130</td>\n",
              "      <td>0.064081</td>\n",
              "      <td>0.042735</td>\n",
              "      <td>0.011885</td>\n",
              "      <td>0.073315</td>\n",
              "      <td>0.123716</td>\n",
              "      <td>0.018200</td>\n",
              "      <td>0.032898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475693</th>\n",
              "      <td>random</td>\n",
              "      <td>0.133803</td>\n",
              "      <td>0.044669</td>\n",
              "      <td>0.049296</td>\n",
              "      <td>0.008922</td>\n",
              "      <td>0.054471</td>\n",
              "      <td>0.096774</td>\n",
              "      <td>0.013109</td>\n",
              "      <td>0.032757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>664553</th>\n",
              "      <td>x</td>\n",
              "      <td>0.109467</td>\n",
              "      <td>0.050467</td>\n",
              "      <td>0.054371</td>\n",
              "      <td>0.010944</td>\n",
              "      <td>0.067031</td>\n",
              "      <td>0.092599</td>\n",
              "      <td>0.018198</td>\n",
              "      <td>0.032485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175899</th>\n",
              "      <td>einstein condensate</td>\n",
              "      <td>0.052503</td>\n",
              "      <td>0.010172</td>\n",
              "      <td>0.044362</td>\n",
              "      <td>0.003778</td>\n",
              "      <td>0.013519</td>\n",
              "      <td>0.034747</td>\n",
              "      <td>0.006019</td>\n",
              "      <td>0.032095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57384</th>\n",
              "      <td>bose einstein condensate</td>\n",
              "      <td>0.052503</td>\n",
              "      <td>0.010172</td>\n",
              "      <td>0.044362</td>\n",
              "      <td>0.003778</td>\n",
              "      <td>0.013519</td>\n",
              "      <td>0.034747</td>\n",
              "      <td>0.006019</td>\n",
              "      <td>0.032095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347238</th>\n",
              "      <td>materials</td>\n",
              "      <td>0.267216</td>\n",
              "      <td>0.036398</td>\n",
              "      <td>0.043250</td>\n",
              "      <td>0.003221</td>\n",
              "      <td>0.041590</td>\n",
              "      <td>0.250075</td>\n",
              "      <td>0.005560</td>\n",
              "      <td>0.032082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42563</th>\n",
              "      <td>based</td>\n",
              "      <td>0.264249</td>\n",
              "      <td>0.082667</td>\n",
              "      <td>0.043530</td>\n",
              "      <td>0.003641</td>\n",
              "      <td>0.098054</td>\n",
              "      <td>0.237944</td>\n",
              "      <td>0.006409</td>\n",
              "      <td>0.032009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>331710</th>\n",
              "      <td>liquid</td>\n",
              "      <td>0.103873</td>\n",
              "      <td>0.039669</td>\n",
              "      <td>0.041845</td>\n",
              "      <td>0.010417</td>\n",
              "      <td>0.053500</td>\n",
              "      <td>0.074369</td>\n",
              "      <td>0.019958</td>\n",
              "      <td>0.031854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333092</th>\n",
              "      <td>local</td>\n",
              "      <td>0.185207</td>\n",
              "      <td>0.088567</td>\n",
              "      <td>0.045840</td>\n",
              "      <td>0.010884</td>\n",
              "      <td>0.111081</td>\n",
              "      <td>0.172990</td>\n",
              "      <td>0.016662</td>\n",
              "      <td>0.031791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154452</th>\n",
              "      <td>disorder</td>\n",
              "      <td>0.097606</td>\n",
              "      <td>0.037513</td>\n",
              "      <td>0.053147</td>\n",
              "      <td>0.006431</td>\n",
              "      <td>0.055586</td>\n",
              "      <td>0.079205</td>\n",
              "      <td>0.016336</td>\n",
              "      <td>0.031590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216227</th>\n",
              "      <td>ferromagnetic</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.044944</td>\n",
              "      <td>0.042824</td>\n",
              "      <td>0.009130</td>\n",
              "      <td>0.055352</td>\n",
              "      <td>0.074171</td>\n",
              "      <td>0.015573</td>\n",
              "      <td>0.031528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252512</th>\n",
              "      <td>glass</td>\n",
              "      <td>0.066869</td>\n",
              "      <td>0.016337</td>\n",
              "      <td>0.042179</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>0.022081</td>\n",
              "      <td>0.050459</td>\n",
              "      <td>0.008912</td>\n",
              "      <td>0.031498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340366</th>\n",
              "      <td>magnetic field</td>\n",
              "      <td>0.135531</td>\n",
              "      <td>0.073120</td>\n",
              "      <td>0.040665</td>\n",
              "      <td>0.005729</td>\n",
              "      <td>0.084259</td>\n",
              "      <td>0.106134</td>\n",
              "      <td>0.008304</td>\n",
              "      <td>0.031498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131582</th>\n",
              "      <td>density</td>\n",
              "      <td>0.228639</td>\n",
              "      <td>0.144366</td>\n",
              "      <td>0.041841</td>\n",
              "      <td>0.012431</td>\n",
              "      <td>0.169690</td>\n",
              "      <td>0.211734</td>\n",
              "      <td>0.019539</td>\n",
              "      <td>0.031399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227781</th>\n",
              "      <td>fluctuations</td>\n",
              "      <td>0.122857</td>\n",
              "      <td>0.060677</td>\n",
              "      <td>0.042857</td>\n",
              "      <td>0.008328</td>\n",
              "      <td>0.069649</td>\n",
              "      <td>0.108257</td>\n",
              "      <td>0.012860</td>\n",
              "      <td>0.031337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>579495</th>\n",
              "      <td>symmetry</td>\n",
              "      <td>0.174902</td>\n",
              "      <td>0.066345</td>\n",
              "      <td>0.038717</td>\n",
              "      <td>0.004121</td>\n",
              "      <td>0.079104</td>\n",
              "      <td>0.162130</td>\n",
              "      <td>0.009727</td>\n",
              "      <td>0.031190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97275</th>\n",
              "      <td>conductivity</td>\n",
              "      <td>0.127747</td>\n",
              "      <td>0.068702</td>\n",
              "      <td>0.040179</td>\n",
              "      <td>0.011095</td>\n",
              "      <td>0.087830</td>\n",
              "      <td>0.108691</td>\n",
              "      <td>0.014358</td>\n",
              "      <td>0.031124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369657</th>\n",
              "      <td>molecular</td>\n",
              "      <td>0.110104</td>\n",
              "      <td>0.035120</td>\n",
              "      <td>0.041801</td>\n",
              "      <td>0.009346</td>\n",
              "      <td>0.052294</td>\n",
              "      <td>0.094806</td>\n",
              "      <td>0.017431</td>\n",
              "      <td>0.031005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359326</th>\n",
              "      <td>mgb2</td>\n",
              "      <td>0.053140</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.053140</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.030493</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.030221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184547</th>\n",
              "      <td>energy</td>\n",
              "      <td>0.280183</td>\n",
              "      <td>0.189385</td>\n",
              "      <td>0.039258</td>\n",
              "      <td>0.011043</td>\n",
              "      <td>0.204133</td>\n",
              "      <td>0.268166</td>\n",
              "      <td>0.020556</td>\n",
              "      <td>0.030027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>638237</th>\n",
              "      <td>using</td>\n",
              "      <td>0.313472</td>\n",
              "      <td>0.166384</td>\n",
              "      <td>0.035660</td>\n",
              "      <td>0.003040</td>\n",
              "      <td>0.184709</td>\n",
              "      <td>0.291104</td>\n",
              "      <td>0.007076</td>\n",
              "      <td>0.029815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402830</th>\n",
              "      <td>one</td>\n",
              "      <td>0.245448</td>\n",
              "      <td>0.162287</td>\n",
              "      <td>0.041322</td>\n",
              "      <td>0.008794</td>\n",
              "      <td>0.186452</td>\n",
              "      <td>0.216515</td>\n",
              "      <td>0.013704</td>\n",
              "      <td>0.029358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509507</th>\n",
              "      <td>scale</td>\n",
              "      <td>0.166779</td>\n",
              "      <td>0.097561</td>\n",
              "      <td>0.041262</td>\n",
              "      <td>0.009615</td>\n",
              "      <td>0.113569</td>\n",
              "      <td>0.149772</td>\n",
              "      <td>0.016148</td>\n",
              "      <td>0.029223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410314</th>\n",
              "      <td>orbit</td>\n",
              "      <td>0.106159</td>\n",
              "      <td>0.027451</td>\n",
              "      <td>0.038462</td>\n",
              "      <td>0.007366</td>\n",
              "      <td>0.036397</td>\n",
              "      <td>0.092634</td>\n",
              "      <td>0.009402</td>\n",
              "      <td>0.028459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>652417</th>\n",
              "      <td>wave</td>\n",
              "      <td>0.119008</td>\n",
              "      <td>0.061911</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>0.009272</td>\n",
              "      <td>0.077280</td>\n",
              "      <td>0.100636</td>\n",
              "      <td>0.014014</td>\n",
              "      <td>0.027829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494631</th>\n",
              "      <td>resonance</td>\n",
              "      <td>0.099746</td>\n",
              "      <td>0.036827</td>\n",
              "      <td>0.040791</td>\n",
              "      <td>0.005272</td>\n",
              "      <td>0.045886</td>\n",
              "      <td>0.085394</td>\n",
              "      <td>0.013981</td>\n",
              "      <td>0.027739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163485</th>\n",
              "      <td>driven</td>\n",
              "      <td>0.121597</td>\n",
              "      <td>0.023256</td>\n",
              "      <td>0.036442</td>\n",
              "      <td>0.005282</td>\n",
              "      <td>0.035128</td>\n",
              "      <td>0.105823</td>\n",
              "      <td>0.010659</td>\n",
              "      <td>0.027443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324559</th>\n",
              "      <td>learning</td>\n",
              "      <td>0.055689</td>\n",
              "      <td>0.000823</td>\n",
              "      <td>0.032204</td>\n",
              "      <td>0.000673</td>\n",
              "      <td>0.000793</td>\n",
              "      <td>0.052870</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.027222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133227</th>\n",
              "      <td>dependence</td>\n",
              "      <td>0.281184</td>\n",
              "      <td>0.194737</td>\n",
              "      <td>0.041620</td>\n",
              "      <td>0.011429</td>\n",
              "      <td>0.218077</td>\n",
              "      <td>0.246362</td>\n",
              "      <td>0.016688</td>\n",
              "      <td>0.027180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259637</th>\n",
              "      <td>hall</td>\n",
              "      <td>0.070896</td>\n",
              "      <td>0.015385</td>\n",
              "      <td>0.038462</td>\n",
              "      <td>0.008816</td>\n",
              "      <td>0.026911</td>\n",
              "      <td>0.058189</td>\n",
              "      <td>0.015437</td>\n",
              "      <td>0.026692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74776</th>\n",
              "      <td>chain</td>\n",
              "      <td>0.079190</td>\n",
              "      <td>0.028634</td>\n",
              "      <td>0.036735</td>\n",
              "      <td>0.007947</td>\n",
              "      <td>0.041166</td>\n",
              "      <td>0.066121</td>\n",
              "      <td>0.014646</td>\n",
              "      <td>0.026691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>601242</th>\n",
              "      <td>thermal</td>\n",
              "      <td>0.107256</td>\n",
              "      <td>0.036474</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.007417</td>\n",
              "      <td>0.047579</td>\n",
              "      <td>0.098269</td>\n",
              "      <td>0.010288</td>\n",
              "      <td>0.026616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121454</th>\n",
              "      <td>current</td>\n",
              "      <td>0.125277</td>\n",
              "      <td>0.062847</td>\n",
              "      <td>0.034611</td>\n",
              "      <td>0.005282</td>\n",
              "      <td>0.071560</td>\n",
              "      <td>0.112559</td>\n",
              "      <td>0.012870</td>\n",
              "      <td>0.026451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443669</th>\n",
              "      <td>polarization</td>\n",
              "      <td>0.108564</td>\n",
              "      <td>0.033956</td>\n",
              "      <td>0.033040</td>\n",
              "      <td>0.004249</td>\n",
              "      <td>0.043017</td>\n",
              "      <td>0.099392</td>\n",
              "      <td>0.007951</td>\n",
              "      <td>0.026208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221611</th>\n",
              "      <td>films</td>\n",
              "      <td>0.058716</td>\n",
              "      <td>0.014060</td>\n",
              "      <td>0.041868</td>\n",
              "      <td>0.007030</td>\n",
              "      <td>0.020489</td>\n",
              "      <td>0.047787</td>\n",
              "      <td>0.010703</td>\n",
              "      <td>0.024775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24816</th>\n",
              "      <td>antiferromagnetic</td>\n",
              "      <td>0.096118</td>\n",
              "      <td>0.036137</td>\n",
              "      <td>0.042514</td>\n",
              "      <td>0.006070</td>\n",
              "      <td>0.043714</td>\n",
              "      <td>0.072477</td>\n",
              "      <td>0.013197</td>\n",
              "      <td>0.024771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649853</th>\n",
              "      <td>vortex</td>\n",
              "      <td>0.061192</td>\n",
              "      <td>0.009422</td>\n",
              "      <td>0.035427</td>\n",
              "      <td>0.002706</td>\n",
              "      <td>0.014259</td>\n",
              "      <td>0.044650</td>\n",
              "      <td>0.005367</td>\n",
              "      <td>0.024771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246032</th>\n",
              "      <td>gas</td>\n",
              "      <td>0.074502</td>\n",
              "      <td>0.027696</td>\n",
              "      <td>0.039829</td>\n",
              "      <td>0.006418</td>\n",
              "      <td>0.033176</td>\n",
              "      <td>0.060329</td>\n",
              "      <td>0.009620</td>\n",
              "      <td>0.024767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403096</th>\n",
              "      <td>one dimensional</td>\n",
              "      <td>0.095238</td>\n",
              "      <td>0.037164</td>\n",
              "      <td>0.036842</td>\n",
              "      <td>0.007063</td>\n",
              "      <td>0.050842</td>\n",
              "      <td>0.073089</td>\n",
              "      <td>0.010833</td>\n",
              "      <td>0.024465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>561252</th>\n",
              "      <td>strong</td>\n",
              "      <td>0.211509</td>\n",
              "      <td>0.114943</td>\n",
              "      <td>0.034158</td>\n",
              "      <td>0.007084</td>\n",
              "      <td>0.141759</td>\n",
              "      <td>0.199056</td>\n",
              "      <td>0.013004</td>\n",
              "      <td>0.024462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>528846</th>\n",
              "      <td>simulations</td>\n",
              "      <td>0.230289</td>\n",
              "      <td>0.092522</td>\n",
              "      <td>0.035411</td>\n",
              "      <td>0.008876</td>\n",
              "      <td>0.117197</td>\n",
              "      <td>0.204543</td>\n",
              "      <td>0.013228</td>\n",
              "      <td>0.024217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610020</th>\n",
              "      <td>time</td>\n",
              "      <td>0.212486</td>\n",
              "      <td>0.116190</td>\n",
              "      <td>0.032110</td>\n",
              "      <td>0.010110</td>\n",
              "      <td>0.136120</td>\n",
              "      <td>0.198384</td>\n",
              "      <td>0.014942</td>\n",
              "      <td>0.024205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424002</th>\n",
              "      <td>particle</td>\n",
              "      <td>0.144181</td>\n",
              "      <td>0.077844</td>\n",
              "      <td>0.033451</td>\n",
              "      <td>0.007186</td>\n",
              "      <td>0.094100</td>\n",
              "      <td>0.129505</td>\n",
              "      <td>0.013945</td>\n",
              "      <td>0.024169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123440</th>\n",
              "      <td>d</td>\n",
              "      <td>0.116412</td>\n",
              "      <td>0.039289</td>\n",
              "      <td>0.038817</td>\n",
              "      <td>0.002230</td>\n",
              "      <td>0.049718</td>\n",
              "      <td>0.094801</td>\n",
              "      <td>0.004928</td>\n",
              "      <td>0.023853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>624264</th>\n",
              "      <td>tunneling</td>\n",
              "      <td>0.081031</td>\n",
              "      <td>0.030120</td>\n",
              "      <td>0.033956</td>\n",
              "      <td>0.004450</td>\n",
              "      <td>0.037671</td>\n",
              "      <td>0.061192</td>\n",
              "      <td>0.007525</td>\n",
              "      <td>0.023777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>353709</th>\n",
              "      <td>mechanism</td>\n",
              "      <td>0.197006</td>\n",
              "      <td>0.070476</td>\n",
              "      <td>0.028956</td>\n",
              "      <td>0.006135</td>\n",
              "      <td>0.082803</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.012887</td>\n",
              "      <td>0.023741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472777</th>\n",
              "      <td>quasi</td>\n",
              "      <td>0.095714</td>\n",
              "      <td>0.052427</td>\n",
              "      <td>0.037143</td>\n",
              "      <td>0.006608</td>\n",
              "      <td>0.063224</td>\n",
              "      <td>0.078410</td>\n",
              "      <td>0.011444</td>\n",
              "      <td>0.023547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>511010</th>\n",
              "      <td>scattering</td>\n",
              "      <td>0.119005</td>\n",
              "      <td>0.057092</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.008344</td>\n",
              "      <td>0.066944</td>\n",
              "      <td>0.102816</td>\n",
              "      <td>0.011574</td>\n",
              "      <td>0.023547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434364</th>\n",
              "      <td>phase transition</td>\n",
              "      <td>0.097547</td>\n",
              "      <td>0.046099</td>\n",
              "      <td>0.034394</td>\n",
              "      <td>0.011450</td>\n",
              "      <td>0.056667</td>\n",
              "      <td>0.078582</td>\n",
              "      <td>0.016941</td>\n",
              "      <td>0.023414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107338</th>\n",
              "      <td>control</td>\n",
              "      <td>0.138257</td>\n",
              "      <td>0.022099</td>\n",
              "      <td>0.031274</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>0.030275</td>\n",
              "      <td>0.124537</td>\n",
              "      <td>0.004628</td>\n",
              "      <td>0.023016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>556797</th>\n",
              "      <td>statistical</td>\n",
              "      <td>0.084302</td>\n",
              "      <td>0.034004</td>\n",
              "      <td>0.033987</td>\n",
              "      <td>0.005705</td>\n",
              "      <td>0.045359</td>\n",
              "      <td>0.064748</td>\n",
              "      <td>0.009209</td>\n",
              "      <td>0.022962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28462</th>\n",
              "      <td>approach</td>\n",
              "      <td>0.193757</td>\n",
              "      <td>0.083736</td>\n",
              "      <td>0.031116</td>\n",
              "      <td>0.008547</td>\n",
              "      <td>0.100735</td>\n",
              "      <td>0.170370</td>\n",
              "      <td>0.014010</td>\n",
              "      <td>0.022940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321990</th>\n",
              "      <td>layer</td>\n",
              "      <td>0.130932</td>\n",
              "      <td>0.030560</td>\n",
              "      <td>0.031746</td>\n",
              "      <td>0.004854</td>\n",
              "      <td>0.045193</td>\n",
              "      <td>0.112297</td>\n",
              "      <td>0.009283</td>\n",
              "      <td>0.022759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>647191</th>\n",
              "      <td>via</td>\n",
              "      <td>0.104651</td>\n",
              "      <td>0.022378</td>\n",
              "      <td>0.027340</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.032348</td>\n",
              "      <td>0.093262</td>\n",
              "      <td>0.001835</td>\n",
              "      <td>0.022416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50537</th>\n",
              "      <td>between</td>\n",
              "      <td>0.330394</td>\n",
              "      <td>0.199605</td>\n",
              "      <td>0.031712</td>\n",
              "      <td>0.008547</td>\n",
              "      <td>0.229358</td>\n",
              "      <td>0.308056</td>\n",
              "      <td>0.012436</td>\n",
              "      <td>0.022298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>605781</th>\n",
              "      <td>thin</td>\n",
              "      <td>0.068611</td>\n",
              "      <td>0.017787</td>\n",
              "      <td>0.030541</td>\n",
              "      <td>0.005059</td>\n",
              "      <td>0.023840</td>\n",
              "      <td>0.057038</td>\n",
              "      <td>0.008257</td>\n",
              "      <td>0.021996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7337</th>\n",
              "      <td>active</td>\n",
              "      <td>0.080588</td>\n",
              "      <td>0.007105</td>\n",
              "      <td>0.028522</td>\n",
              "      <td>0.001122</td>\n",
              "      <td>0.020251</td>\n",
              "      <td>0.068152</td>\n",
              "      <td>0.001769</td>\n",
              "      <td>0.021574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121155</th>\n",
              "      <td>cuprates</td>\n",
              "      <td>0.054386</td>\n",
              "      <td>0.006653</td>\n",
              "      <td>0.031480</td>\n",
              "      <td>0.001331</td>\n",
              "      <td>0.012082</td>\n",
              "      <td>0.037003</td>\n",
              "      <td>0.003603</td>\n",
              "      <td>0.021407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160957</th>\n",
              "      <td>doped</td>\n",
              "      <td>0.054628</td>\n",
              "      <td>0.016420</td>\n",
              "      <td>0.027778</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>0.027230</td>\n",
              "      <td>0.042029</td>\n",
              "      <td>0.010430</td>\n",
              "      <td>0.021266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59730</th>\n",
              "      <td>boundary</td>\n",
              "      <td>0.129012</td>\n",
              "      <td>0.043845</td>\n",
              "      <td>0.032919</td>\n",
              "      <td>0.002535</td>\n",
              "      <td>0.064294</td>\n",
              "      <td>0.113079</td>\n",
              "      <td>0.008266</td>\n",
              "      <td>0.021247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22619</th>\n",
              "      <td>anisotropy</td>\n",
              "      <td>0.086181</td>\n",
              "      <td>0.031972</td>\n",
              "      <td>0.032689</td>\n",
              "      <td>0.005217</td>\n",
              "      <td>0.046103</td>\n",
              "      <td>0.070387</td>\n",
              "      <td>0.012132</td>\n",
              "      <td>0.021185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436150</th>\n",
              "      <td>phonon</td>\n",
              "      <td>0.076230</td>\n",
              "      <td>0.021127</td>\n",
              "      <td>0.029977</td>\n",
              "      <td>0.005545</td>\n",
              "      <td>0.033060</td>\n",
              "      <td>0.059573</td>\n",
              "      <td>0.009786</td>\n",
              "      <td>0.020944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311087</th>\n",
              "      <td>junction</td>\n",
              "      <td>0.056034</td>\n",
              "      <td>0.012882</td>\n",
              "      <td>0.033215</td>\n",
              "      <td>0.005312</td>\n",
              "      <td>0.024699</td>\n",
              "      <td>0.035786</td>\n",
              "      <td>0.009039</td>\n",
              "      <td>0.020929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147138</th>\n",
              "      <td>diffusion</td>\n",
              "      <td>0.074713</td>\n",
              "      <td>0.033215</td>\n",
              "      <td>0.030534</td>\n",
              "      <td>0.006832</td>\n",
              "      <td>0.045557</td>\n",
              "      <td>0.060245</td>\n",
              "      <td>0.012203</td>\n",
              "      <td>0.020817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336305</th>\n",
              "      <td>low</td>\n",
              "      <td>0.185945</td>\n",
              "      <td>0.124046</td>\n",
              "      <td>0.033384</td>\n",
              "      <td>0.007982</td>\n",
              "      <td>0.141676</td>\n",
              "      <td>0.174343</td>\n",
              "      <td>0.010833</td>\n",
              "      <td>0.020795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630917</th>\n",
              "      <td>under</td>\n",
              "      <td>0.315665</td>\n",
              "      <td>0.109155</td>\n",
              "      <td>0.033577</td>\n",
              "      <td>0.007042</td>\n",
              "      <td>0.125265</td>\n",
              "      <td>0.288056</td>\n",
              "      <td>0.009706</td>\n",
              "      <td>0.020552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514544</th>\n",
              "      <td>self</td>\n",
              "      <td>0.085648</td>\n",
              "      <td>0.038732</td>\n",
              "      <td>0.032578</td>\n",
              "      <td>0.004559</td>\n",
              "      <td>0.053685</td>\n",
              "      <td>0.069668</td>\n",
              "      <td>0.012132</td>\n",
              "      <td>0.020545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162280</th>\n",
              "      <td>dots</td>\n",
              "      <td>0.037775</td>\n",
              "      <td>0.006281</td>\n",
              "      <td>0.031169</td>\n",
              "      <td>0.003081</td>\n",
              "      <td>0.011520</td>\n",
              "      <td>0.027509</td>\n",
              "      <td>0.005063</td>\n",
              "      <td>0.020545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296852</th>\n",
              "      <td>interface</td>\n",
              "      <td>0.078086</td>\n",
              "      <td>0.019324</td>\n",
              "      <td>0.025819</td>\n",
              "      <td>0.003610</td>\n",
              "      <td>0.026911</td>\n",
              "      <td>0.067528</td>\n",
              "      <td>0.008101</td>\n",
              "      <td>0.020333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396876</th>\n",
              "      <td>o</td>\n",
              "      <td>0.067769</td>\n",
              "      <td>0.020343</td>\n",
              "      <td>0.033549</td>\n",
              "      <td>0.004283</td>\n",
              "      <td>0.030411</td>\n",
              "      <td>0.049537</td>\n",
              "      <td>0.007785</td>\n",
              "      <td>0.020185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515333</th>\n",
              "      <td>semiconductor</td>\n",
              "      <td>0.067974</td>\n",
              "      <td>0.018895</td>\n",
              "      <td>0.030624</td>\n",
              "      <td>0.005525</td>\n",
              "      <td>0.024503</td>\n",
              "      <td>0.057881</td>\n",
              "      <td>0.011081</td>\n",
              "      <td>0.020024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197289</th>\n",
              "      <td>exact</td>\n",
              "      <td>0.125954</td>\n",
              "      <td>0.050542</td>\n",
              "      <td>0.028626</td>\n",
              "      <td>0.003209</td>\n",
              "      <td>0.059033</td>\n",
              "      <td>0.101529</td>\n",
              "      <td>0.005807</td>\n",
              "      <td>0.020018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224426</th>\n",
              "      <td>finite</td>\n",
              "      <td>0.147664</td>\n",
              "      <td>0.067143</td>\n",
              "      <td>0.031776</td>\n",
              "      <td>0.005291</td>\n",
              "      <td>0.087122</td>\n",
              "      <td>0.112527</td>\n",
              "      <td>0.009074</td>\n",
              "      <td>0.020018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>351906</th>\n",
              "      <td>measurements</td>\n",
              "      <td>0.205567</td>\n",
              "      <td>0.112754</td>\n",
              "      <td>0.026490</td>\n",
              "      <td>0.004926</td>\n",
              "      <td>0.130422</td>\n",
              "      <td>0.184330</td>\n",
              "      <td>0.010312</td>\n",
              "      <td>0.019825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23788</th>\n",
              "      <td>anomalous</td>\n",
              "      <td>0.066510</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.027675</td>\n",
              "      <td>0.003534</td>\n",
              "      <td>0.033760</td>\n",
              "      <td>0.056506</td>\n",
              "      <td>0.008525</td>\n",
              "      <td>0.019759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157686</th>\n",
              "      <td>distribution</td>\n",
              "      <td>0.141369</td>\n",
              "      <td>0.065089</td>\n",
              "      <td>0.028419</td>\n",
              "      <td>0.003819</td>\n",
              "      <td>0.081105</td>\n",
              "      <td>0.121062</td>\n",
              "      <td>0.006679</td>\n",
              "      <td>0.019726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>470722</th>\n",
              "      <td>quantum dots</td>\n",
              "      <td>0.034627</td>\n",
              "      <td>0.005653</td>\n",
              "      <td>0.029381</td>\n",
              "      <td>0.003081</td>\n",
              "      <td>0.010771</td>\n",
              "      <td>0.023606</td>\n",
              "      <td>0.005050</td>\n",
              "      <td>0.019612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>543378</th>\n",
              "      <td>spectroscopy</td>\n",
              "      <td>0.081004</td>\n",
              "      <td>0.011189</td>\n",
              "      <td>0.028470</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>0.018200</td>\n",
              "      <td>0.073148</td>\n",
              "      <td>0.006665</td>\n",
              "      <td>0.019424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>546786</th>\n",
              "      <td>spin orbit</td>\n",
              "      <td>0.065758</td>\n",
              "      <td>0.003738</td>\n",
              "      <td>0.028045</td>\n",
              "      <td>0.001754</td>\n",
              "      <td>0.008984</td>\n",
              "      <td>0.057085</td>\n",
              "      <td>0.002730</td>\n",
              "      <td>0.019342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80772</th>\n",
              "      <td>chiral</td>\n",
              "      <td>0.054765</td>\n",
              "      <td>0.001642</td>\n",
              "      <td>0.023810</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.010150</td>\n",
              "      <td>0.047633</td>\n",
              "      <td>0.003170</td>\n",
              "      <td>0.019314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515852</th>\n",
              "      <td>semimetal</td>\n",
              "      <td>0.037464</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>0.026756</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.000545</td>\n",
              "      <td>0.030730</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.019192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>442317</th>\n",
              "      <td>point</td>\n",
              "      <td>0.159298</td>\n",
              "      <td>0.082645</td>\n",
              "      <td>0.028881</td>\n",
              "      <td>0.006912</td>\n",
              "      <td>0.111064</td>\n",
              "      <td>0.135942</td>\n",
              "      <td>0.009205</td>\n",
              "      <td>0.019089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379981</th>\n",
              "      <td>nanotubes</td>\n",
              "      <td>0.035264</td>\n",
              "      <td>0.001766</td>\n",
              "      <td>0.033696</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.005743</td>\n",
              "      <td>0.024907</td>\n",
              "      <td>0.002674</td>\n",
              "      <td>0.019051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445139</th>\n",
              "      <td>polymer</td>\n",
              "      <td>0.043893</td>\n",
              "      <td>0.013575</td>\n",
              "      <td>0.028169</td>\n",
              "      <td>0.003759</td>\n",
              "      <td>0.018609</td>\n",
              "      <td>0.030331</td>\n",
              "      <td>0.008074</td>\n",
              "      <td>0.018805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>471204</th>\n",
              "      <td>quantum hall</td>\n",
              "      <td>0.034783</td>\n",
              "      <td>0.007345</td>\n",
              "      <td>0.028419</td>\n",
              "      <td>0.002423</td>\n",
              "      <td>0.011408</td>\n",
              "      <td>0.025174</td>\n",
              "      <td>0.005577</td>\n",
              "      <td>0.018805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47236</th>\n",
              "      <td>behavior</td>\n",
              "      <td>0.154623</td>\n",
              "      <td>0.089333</td>\n",
              "      <td>0.026490</td>\n",
              "      <td>0.004058</td>\n",
              "      <td>0.113560</td>\n",
              "      <td>0.131742</td>\n",
              "      <td>0.007724</td>\n",
              "      <td>0.018786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>447672</th>\n",
              "      <td>potential</td>\n",
              "      <td>0.202994</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>0.025184</td>\n",
              "      <td>0.002833</td>\n",
              "      <td>0.099646</td>\n",
              "      <td>0.181204</td>\n",
              "      <td>0.010361</td>\n",
              "      <td>0.018698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505988</th>\n",
              "      <td>s</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.101504</td>\n",
              "      <td>0.031776</td>\n",
              "      <td>0.004274</td>\n",
              "      <td>0.116893</td>\n",
              "      <td>0.151389</td>\n",
              "      <td>0.012375</td>\n",
              "      <td>0.018502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54284</th>\n",
              "      <td>bilayer</td>\n",
              "      <td>0.044785</td>\n",
              "      <td>0.003396</td>\n",
              "      <td>0.026741</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.005810</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>0.002141</td>\n",
              "      <td>0.018426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415214</th>\n",
              "      <td>oscillations</td>\n",
              "      <td>0.085253</td>\n",
              "      <td>0.035088</td>\n",
              "      <td>0.029677</td>\n",
              "      <td>0.005217</td>\n",
              "      <td>0.041928</td>\n",
              "      <td>0.068587</td>\n",
              "      <td>0.008981</td>\n",
              "      <td>0.018338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>338981</th>\n",
              "      <td>machine</td>\n",
              "      <td>0.046707</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.021099</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.000808</td>\n",
              "      <td>0.042130</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.018333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69538</th>\n",
              "      <td>carbon</td>\n",
              "      <td>0.045340</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.032609</td>\n",
              "      <td>0.002221</td>\n",
              "      <td>0.010346</td>\n",
              "      <td>0.029740</td>\n",
              "      <td>0.005248</td>\n",
              "      <td>0.018216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190806</th>\n",
              "      <td>equation</td>\n",
              "      <td>0.138922</td>\n",
              "      <td>0.072267</td>\n",
              "      <td>0.033647</td>\n",
              "      <td>0.005495</td>\n",
              "      <td>0.082407</td>\n",
              "      <td>0.119249</td>\n",
              "      <td>0.008714</td>\n",
              "      <td>0.017943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>608072</th>\n",
              "      <td>through</td>\n",
              "      <td>0.174323</td>\n",
              "      <td>0.049236</td>\n",
              "      <td>0.026265</td>\n",
              "      <td>0.004608</td>\n",
              "      <td>0.065454</td>\n",
              "      <td>0.154352</td>\n",
              "      <td>0.006116</td>\n",
              "      <td>0.017931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>613674</th>\n",
              "      <td>topological insulator</td>\n",
              "      <td>0.040268</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.026116</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.034364</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.017812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>542628</th>\n",
              "      <td>spectra</td>\n",
              "      <td>0.091969</td>\n",
              "      <td>0.041333</td>\n",
              "      <td>0.029557</td>\n",
              "      <td>0.003958</td>\n",
              "      <td>0.054305</td>\n",
              "      <td>0.076245</td>\n",
              "      <td>0.007052</td>\n",
              "      <td>0.017737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187258</th>\n",
              "      <td>enhanced</td>\n",
              "      <td>0.111307</td>\n",
              "      <td>0.020333</td>\n",
              "      <td>0.024825</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.032454</td>\n",
              "      <td>0.095833</td>\n",
              "      <td>0.005156</td>\n",
              "      <td>0.017626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56030</th>\n",
              "      <td>body</td>\n",
              "      <td>0.089798</td>\n",
              "      <td>0.017831</td>\n",
              "      <td>0.026420</td>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.023686</td>\n",
              "      <td>0.075253</td>\n",
              "      <td>0.003383</td>\n",
              "      <td>0.017617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40461</th>\n",
              "      <td>band</td>\n",
              "      <td>0.126374</td>\n",
              "      <td>0.038750</td>\n",
              "      <td>0.023632</td>\n",
              "      <td>0.003552</td>\n",
              "      <td>0.045799</td>\n",
              "      <td>0.114626</td>\n",
              "      <td>0.006152</td>\n",
              "      <td>0.017617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>488269</th>\n",
              "      <td>relaxation</td>\n",
              "      <td>0.082437</td>\n",
              "      <td>0.033201</td>\n",
              "      <td>0.027826</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>0.045278</td>\n",
              "      <td>0.062679</td>\n",
              "      <td>0.007500</td>\n",
              "      <td>0.017592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225288</th>\n",
              "      <td>first</td>\n",
              "      <td>0.177764</td>\n",
              "      <td>0.085648</td>\n",
              "      <td>0.023725</td>\n",
              "      <td>0.002692</td>\n",
              "      <td>0.101258</td>\n",
              "      <td>0.164115</td>\n",
              "      <td>0.008087</td>\n",
              "      <td>0.017524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188550</th>\n",
              "      <td>entanglement</td>\n",
              "      <td>0.050907</td>\n",
              "      <td>0.003515</td>\n",
              "      <td>0.026042</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.005199</td>\n",
              "      <td>0.042029</td>\n",
              "      <td>0.002752</td>\n",
              "      <td>0.017492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161908</th>\n",
              "      <td>dot</td>\n",
              "      <td>0.048913</td>\n",
              "      <td>0.007156</td>\n",
              "      <td>0.023913</td>\n",
              "      <td>0.003769</td>\n",
              "      <td>0.014523</td>\n",
              "      <td>0.033643</td>\n",
              "      <td>0.006224</td>\n",
              "      <td>0.017472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305122</th>\n",
              "      <td>iron</td>\n",
              "      <td>0.036145</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.023346</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.029246</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.017439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281402</th>\n",
              "      <td>impurity</td>\n",
              "      <td>0.059150</td>\n",
              "      <td>0.017365</td>\n",
              "      <td>0.022609</td>\n",
              "      <td>0.003029</td>\n",
              "      <td>0.022416</td>\n",
              "      <td>0.049541</td>\n",
              "      <td>0.005468</td>\n",
              "      <td>0.017431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89015</th>\n",
              "      <td>comment</td>\n",
              "      <td>0.024730</td>\n",
              "      <td>0.001078</td>\n",
              "      <td>0.026408</td>\n",
              "      <td>0.000702</td>\n",
              "      <td>0.003772</td>\n",
              "      <td>0.014559</td>\n",
              "      <td>0.002001</td>\n",
              "      <td>0.017405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212869</th>\n",
              "      <td>fe</td>\n",
              "      <td>0.059081</td>\n",
              "      <td>0.002762</td>\n",
              "      <td>0.026042</td>\n",
              "      <td>0.001328</td>\n",
              "      <td>0.008037</td>\n",
              "      <td>0.048225</td>\n",
              "      <td>0.003058</td>\n",
              "      <td>0.017257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345427</th>\n",
              "      <td>many</td>\n",
              "      <td>0.134069</td>\n",
              "      <td>0.036832</td>\n",
              "      <td>0.023190</td>\n",
              "      <td>0.002361</td>\n",
              "      <td>0.046789</td>\n",
              "      <td>0.122986</td>\n",
              "      <td>0.004689</td>\n",
              "      <td>0.017247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359315</th>\n",
              "      <td>mgb</td>\n",
              "      <td>0.042735</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.031339</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000299</td>\n",
              "      <td>0.020692</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.017152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305495</th>\n",
              "      <td>ising</td>\n",
              "      <td>0.053286</td>\n",
              "      <td>0.015404</td>\n",
              "      <td>0.026455</td>\n",
              "      <td>0.004184</td>\n",
              "      <td>0.024654</td>\n",
              "      <td>0.040979</td>\n",
              "      <td>0.008021</td>\n",
              "      <td>0.017125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151140</th>\n",
              "      <td>dirac</td>\n",
              "      <td>0.055019</td>\n",
              "      <td>0.001167</td>\n",
              "      <td>0.021665</td>\n",
              "      <td>0.001104</td>\n",
              "      <td>0.003674</td>\n",
              "      <td>0.046899</td>\n",
              "      <td>0.000696</td>\n",
              "      <td>0.017014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454043</th>\n",
              "      <td>pressure</td>\n",
              "      <td>0.057681</td>\n",
              "      <td>0.011091</td>\n",
              "      <td>0.025717</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.021101</td>\n",
              "      <td>0.048056</td>\n",
              "      <td>0.004893</td>\n",
              "      <td>0.016894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>603</th>\n",
              "      <td>2d</td>\n",
              "      <td>0.078843</td>\n",
              "      <td>0.016478</td>\n",
              "      <td>0.026718</td>\n",
              "      <td>0.003617</td>\n",
              "      <td>0.022191</td>\n",
              "      <td>0.068354</td>\n",
              "      <td>0.005148</td>\n",
              "      <td>0.016880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>602145</th>\n",
              "      <td>thermodynamic</td>\n",
              "      <td>0.077571</td>\n",
              "      <td>0.028261</td>\n",
              "      <td>0.031304</td>\n",
              "      <td>0.004651</td>\n",
              "      <td>0.041192</td>\n",
              "      <td>0.063083</td>\n",
              "      <td>0.008916</td>\n",
              "      <td>0.016820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162525</th>\n",
              "      <td>double</td>\n",
              "      <td>0.056462</td>\n",
              "      <td>0.021761</td>\n",
              "      <td>0.029644</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.031296</td>\n",
              "      <td>0.044078</td>\n",
              "      <td>0.008283</td>\n",
              "      <td>0.016774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>470528</th>\n",
              "      <td>quantum dot</td>\n",
              "      <td>0.034884</td>\n",
              "      <td>0.006086</td>\n",
              "      <td>0.022826</td>\n",
              "      <td>0.002513</td>\n",
              "      <td>0.011759</td>\n",
              "      <td>0.027695</td>\n",
              "      <td>0.005928</td>\n",
              "      <td>0.016729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371771</th>\n",
              "      <td>monolayer</td>\n",
              "      <td>0.052239</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.024902</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.004084</td>\n",
              "      <td>0.040145</td>\n",
              "      <td>0.000817</td>\n",
              "      <td>0.016648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254679</th>\n",
              "      <td>granular</td>\n",
              "      <td>0.036626</td>\n",
              "      <td>0.003534</td>\n",
              "      <td>0.033296</td>\n",
              "      <td>0.002282</td>\n",
              "      <td>0.006004</td>\n",
              "      <td>0.021909</td>\n",
              "      <td>0.003703</td>\n",
              "      <td>0.016575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91326</th>\n",
              "      <td>complex</td>\n",
              "      <td>0.116366</td>\n",
              "      <td>0.025806</td>\n",
              "      <td>0.023307</td>\n",
              "      <td>0.003364</td>\n",
              "      <td>0.036700</td>\n",
              "      <td>0.096852</td>\n",
              "      <td>0.005460</td>\n",
              "      <td>0.016455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26813</th>\n",
              "      <td>applied</td>\n",
              "      <td>0.298107</td>\n",
              "      <td>0.113027</td>\n",
              "      <td>0.025157</td>\n",
              "      <td>0.005019</td>\n",
              "      <td>0.123853</td>\n",
              "      <td>0.278582</td>\n",
              "      <td>0.008869</td>\n",
              "      <td>0.016427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274294</th>\n",
              "      <td>hubbard</td>\n",
              "      <td>0.045775</td>\n",
              "      <td>0.015990</td>\n",
              "      <td>0.026273</td>\n",
              "      <td>0.003924</td>\n",
              "      <td>0.023916</td>\n",
              "      <td>0.036106</td>\n",
              "      <td>0.008368</td>\n",
              "      <td>0.016379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57517</th>\n",
              "      <td>bosonic</td>\n",
              "      <td>0.057797</td>\n",
              "      <td>0.024431</td>\n",
              "      <td>0.023613</td>\n",
              "      <td>0.002912</td>\n",
              "      <td>0.030300</td>\n",
              "      <td>0.041760</td>\n",
              "      <td>0.007308</td>\n",
              "      <td>0.016283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>606952</th>\n",
              "      <td>three</td>\n",
              "      <td>0.127907</td>\n",
              "      <td>0.057229</td>\n",
              "      <td>0.024969</td>\n",
              "      <td>0.002099</td>\n",
              "      <td>0.077518</td>\n",
              "      <td>0.115684</td>\n",
              "      <td>0.007097</td>\n",
              "      <td>0.016088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84302</th>\n",
              "      <td>cluster</td>\n",
              "      <td>0.061538</td>\n",
              "      <td>0.022430</td>\n",
              "      <td>0.027223</td>\n",
              "      <td>0.003578</td>\n",
              "      <td>0.028990</td>\n",
              "      <td>0.046655</td>\n",
              "      <td>0.006154</td>\n",
              "      <td>0.016059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>584901</th>\n",
              "      <td>t</td>\n",
              "      <td>0.175523</td>\n",
              "      <td>0.073133</td>\n",
              "      <td>0.027668</td>\n",
              "      <td>0.001691</td>\n",
              "      <td>0.081858</td>\n",
              "      <td>0.158104</td>\n",
              "      <td>0.003493</td>\n",
              "      <td>0.015902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>575103</th>\n",
              "      <td>superfluid</td>\n",
              "      <td>0.047149</td>\n",
              "      <td>0.009881</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>0.001976</td>\n",
              "      <td>0.013761</td>\n",
              "      <td>0.034554</td>\n",
              "      <td>0.006004</td>\n",
              "      <td>0.015876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191949</th>\n",
              "      <td>equilibrium</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.036866</td>\n",
              "      <td>0.024882</td>\n",
              "      <td>0.002364</td>\n",
              "      <td>0.049534</td>\n",
              "      <td>0.077507</td>\n",
              "      <td>0.008916</td>\n",
              "      <td>0.015866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388319</th>\n",
              "      <td>noise</td>\n",
              "      <td>0.046985</td>\n",
              "      <td>0.018764</td>\n",
              "      <td>0.025166</td>\n",
              "      <td>0.003975</td>\n",
              "      <td>0.024325</td>\n",
              "      <td>0.036160</td>\n",
              "      <td>0.004951</td>\n",
              "      <td>0.015860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66248</th>\n",
              "      <td>calculations</td>\n",
              "      <td>0.247036</td>\n",
              "      <td>0.153197</td>\n",
              "      <td>0.029565</td>\n",
              "      <td>0.002395</td>\n",
              "      <td>0.177627</td>\n",
              "      <td>0.208257</td>\n",
              "      <td>0.007805</td>\n",
              "      <td>0.015799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>537911</th>\n",
              "      <td>solution</td>\n",
              "      <td>0.093913</td>\n",
              "      <td>0.043860</td>\n",
              "      <td>0.022430</td>\n",
              "      <td>0.004637</td>\n",
              "      <td>0.055760</td>\n",
              "      <td>0.074917</td>\n",
              "      <td>0.006883</td>\n",
              "      <td>0.015772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437446</th>\n",
              "      <td>photon</td>\n",
              "      <td>0.059723</td>\n",
              "      <td>0.003630</td>\n",
              "      <td>0.023256</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.011222</td>\n",
              "      <td>0.051939</td>\n",
              "      <td>0.002426</td>\n",
              "      <td>0.015744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>338994</th>\n",
              "      <td>machine learning</td>\n",
              "      <td>0.041916</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.018878</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.036852</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.015741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69636</th>\n",
              "      <td>carbon nanotubes</td>\n",
              "      <td>0.034005</td>\n",
              "      <td>0.001076</td>\n",
              "      <td>0.030435</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.003747</td>\n",
              "      <td>0.021747</td>\n",
              "      <td>0.001782</td>\n",
              "      <td>0.015547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189307</th>\n",
              "      <td>entropy</td>\n",
              "      <td>0.063017</td>\n",
              "      <td>0.015581</td>\n",
              "      <td>0.022915</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.024231</td>\n",
              "      <td>0.051236</td>\n",
              "      <td>0.005307</td>\n",
              "      <td>0.015511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>455301</th>\n",
              "      <td>principles</td>\n",
              "      <td>0.094849</td>\n",
              "      <td>0.017713</td>\n",
              "      <td>0.021352</td>\n",
              "      <td>0.003306</td>\n",
              "      <td>0.028135</td>\n",
              "      <td>0.086642</td>\n",
              "      <td>0.005173</td>\n",
              "      <td>0.015496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127192</th>\n",
              "      <td>defects</td>\n",
              "      <td>0.061632</td>\n",
              "      <td>0.009242</td>\n",
              "      <td>0.020025</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.017288</td>\n",
              "      <td>0.052315</td>\n",
              "      <td>0.004550</td>\n",
              "      <td>0.015463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>456325</th>\n",
              "      <td>probe</td>\n",
              "      <td>0.068109</td>\n",
              "      <td>0.007634</td>\n",
              "      <td>0.022539</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.016013</td>\n",
              "      <td>0.057540</td>\n",
              "      <td>0.003538</td>\n",
              "      <td>0.015441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390945</th>\n",
              "      <td>nonlinear</td>\n",
              "      <td>0.058140</td>\n",
              "      <td>0.015581</td>\n",
              "      <td>0.022989</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.026580</td>\n",
              "      <td>0.044253</td>\n",
              "      <td>0.008670</td>\n",
              "      <td>0.015291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>559091</th>\n",
              "      <td>strain</td>\n",
              "      <td>0.059758</td>\n",
              "      <td>0.005272</td>\n",
              "      <td>0.020915</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.009480</td>\n",
              "      <td>0.047024</td>\n",
              "      <td>0.002123</td>\n",
              "      <td>0.015268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486692</th>\n",
              "      <td>related</td>\n",
              "      <td>0.201739</td>\n",
              "      <td>0.106569</td>\n",
              "      <td>0.021223</td>\n",
              "      <td>0.004844</td>\n",
              "      <td>0.139500</td>\n",
              "      <td>0.181189</td>\n",
              "      <td>0.008716</td>\n",
              "      <td>0.015230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345464</th>\n",
              "      <td>many body</td>\n",
              "      <td>0.074389</td>\n",
              "      <td>0.008489</td>\n",
              "      <td>0.020476</td>\n",
              "      <td>0.001078</td>\n",
              "      <td>0.013613</td>\n",
              "      <td>0.060242</td>\n",
              "      <td>0.002538</td>\n",
              "      <td>0.015153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357537</th>\n",
              "      <td>method</td>\n",
              "      <td>0.201796</td>\n",
              "      <td>0.105360</td>\n",
              "      <td>0.023601</td>\n",
              "      <td>0.003810</td>\n",
              "      <td>0.124583</td>\n",
              "      <td>0.185068</td>\n",
              "      <td>0.009705</td>\n",
              "      <td>0.015063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>561036</th>\n",
              "      <td>stripe</td>\n",
              "      <td>0.035088</td>\n",
              "      <td>0.005690</td>\n",
              "      <td>0.022857</td>\n",
              "      <td>0.000583</td>\n",
              "      <td>0.009211</td>\n",
              "      <td>0.025994</td>\n",
              "      <td>0.001664</td>\n",
              "      <td>0.014985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>442211</th>\n",
              "      <td>pnictides</td>\n",
              "      <td>0.027563</td>\n",
              "      <td>0.000526</td>\n",
              "      <td>0.022051</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.019800</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.014896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310720</th>\n",
              "      <td>josephson</td>\n",
              "      <td>0.036364</td>\n",
              "      <td>0.008137</td>\n",
              "      <td>0.023715</td>\n",
              "      <td>0.003238</td>\n",
              "      <td>0.012597</td>\n",
              "      <td>0.024261</td>\n",
              "      <td>0.006183</td>\n",
              "      <td>0.014658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182129</th>\n",
              "      <td>emergence</td>\n",
              "      <td>0.139760</td>\n",
              "      <td>0.011189</td>\n",
              "      <td>0.019668</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.012252</td>\n",
              "      <td>0.127593</td>\n",
              "      <td>0.001089</td>\n",
              "      <td>0.014630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14955</th>\n",
              "      <td>alloys</td>\n",
              "      <td>0.043004</td>\n",
              "      <td>0.004458</td>\n",
              "      <td>0.019897</td>\n",
              "      <td>0.000928</td>\n",
              "      <td>0.008764</td>\n",
              "      <td>0.026733</td>\n",
              "      <td>0.003538</td>\n",
              "      <td>0.014622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247932</th>\n",
              "      <td>general</td>\n",
              "      <td>0.152850</td>\n",
              "      <td>0.094479</td>\n",
              "      <td>0.023774</td>\n",
              "      <td>0.005332</td>\n",
              "      <td>0.110082</td>\n",
              "      <td>0.143291</td>\n",
              "      <td>0.007544</td>\n",
              "      <td>0.014430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120524</th>\n",
              "      <td>cu</td>\n",
              "      <td>0.054545</td>\n",
              "      <td>0.009579</td>\n",
              "      <td>0.025898</td>\n",
              "      <td>0.002094</td>\n",
              "      <td>0.014029</td>\n",
              "      <td>0.031590</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.014405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397658</th>\n",
              "      <td>observed</td>\n",
              "      <td>0.259233</td>\n",
              "      <td>0.149648</td>\n",
              "      <td>0.020177</td>\n",
              "      <td>0.002660</td>\n",
              "      <td>0.175229</td>\n",
              "      <td>0.240444</td>\n",
              "      <td>0.005383</td>\n",
              "      <td>0.014311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317444</th>\n",
              "      <td>large</td>\n",
              "      <td>0.180777</td>\n",
              "      <td>0.081905</td>\n",
              "      <td>0.020290</td>\n",
              "      <td>0.001263</td>\n",
              "      <td>0.114733</td>\n",
              "      <td>0.169729</td>\n",
              "      <td>0.006555</td>\n",
              "      <td>0.014302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>657402</th>\n",
              "      <td>weyl</td>\n",
              "      <td>0.031700</td>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.024496</td>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.022274</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>0.014302</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad49b9de-5f52-4472-9766-6c6a705c2af8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ad49b9de-5f52-4472-9766-6c6a705c2af8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ad49b9de-5f52-4472-9766-6c6a705c2af8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a51ea56e-1074-46a9-97f0-aade95be5e51\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a51ea56e-1074-46a9-97f0-aade95be5e51')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a51ea56e-1074-46a9-97f0-aade95be5e51 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_data\",\n  \"rows\": 200,\n  \"fields\": [\n    {\n      \"column\": \"term\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"scattering\",\n          \"topological\",\n          \"single\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max_monthly_abstract_df\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09175155860365881,\n        \"min\": 0.02472952086553323,\n        \"max\": 0.4299489506522972,\n        \"num_unique_values\": 199,\n        \"samples\": [\n          0.09611829944547134,\n          0.16579973992197658,\n          0.1859452736318408\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min_monthly_abstract_df\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06407894650146174,\n        \"min\": 0.0005047955577990914,\n        \"max\": 0.2824561403508772,\n        \"num_unique_values\": 195,\n        \"samples\": [\n          0.0019083969465648854,\n          0.2694214876033058,\n          0.01540436456996149\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max_monthly_title_df\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.024369074112635492,\n        \"min\": 0.018878400888395337,\n        \"max\": 0.1642512077294686,\n        \"num_unique_values\": 191,\n        \"samples\": [\n          0.03130434782608696,\n          0.034482758620689655,\n          0.025157232704402517\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min_monthly_title_df\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01284202777137225,\n        \"min\": 0.0005047955577990914,\n        \"max\": 0.08609271523178808,\n        \"num_unique_values\": 184,\n        \"samples\": [\n          0.027096774193548386,\n          0.005272407732864675,\n          0.00336417157275021\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min_semesterly_abstract_df\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07163823878588842,\n        \"min\": 0.0001397233477714126,\n        \"max\": 0.31356877323420074,\n        \"num_unique_values\": 196,\n        \"samples\": [\n          0.00019051247856734617,\n          0.01152008991289688,\n          0.3037654788981552\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max_semesterly_abstract_df\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08835313635123436,\n        \"min\": 0.014558689717925387,\n        \"max\": 0.40597468354430377,\n        \"num_unique_values\": 199,\n        \"samples\": [\n          0.07247706422018349,\n          0.15150605423796656,\n          0.17434284430465063\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min_semesterly_title_df\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.016045423703380027,\n        \"min\": 9.365926758452749e-05,\n        \"max\": 0.10677466863033873,\n        \"num_unique_values\": 195,\n        \"samples\": [\n          0.0061162079510703364,\n          0.041574074074074076,\n          0.000696217219772569\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max_semesterly_title_df\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02096659285574463,\n        \"min\": 0.014302191464821222,\n        \"max\": 0.13943054357204487,\n        \"num_unique_values\": 191,\n        \"samples\": [\n          0.016879934658317452,\n          0.026692291266282298,\n          0.016455194290245837\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also look at the distribution of maximum semesterly document frequencies for the terms in the vocabulary:"
      ],
      "metadata": {
        "id": "0bNbcg0G9puB"
      },
      "id": "0bNbcg0G9puB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5466fd23-b77a-4987-b7d4-17b6376f5fc4",
      "metadata": {
        "id": "5466fd23-b77a-4987-b7d4-17b6376f5fc4",
        "outputId": "efa160db-cbb5-463d-d345-7c914141cd08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    669306.000000\n",
              "mean          0.000387\n",
              "std           0.000703\n",
              "min           0.000306\n",
              "25%           0.000306\n",
              "50%           0.000306\n",
              "75%           0.000372\n",
              "max           0.139431\n",
              "Name: max_semesterly_title_df, dtype: float64"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_data['max_semesterly_title_df'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now save some of our processed time-series data for machine learning in the next notebook. We will consider the top 500 terms by maximum semesterly document frequency in the title and abstract:"
      ],
      "metadata": {
        "id": "1fgZCViK-gwU"
      },
      "id": "1fgZCViK-gwU"
    },
    {
      "cell_type": "code",
      "source": [
        "title_keywords = df_data.sort_values('max_semesterly_title_df', ascending=False).head(500)\n",
        "abstract_keywords = df_data.sort_values('max_semesterly_abstract_df', ascending=False).head(500)"
      ],
      "metadata": {
        "id": "1U3M7GLn6_6L"
      },
      "id": "1U3M7GLn6_6L",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the title keywords, we will save the monthly values of the document frequencies in both the title and abstract:"
      ],
      "metadata": {
        "id": "89Uxdbmr-xWO"
      },
      "id": "89Uxdbmr-xWO"
    },
    {
      "cell_type": "code",
      "source": [
        "title_keywords_monthly_title_data = pd.DataFrame(monthly_titles_df[:, title_keywords.index], columns=title_keywords['term'])\n",
        "title_keywords_monthly_title_data['months_since_Jan_2000'] = np.arange(latest_month-earliest_month)\n",
        "title_keywords_monthly_title_data.set_index('months_since_Jan_2000', inplace=True)\n",
        "title_keywords_monthly_title_data.head(10)"
      ],
      "metadata": {
        "id": "nO4hfeDa6wpL",
        "outputId": "b68055e1-5141-4814-f3ff-5b63ffb0437a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "id": "nO4hfeDa6wpL",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "term                    quantum     model  superconducting      spin  \\\n",
              "months_since_Jan_2000                                                  \n",
              "0                      0.086957  0.112648         0.100791  0.088933   \n",
              "1                      0.118774  0.124521         0.084291  0.095785   \n",
              "2                      0.133568  0.117750         0.098418  0.086116   \n",
              "3                      0.127542  0.138632         0.085028  0.075786   \n",
              "4                      0.095076  0.147708         0.062818  0.093379   \n",
              "5                      0.108656  0.134438         0.101289  0.084715   \n",
              "6                      0.089789  0.133803         0.082746  0.089789   \n",
              "7                      0.106667  0.142857         0.085714  0.041905   \n",
              "8                      0.138318  0.123364         0.071028  0.078505   \n",
              "9                      0.107078  0.112523         0.079855  0.099819   \n",
              "\n",
              "term                   magnetic    effect  electron  dimensional     phase  \\\n",
              "months_since_Jan_2000                                                        \n",
              "0                      0.090909  0.081028  0.081028     0.084980  0.083004   \n",
              "1                      0.103448  0.084291  0.055556     0.067050  0.078544   \n",
              "2                      0.091388  0.075571  0.063269     0.086116  0.086116   \n",
              "3                      0.096118  0.073937  0.059150     0.077634  0.060998   \n",
              "4                      0.091681  0.057725  0.057725     0.086587  0.049236   \n",
              "5                      0.082873  0.073665  0.053407     0.066298  0.084715   \n",
              "6                      0.070423  0.066901  0.080986     0.065141  0.072183   \n",
              "7                      0.080000  0.078095  0.064762     0.064762  0.060952   \n",
              "8                      0.080374  0.089720  0.069159     0.100935  0.087850   \n",
              "9                      0.083485  0.072595  0.058076     0.088929  0.067151   \n",
              "\n",
              "term                     system  ...    torque      cold  compared  \\\n",
              "months_since_Jan_2000            ...                                 \n",
              "0                      0.077075  ...  0.003953  0.001976  0.005929   \n",
              "1                      0.072797  ...  0.001916  0.001916  0.005747   \n",
              "2                      0.084359  ...  0.001757  0.001757  0.010545   \n",
              "3                      0.060998  ...  0.001848  0.005545  0.001848   \n",
              "4                      0.064516  ...  0.003396  0.001698  0.008489   \n",
              "5                      0.079190  ...  0.001842  0.005525  0.003683   \n",
              "6                      0.084507  ...  0.001761  0.001761  0.003521   \n",
              "7                      0.085714  ...  0.001905  0.001905  0.009524   \n",
              "8                      0.065421  ...  0.001869  0.007477  0.003738   \n",
              "9                      0.065336  ...  0.001815  0.001815  0.007260   \n",
              "\n",
              "term                   hydrodynamic  momentum      pair    kitaev     gauge  \\\n",
              "months_since_Jan_2000                                                         \n",
              "0                          0.007905  0.009881  0.007905  0.001976  0.003953   \n",
              "1                          0.001916  0.003831  0.007663  0.001916  0.003831   \n",
              "2                          0.008787  0.010545  0.007030  0.001757  0.005272   \n",
              "3                          0.001848  0.003697  0.012939  0.001848  0.005545   \n",
              "4                          0.006791  0.003396  0.005093  0.001698  0.001698   \n",
              "5                          0.005525  0.001842  0.005525  0.001842  0.009208   \n",
              "6                          0.001761  0.012324  0.001761  0.001761  0.010563   \n",
              "7                          0.007619  0.001905  0.007619  0.001905  0.007619   \n",
              "8                          0.009346  0.007477  0.011215  0.001869  0.005607   \n",
              "9                          0.007260  0.007260  0.003630  0.001815  0.005445   \n",
              "\n",
              "term                      shape   vacancy  \n",
              "months_since_Jan_2000                      \n",
              "0                      0.001976  0.001976  \n",
              "1                      0.009579  0.001916  \n",
              "2                      0.007030  0.001757  \n",
              "3                      0.001848  0.001848  \n",
              "4                      0.005093  0.001698  \n",
              "5                      0.007366  0.003683  \n",
              "6                      0.005282  0.001761  \n",
              "7                      0.007619  0.001905  \n",
              "8                      0.009346  0.007477  \n",
              "9                      0.003630  0.001815  \n",
              "\n",
              "[10 rows x 500 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9e76e65-20de-4774-8f56-3caf4a7d58ab\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>term</th>\n",
              "      <th>quantum</th>\n",
              "      <th>model</th>\n",
              "      <th>superconducting</th>\n",
              "      <th>spin</th>\n",
              "      <th>magnetic</th>\n",
              "      <th>effect</th>\n",
              "      <th>electron</th>\n",
              "      <th>dimensional</th>\n",
              "      <th>phase</th>\n",
              "      <th>system</th>\n",
              "      <th>...</th>\n",
              "      <th>torque</th>\n",
              "      <th>cold</th>\n",
              "      <th>compared</th>\n",
              "      <th>hydrodynamic</th>\n",
              "      <th>momentum</th>\n",
              "      <th>pair</th>\n",
              "      <th>kitaev</th>\n",
              "      <th>gauge</th>\n",
              "      <th>shape</th>\n",
              "      <th>vacancy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>months_since_Jan_2000</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.112648</td>\n",
              "      <td>0.100791</td>\n",
              "      <td>0.088933</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.081028</td>\n",
              "      <td>0.081028</td>\n",
              "      <td>0.084980</td>\n",
              "      <td>0.083004</td>\n",
              "      <td>0.077075</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003953</td>\n",
              "      <td>0.001976</td>\n",
              "      <td>0.005929</td>\n",
              "      <td>0.007905</td>\n",
              "      <td>0.009881</td>\n",
              "      <td>0.007905</td>\n",
              "      <td>0.001976</td>\n",
              "      <td>0.003953</td>\n",
              "      <td>0.001976</td>\n",
              "      <td>0.001976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.118774</td>\n",
              "      <td>0.124521</td>\n",
              "      <td>0.084291</td>\n",
              "      <td>0.095785</td>\n",
              "      <td>0.103448</td>\n",
              "      <td>0.084291</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.067050</td>\n",
              "      <td>0.078544</td>\n",
              "      <td>0.072797</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.005747</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.003831</td>\n",
              "      <td>0.007663</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.003831</td>\n",
              "      <td>0.009579</td>\n",
              "      <td>0.001916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.133568</td>\n",
              "      <td>0.117750</td>\n",
              "      <td>0.098418</td>\n",
              "      <td>0.086116</td>\n",
              "      <td>0.091388</td>\n",
              "      <td>0.075571</td>\n",
              "      <td>0.063269</td>\n",
              "      <td>0.086116</td>\n",
              "      <td>0.086116</td>\n",
              "      <td>0.084359</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.010545</td>\n",
              "      <td>0.008787</td>\n",
              "      <td>0.010545</td>\n",
              "      <td>0.007030</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.005272</td>\n",
              "      <td>0.007030</td>\n",
              "      <td>0.001757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.127542</td>\n",
              "      <td>0.138632</td>\n",
              "      <td>0.085028</td>\n",
              "      <td>0.075786</td>\n",
              "      <td>0.096118</td>\n",
              "      <td>0.073937</td>\n",
              "      <td>0.059150</td>\n",
              "      <td>0.077634</td>\n",
              "      <td>0.060998</td>\n",
              "      <td>0.060998</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.005545</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.003697</td>\n",
              "      <td>0.012939</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.005545</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.001848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.095076</td>\n",
              "      <td>0.147708</td>\n",
              "      <td>0.062818</td>\n",
              "      <td>0.093379</td>\n",
              "      <td>0.091681</td>\n",
              "      <td>0.057725</td>\n",
              "      <td>0.057725</td>\n",
              "      <td>0.086587</td>\n",
              "      <td>0.049236</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003396</td>\n",
              "      <td>0.001698</td>\n",
              "      <td>0.008489</td>\n",
              "      <td>0.006791</td>\n",
              "      <td>0.003396</td>\n",
              "      <td>0.005093</td>\n",
              "      <td>0.001698</td>\n",
              "      <td>0.001698</td>\n",
              "      <td>0.005093</td>\n",
              "      <td>0.001698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.108656</td>\n",
              "      <td>0.134438</td>\n",
              "      <td>0.101289</td>\n",
              "      <td>0.084715</td>\n",
              "      <td>0.082873</td>\n",
              "      <td>0.073665</td>\n",
              "      <td>0.053407</td>\n",
              "      <td>0.066298</td>\n",
              "      <td>0.084715</td>\n",
              "      <td>0.079190</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.005525</td>\n",
              "      <td>0.003683</td>\n",
              "      <td>0.005525</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.005525</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>0.007366</td>\n",
              "      <td>0.003683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.089789</td>\n",
              "      <td>0.133803</td>\n",
              "      <td>0.082746</td>\n",
              "      <td>0.089789</td>\n",
              "      <td>0.070423</td>\n",
              "      <td>0.066901</td>\n",
              "      <td>0.080986</td>\n",
              "      <td>0.065141</td>\n",
              "      <td>0.072183</td>\n",
              "      <td>0.084507</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.003521</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.012324</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.010563</td>\n",
              "      <td>0.005282</td>\n",
              "      <td>0.001761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.106667</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>0.041905</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.078095</td>\n",
              "      <td>0.064762</td>\n",
              "      <td>0.064762</td>\n",
              "      <td>0.060952</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.009524</td>\n",
              "      <td>0.007619</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.007619</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.007619</td>\n",
              "      <td>0.007619</td>\n",
              "      <td>0.001905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.138318</td>\n",
              "      <td>0.123364</td>\n",
              "      <td>0.071028</td>\n",
              "      <td>0.078505</td>\n",
              "      <td>0.080374</td>\n",
              "      <td>0.089720</td>\n",
              "      <td>0.069159</td>\n",
              "      <td>0.100935</td>\n",
              "      <td>0.087850</td>\n",
              "      <td>0.065421</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.007477</td>\n",
              "      <td>0.003738</td>\n",
              "      <td>0.009346</td>\n",
              "      <td>0.007477</td>\n",
              "      <td>0.011215</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.005607</td>\n",
              "      <td>0.009346</td>\n",
              "      <td>0.007477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.107078</td>\n",
              "      <td>0.112523</td>\n",
              "      <td>0.079855</td>\n",
              "      <td>0.099819</td>\n",
              "      <td>0.083485</td>\n",
              "      <td>0.072595</td>\n",
              "      <td>0.058076</td>\n",
              "      <td>0.088929</td>\n",
              "      <td>0.067151</td>\n",
              "      <td>0.065336</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.007260</td>\n",
              "      <td>0.007260</td>\n",
              "      <td>0.007260</td>\n",
              "      <td>0.003630</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.005445</td>\n",
              "      <td>0.003630</td>\n",
              "      <td>0.001815</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 500 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9e76e65-20de-4774-8f56-3caf4a7d58ab')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e9e76e65-20de-4774-8f56-3caf4a7d58ab button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e9e76e65-20de-4774-8f56-3caf4a7d58ab');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-86c1bdff-2059-40a7-88c6-97c32e8bdd7f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-86c1bdff-2059-40a7-88c6-97c32e8bdd7f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-86c1bdff-2059-40a7-88c6-97c32e8bdd7f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "title_keywords_monthly_title_data"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title_keywords_monthly_title_data.to_csv('title_keywords_monthly_title_data.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "zgg8EkLA8jVQ"
      },
      "id": "zgg8EkLA8jVQ",
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_keywords_monthly_abstract_data = pd.DataFrame(monthly_abstracts_df[:, title_keywords.index], columns=title_keywords['term'])\n",
        "title_keywords_monthly_abstract_data['months_since_Jan_2000'] = np.arange(latest_month-earliest_month)\n",
        "title_keywords_monthly_abstract_data.set_index('months_since_Jan_2000', inplace=True)\n",
        "title_keywords_monthly_abstract_data.to_csv('title_keywords_monthly_abstract_data.csv', sep=',', encoding='utf-8')\n",
        "title_keywords_monthly_abstract_data.head(10)"
      ],
      "metadata": {
        "id": "xak4yzdY8ygs",
        "outputId": "bd7adedc-9d77-488e-dd19-2b33abd07d8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "id": "xak4yzdY8ygs",
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "term                    quantum     model  superconducting      spin  \\\n",
              "months_since_Jan_2000                                                  \n",
              "0                      0.181818  0.377470         0.162055  0.189723   \n",
              "1                      0.208812  0.373563         0.143678  0.185824   \n",
              "2                      0.179262  0.393673         0.147627  0.198594   \n",
              "3                      0.186691  0.375231         0.131238  0.205176   \n",
              "4                      0.183362  0.348048         0.113752  0.191851   \n",
              "5                      0.204420  0.397790         0.128913  0.197053   \n",
              "6                      0.170775  0.390845         0.121479  0.204225   \n",
              "7                      0.190476  0.392381         0.127619  0.144762   \n",
              "8                      0.207477  0.370093         0.121495  0.164486   \n",
              "9                      0.190563  0.373866         0.128857  0.214156   \n",
              "\n",
              "term                   magnetic    effect  electron  dimensional     phase  \\\n",
              "months_since_Jan_2000                                                        \n",
              "0                      0.229249  0.290514  0.241107     0.239130  0.243083   \n",
              "1                      0.210728  0.247126  0.180077     0.226054  0.218391   \n",
              "2                      0.221441  0.254833  0.209139     0.265378  0.212654   \n",
              "3                      0.221811  0.210721  0.203327     0.242144  0.232902   \n",
              "4                      0.239389  0.247878  0.176570     0.252971  0.217317   \n",
              "5                      0.233886  0.239411  0.191529     0.224678  0.246777   \n",
              "6                      0.218310  0.265845  0.218310     0.227113  0.250000   \n",
              "7                      0.188571  0.251429  0.180952     0.245714  0.219048   \n",
              "8                      0.203738  0.257944  0.192523     0.222430  0.229907   \n",
              "9                      0.226860  0.254083  0.221416     0.254083  0.239564   \n",
              "\n",
              "term                     system  ...    torque      cold  compared  \\\n",
              "months_since_Jan_2000            ...                                 \n",
              "0                      0.288538  ...  0.003953  0.007905  0.126482   \n",
              "1                      0.285441  ...  0.001916  0.005747  0.109195   \n",
              "2                      0.311072  ...  0.007030  0.005272  0.166960   \n",
              "3                      0.290203  ...  0.003697  0.005545  0.123845   \n",
              "4                      0.297114  ...  0.003396  0.001698  0.125637   \n",
              "5                      0.292818  ...  0.001842  0.005525  0.117864   \n",
              "6                      0.341549  ...  0.001761  0.007042  0.107394   \n",
              "7                      0.356190  ...  0.003810  0.003810  0.120000   \n",
              "8                      0.340187  ...  0.003738  0.011215  0.112150   \n",
              "9                      0.308530  ...  0.003630  0.003630  0.103448   \n",
              "\n",
              "term                   hydrodynamic  momentum      pair    kitaev     gauge  \\\n",
              "months_since_Jan_2000                                                         \n",
              "0                          0.011858  0.053360  0.045455  0.001976  0.007905   \n",
              "1                          0.011494  0.065134  0.034483  0.001916  0.017241   \n",
              "2                          0.010545  0.063269  0.029877  0.001757  0.015817   \n",
              "3                          0.009242  0.057301  0.042514  0.001848  0.016636   \n",
              "4                          0.015280  0.056027  0.032258  0.001698  0.020374   \n",
              "5                          0.016575  0.038674  0.031308  0.001842  0.025783   \n",
              "6                          0.014085  0.066901  0.024648  0.001761  0.026408   \n",
              "7                          0.017143  0.047619  0.034286  0.001905  0.015238   \n",
              "8                          0.014953  0.035514  0.031776  0.001869  0.007477   \n",
              "9                          0.016334  0.052632  0.047187  0.001815  0.021779   \n",
              "\n",
              "term                      shape   vacancy  \n",
              "months_since_Jan_2000                      \n",
              "0                      0.031621  0.005929  \n",
              "1                      0.040230  0.003831  \n",
              "2                      0.038664  0.005272  \n",
              "3                      0.036969  0.001848  \n",
              "4                      0.028862  0.003396  \n",
              "5                      0.031308  0.009208  \n",
              "6                      0.028169  0.005282  \n",
              "7                      0.045714  0.005714  \n",
              "8                      0.033645  0.009346  \n",
              "9                      0.032668  0.005445  \n",
              "\n",
              "[10 rows x 500 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13dec1ce-327f-476b-accf-1a67a2c7cabe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>term</th>\n",
              "      <th>quantum</th>\n",
              "      <th>model</th>\n",
              "      <th>superconducting</th>\n",
              "      <th>spin</th>\n",
              "      <th>magnetic</th>\n",
              "      <th>effect</th>\n",
              "      <th>electron</th>\n",
              "      <th>dimensional</th>\n",
              "      <th>phase</th>\n",
              "      <th>system</th>\n",
              "      <th>...</th>\n",
              "      <th>torque</th>\n",
              "      <th>cold</th>\n",
              "      <th>compared</th>\n",
              "      <th>hydrodynamic</th>\n",
              "      <th>momentum</th>\n",
              "      <th>pair</th>\n",
              "      <th>kitaev</th>\n",
              "      <th>gauge</th>\n",
              "      <th>shape</th>\n",
              "      <th>vacancy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>months_since_Jan_2000</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.377470</td>\n",
              "      <td>0.162055</td>\n",
              "      <td>0.189723</td>\n",
              "      <td>0.229249</td>\n",
              "      <td>0.290514</td>\n",
              "      <td>0.241107</td>\n",
              "      <td>0.239130</td>\n",
              "      <td>0.243083</td>\n",
              "      <td>0.288538</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003953</td>\n",
              "      <td>0.007905</td>\n",
              "      <td>0.126482</td>\n",
              "      <td>0.011858</td>\n",
              "      <td>0.053360</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>0.001976</td>\n",
              "      <td>0.007905</td>\n",
              "      <td>0.031621</td>\n",
              "      <td>0.005929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.208812</td>\n",
              "      <td>0.373563</td>\n",
              "      <td>0.143678</td>\n",
              "      <td>0.185824</td>\n",
              "      <td>0.210728</td>\n",
              "      <td>0.247126</td>\n",
              "      <td>0.180077</td>\n",
              "      <td>0.226054</td>\n",
              "      <td>0.218391</td>\n",
              "      <td>0.285441</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.005747</td>\n",
              "      <td>0.109195</td>\n",
              "      <td>0.011494</td>\n",
              "      <td>0.065134</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.017241</td>\n",
              "      <td>0.040230</td>\n",
              "      <td>0.003831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.179262</td>\n",
              "      <td>0.393673</td>\n",
              "      <td>0.147627</td>\n",
              "      <td>0.198594</td>\n",
              "      <td>0.221441</td>\n",
              "      <td>0.254833</td>\n",
              "      <td>0.209139</td>\n",
              "      <td>0.265378</td>\n",
              "      <td>0.212654</td>\n",
              "      <td>0.311072</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007030</td>\n",
              "      <td>0.005272</td>\n",
              "      <td>0.166960</td>\n",
              "      <td>0.010545</td>\n",
              "      <td>0.063269</td>\n",
              "      <td>0.029877</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.015817</td>\n",
              "      <td>0.038664</td>\n",
              "      <td>0.005272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.186691</td>\n",
              "      <td>0.375231</td>\n",
              "      <td>0.131238</td>\n",
              "      <td>0.205176</td>\n",
              "      <td>0.221811</td>\n",
              "      <td>0.210721</td>\n",
              "      <td>0.203327</td>\n",
              "      <td>0.242144</td>\n",
              "      <td>0.232902</td>\n",
              "      <td>0.290203</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003697</td>\n",
              "      <td>0.005545</td>\n",
              "      <td>0.123845</td>\n",
              "      <td>0.009242</td>\n",
              "      <td>0.057301</td>\n",
              "      <td>0.042514</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.016636</td>\n",
              "      <td>0.036969</td>\n",
              "      <td>0.001848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.183362</td>\n",
              "      <td>0.348048</td>\n",
              "      <td>0.113752</td>\n",
              "      <td>0.191851</td>\n",
              "      <td>0.239389</td>\n",
              "      <td>0.247878</td>\n",
              "      <td>0.176570</td>\n",
              "      <td>0.252971</td>\n",
              "      <td>0.217317</td>\n",
              "      <td>0.297114</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003396</td>\n",
              "      <td>0.001698</td>\n",
              "      <td>0.125637</td>\n",
              "      <td>0.015280</td>\n",
              "      <td>0.056027</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.001698</td>\n",
              "      <td>0.020374</td>\n",
              "      <td>0.028862</td>\n",
              "      <td>0.003396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.204420</td>\n",
              "      <td>0.397790</td>\n",
              "      <td>0.128913</td>\n",
              "      <td>0.197053</td>\n",
              "      <td>0.233886</td>\n",
              "      <td>0.239411</td>\n",
              "      <td>0.191529</td>\n",
              "      <td>0.224678</td>\n",
              "      <td>0.246777</td>\n",
              "      <td>0.292818</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.005525</td>\n",
              "      <td>0.117864</td>\n",
              "      <td>0.016575</td>\n",
              "      <td>0.038674</td>\n",
              "      <td>0.031308</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.025783</td>\n",
              "      <td>0.031308</td>\n",
              "      <td>0.009208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.170775</td>\n",
              "      <td>0.390845</td>\n",
              "      <td>0.121479</td>\n",
              "      <td>0.204225</td>\n",
              "      <td>0.218310</td>\n",
              "      <td>0.265845</td>\n",
              "      <td>0.218310</td>\n",
              "      <td>0.227113</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.341549</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.007042</td>\n",
              "      <td>0.107394</td>\n",
              "      <td>0.014085</td>\n",
              "      <td>0.066901</td>\n",
              "      <td>0.024648</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.026408</td>\n",
              "      <td>0.028169</td>\n",
              "      <td>0.005282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.190476</td>\n",
              "      <td>0.392381</td>\n",
              "      <td>0.127619</td>\n",
              "      <td>0.144762</td>\n",
              "      <td>0.188571</td>\n",
              "      <td>0.251429</td>\n",
              "      <td>0.180952</td>\n",
              "      <td>0.245714</td>\n",
              "      <td>0.219048</td>\n",
              "      <td>0.356190</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003810</td>\n",
              "      <td>0.003810</td>\n",
              "      <td>0.120000</td>\n",
              "      <td>0.017143</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.034286</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.015238</td>\n",
              "      <td>0.045714</td>\n",
              "      <td>0.005714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.207477</td>\n",
              "      <td>0.370093</td>\n",
              "      <td>0.121495</td>\n",
              "      <td>0.164486</td>\n",
              "      <td>0.203738</td>\n",
              "      <td>0.257944</td>\n",
              "      <td>0.192523</td>\n",
              "      <td>0.222430</td>\n",
              "      <td>0.229907</td>\n",
              "      <td>0.340187</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003738</td>\n",
              "      <td>0.011215</td>\n",
              "      <td>0.112150</td>\n",
              "      <td>0.014953</td>\n",
              "      <td>0.035514</td>\n",
              "      <td>0.031776</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.007477</td>\n",
              "      <td>0.033645</td>\n",
              "      <td>0.009346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.190563</td>\n",
              "      <td>0.373866</td>\n",
              "      <td>0.128857</td>\n",
              "      <td>0.214156</td>\n",
              "      <td>0.226860</td>\n",
              "      <td>0.254083</td>\n",
              "      <td>0.221416</td>\n",
              "      <td>0.254083</td>\n",
              "      <td>0.239564</td>\n",
              "      <td>0.308530</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003630</td>\n",
              "      <td>0.003630</td>\n",
              "      <td>0.103448</td>\n",
              "      <td>0.016334</td>\n",
              "      <td>0.052632</td>\n",
              "      <td>0.047187</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.021779</td>\n",
              "      <td>0.032668</td>\n",
              "      <td>0.005445</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 500 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13dec1ce-327f-476b-accf-1a67a2c7cabe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-13dec1ce-327f-476b-accf-1a67a2c7cabe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-13dec1ce-327f-476b-accf-1a67a2c7cabe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-467d5485-438c-4fd5-8f5e-5f2d948a5806\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-467d5485-438c-4fd5-8f5e-5f2d948a5806')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-467d5485-438c-4fd5-8f5e-5f2d948a5806 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "title_keywords_monthly_abstract_data"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also save the document frequencies for the abstract keywords, but we will only consider the frequencies of these keywords in the abstracts, since their appearance in the titles is less meaningful:"
      ],
      "metadata": {
        "id": "pLx5uyFK-8eF"
      },
      "id": "pLx5uyFK-8eF"
    },
    {
      "cell_type": "code",
      "source": [
        "abstract_keywords_monthly_abstract_data = pd.DataFrame(monthly_abstracts_df[:, abstract_keywords.index], columns=abstract_keywords['term'])\n",
        "abstract_keywords_monthly_abstract_data['months_since_Jan_2000'] = np.arange(latest_month-earliest_month)\n",
        "abstract_keywords_monthly_abstract_data.set_index('months_since_Jan_2000', inplace=True)\n",
        "abstract_keywords_monthly_abstract_data.to_csv('abstract_keywords_monthly_abstract_data.csv', sep=',', encoding='utf-8')\n",
        "abstract_keywords_monthly_abstract_data.head(10)"
      ],
      "metadata": {
        "id": "RyyZq1pe9MxL",
        "outputId": "4c09fdbf-687b-4caf-82f5-455e09fd2969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "id": "RyyZq1pe9MxL",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "term                     system     state     study   results     model  \\\n",
              "months_since_Jan_2000                                                     \n",
              "0                      0.288538  0.294466  0.326087  0.343874  0.377470   \n",
              "1                      0.285441  0.321839  0.354406  0.350575  0.373563   \n",
              "2                      0.311072  0.342707  0.312830  0.326889  0.393673   \n",
              "3                      0.290203  0.312384  0.323475  0.297597  0.375231   \n",
              "4                      0.297114  0.288625  0.319185  0.293718  0.348048   \n",
              "5                      0.292818  0.333333  0.322284  0.302026  0.397790   \n",
              "6                      0.341549  0.309859  0.341549  0.334507  0.390845   \n",
              "7                      0.356190  0.314286  0.329524  0.339048  0.392381   \n",
              "8                      0.340187  0.302804  0.284112  0.310280  0.370093   \n",
              "9                      0.308530  0.324864  0.313975  0.326679  0.373866   \n",
              "\n",
              "term                        two    effect     these      show   between  ...  \\\n",
              "months_since_Jan_2000                                                    ...   \n",
              "0                      0.308300  0.290514  0.185771  0.191700  0.199605  ...   \n",
              "1                      0.291188  0.247126  0.191571  0.197318  0.262452  ...   \n",
              "2                      0.333919  0.254833  0.233743  0.170475  0.246046  ...   \n",
              "3                      0.308688  0.210721  0.199630  0.199630  0.243993  ...   \n",
              "4                      0.317487  0.247878  0.161290  0.222411  0.222411  ...   \n",
              "5                      0.307551  0.239411  0.211786  0.232044  0.263352  ...   \n",
              "6                      0.311620  0.265845  0.216549  0.237676  0.242958  ...   \n",
              "7                      0.337143  0.251429  0.194286  0.228571  0.240000  ...   \n",
              "8                      0.314019  0.257944  0.196262  0.207477  0.239252  ...   \n",
              "9                      0.323049  0.254083  0.177858  0.201452  0.239564  ...   \n",
              "\n",
              "term                   resolved         f  suppressed  patterns     could  \\\n",
              "months_since_Jan_2000                                                       \n",
              "0                      0.007905  0.027668    0.029644  0.011858  0.023715   \n",
              "1                      0.013410  0.030651    0.030651  0.034483  0.022989   \n",
              "2                      0.019332  0.036907    0.029877  0.026362  0.015817   \n",
              "3                      0.014787  0.029575    0.022181  0.022181  0.025878   \n",
              "4                      0.025467  0.039049    0.035654  0.020374  0.020374   \n",
              "5                      0.014733  0.046041    0.036832  0.022099  0.018416   \n",
              "6                      0.014085  0.033451    0.021127  0.031690  0.019366   \n",
              "7                      0.022857  0.026667    0.038095  0.034286  0.024762   \n",
              "8                      0.016822  0.037383    0.022430  0.024299  0.016822   \n",
              "9                      0.010889  0.045372    0.034483  0.016334  0.018149   \n",
              "\n",
              "term                   following  standard  electrolyte   typical      mode  \n",
              "months_since_Jan_2000                                                        \n",
              "0                       0.029644  0.035573     0.017787  0.019763  0.039526  \n",
              "1                       0.026820  0.032567     0.007663  0.030651  0.059387  \n",
              "2                       0.035149  0.029877     0.017575  0.026362  0.028120  \n",
              "3                       0.029575  0.055453     0.009242  0.018484  0.029575  \n",
              "4                       0.054329  0.035654     0.010187  0.027165  0.042445  \n",
              "5                       0.040516  0.036832     0.022099  0.016575  0.031308  \n",
              "6                       0.038732  0.031690     0.017606  0.021127  0.036972  \n",
              "7                       0.045714  0.034286     0.017143  0.026667  0.032381  \n",
              "8                       0.039252  0.026168     0.014953  0.028037  0.033645  \n",
              "9                       0.034483  0.036298     0.019964  0.021779  0.032668  \n",
              "\n",
              "[10 rows x 500 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1217faa7-e6de-4214-a4d6-01671dcc1d9e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>term</th>\n",
              "      <th>system</th>\n",
              "      <th>state</th>\n",
              "      <th>study</th>\n",
              "      <th>results</th>\n",
              "      <th>model</th>\n",
              "      <th>two</th>\n",
              "      <th>effect</th>\n",
              "      <th>these</th>\n",
              "      <th>show</th>\n",
              "      <th>between</th>\n",
              "      <th>...</th>\n",
              "      <th>resolved</th>\n",
              "      <th>f</th>\n",
              "      <th>suppressed</th>\n",
              "      <th>patterns</th>\n",
              "      <th>could</th>\n",
              "      <th>following</th>\n",
              "      <th>standard</th>\n",
              "      <th>electrolyte</th>\n",
              "      <th>typical</th>\n",
              "      <th>mode</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>months_since_Jan_2000</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.288538</td>\n",
              "      <td>0.294466</td>\n",
              "      <td>0.326087</td>\n",
              "      <td>0.343874</td>\n",
              "      <td>0.377470</td>\n",
              "      <td>0.308300</td>\n",
              "      <td>0.290514</td>\n",
              "      <td>0.185771</td>\n",
              "      <td>0.191700</td>\n",
              "      <td>0.199605</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007905</td>\n",
              "      <td>0.027668</td>\n",
              "      <td>0.029644</td>\n",
              "      <td>0.011858</td>\n",
              "      <td>0.023715</td>\n",
              "      <td>0.029644</td>\n",
              "      <td>0.035573</td>\n",
              "      <td>0.017787</td>\n",
              "      <td>0.019763</td>\n",
              "      <td>0.039526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.285441</td>\n",
              "      <td>0.321839</td>\n",
              "      <td>0.354406</td>\n",
              "      <td>0.350575</td>\n",
              "      <td>0.373563</td>\n",
              "      <td>0.291188</td>\n",
              "      <td>0.247126</td>\n",
              "      <td>0.191571</td>\n",
              "      <td>0.197318</td>\n",
              "      <td>0.262452</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013410</td>\n",
              "      <td>0.030651</td>\n",
              "      <td>0.030651</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.022989</td>\n",
              "      <td>0.026820</td>\n",
              "      <td>0.032567</td>\n",
              "      <td>0.007663</td>\n",
              "      <td>0.030651</td>\n",
              "      <td>0.059387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.311072</td>\n",
              "      <td>0.342707</td>\n",
              "      <td>0.312830</td>\n",
              "      <td>0.326889</td>\n",
              "      <td>0.393673</td>\n",
              "      <td>0.333919</td>\n",
              "      <td>0.254833</td>\n",
              "      <td>0.233743</td>\n",
              "      <td>0.170475</td>\n",
              "      <td>0.246046</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019332</td>\n",
              "      <td>0.036907</td>\n",
              "      <td>0.029877</td>\n",
              "      <td>0.026362</td>\n",
              "      <td>0.015817</td>\n",
              "      <td>0.035149</td>\n",
              "      <td>0.029877</td>\n",
              "      <td>0.017575</td>\n",
              "      <td>0.026362</td>\n",
              "      <td>0.028120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.290203</td>\n",
              "      <td>0.312384</td>\n",
              "      <td>0.323475</td>\n",
              "      <td>0.297597</td>\n",
              "      <td>0.375231</td>\n",
              "      <td>0.308688</td>\n",
              "      <td>0.210721</td>\n",
              "      <td>0.199630</td>\n",
              "      <td>0.199630</td>\n",
              "      <td>0.243993</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014787</td>\n",
              "      <td>0.029575</td>\n",
              "      <td>0.022181</td>\n",
              "      <td>0.022181</td>\n",
              "      <td>0.025878</td>\n",
              "      <td>0.029575</td>\n",
              "      <td>0.055453</td>\n",
              "      <td>0.009242</td>\n",
              "      <td>0.018484</td>\n",
              "      <td>0.029575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.297114</td>\n",
              "      <td>0.288625</td>\n",
              "      <td>0.319185</td>\n",
              "      <td>0.293718</td>\n",
              "      <td>0.348048</td>\n",
              "      <td>0.317487</td>\n",
              "      <td>0.247878</td>\n",
              "      <td>0.161290</td>\n",
              "      <td>0.222411</td>\n",
              "      <td>0.222411</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025467</td>\n",
              "      <td>0.039049</td>\n",
              "      <td>0.035654</td>\n",
              "      <td>0.020374</td>\n",
              "      <td>0.020374</td>\n",
              "      <td>0.054329</td>\n",
              "      <td>0.035654</td>\n",
              "      <td>0.010187</td>\n",
              "      <td>0.027165</td>\n",
              "      <td>0.042445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.292818</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.322284</td>\n",
              "      <td>0.302026</td>\n",
              "      <td>0.397790</td>\n",
              "      <td>0.307551</td>\n",
              "      <td>0.239411</td>\n",
              "      <td>0.211786</td>\n",
              "      <td>0.232044</td>\n",
              "      <td>0.263352</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014733</td>\n",
              "      <td>0.046041</td>\n",
              "      <td>0.036832</td>\n",
              "      <td>0.022099</td>\n",
              "      <td>0.018416</td>\n",
              "      <td>0.040516</td>\n",
              "      <td>0.036832</td>\n",
              "      <td>0.022099</td>\n",
              "      <td>0.016575</td>\n",
              "      <td>0.031308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.341549</td>\n",
              "      <td>0.309859</td>\n",
              "      <td>0.341549</td>\n",
              "      <td>0.334507</td>\n",
              "      <td>0.390845</td>\n",
              "      <td>0.311620</td>\n",
              "      <td>0.265845</td>\n",
              "      <td>0.216549</td>\n",
              "      <td>0.237676</td>\n",
              "      <td>0.242958</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014085</td>\n",
              "      <td>0.033451</td>\n",
              "      <td>0.021127</td>\n",
              "      <td>0.031690</td>\n",
              "      <td>0.019366</td>\n",
              "      <td>0.038732</td>\n",
              "      <td>0.031690</td>\n",
              "      <td>0.017606</td>\n",
              "      <td>0.021127</td>\n",
              "      <td>0.036972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.356190</td>\n",
              "      <td>0.314286</td>\n",
              "      <td>0.329524</td>\n",
              "      <td>0.339048</td>\n",
              "      <td>0.392381</td>\n",
              "      <td>0.337143</td>\n",
              "      <td>0.251429</td>\n",
              "      <td>0.194286</td>\n",
              "      <td>0.228571</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022857</td>\n",
              "      <td>0.026667</td>\n",
              "      <td>0.038095</td>\n",
              "      <td>0.034286</td>\n",
              "      <td>0.024762</td>\n",
              "      <td>0.045714</td>\n",
              "      <td>0.034286</td>\n",
              "      <td>0.017143</td>\n",
              "      <td>0.026667</td>\n",
              "      <td>0.032381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.340187</td>\n",
              "      <td>0.302804</td>\n",
              "      <td>0.284112</td>\n",
              "      <td>0.310280</td>\n",
              "      <td>0.370093</td>\n",
              "      <td>0.314019</td>\n",
              "      <td>0.257944</td>\n",
              "      <td>0.196262</td>\n",
              "      <td>0.207477</td>\n",
              "      <td>0.239252</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016822</td>\n",
              "      <td>0.037383</td>\n",
              "      <td>0.022430</td>\n",
              "      <td>0.024299</td>\n",
              "      <td>0.016822</td>\n",
              "      <td>0.039252</td>\n",
              "      <td>0.026168</td>\n",
              "      <td>0.014953</td>\n",
              "      <td>0.028037</td>\n",
              "      <td>0.033645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.308530</td>\n",
              "      <td>0.324864</td>\n",
              "      <td>0.313975</td>\n",
              "      <td>0.326679</td>\n",
              "      <td>0.373866</td>\n",
              "      <td>0.323049</td>\n",
              "      <td>0.254083</td>\n",
              "      <td>0.177858</td>\n",
              "      <td>0.201452</td>\n",
              "      <td>0.239564</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010889</td>\n",
              "      <td>0.045372</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.016334</td>\n",
              "      <td>0.018149</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.036298</td>\n",
              "      <td>0.019964</td>\n",
              "      <td>0.021779</td>\n",
              "      <td>0.032668</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 500 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1217faa7-e6de-4214-a4d6-01671dcc1d9e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1217faa7-e6de-4214-a4d6-01671dcc1d9e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1217faa7-e6de-4214-a4d6-01671dcc1d9e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-35843cb8-3496-46bc-9da9-af8e475c2fa9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-35843cb8-3496-46bc-9da9-af8e475c2fa9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-35843cb8-3496-46bc-9da9-af8e475c2fa9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "abstract_keywords_monthly_abstract_data"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, terms like 'study', 'results', etc. commonly appear in the abstracts, but their document frequency in the title is not as valuable."
      ],
      "metadata": {
        "id": "qFGe145L_PX_"
      },
      "id": "qFGe145L_PX_"
    },
    {
      "cell_type": "markdown",
      "id": "847e3eef-52eb-4653-8e6c-4edd817a7635",
      "metadata": {
        "id": "847e3eef-52eb-4653-8e6c-4edd817a7635"
      },
      "source": [
        "There are a small number of words that, for some semester or semester, appear in nearly half of all abstracts. We're looking for keywords, whose frequency changes significantly over time. I'll find the words that have seen the greatest percentage change in document frequency:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd45e94e-5dd5-4919-8431-a5345d2c6ca0",
      "metadata": {
        "scrolled": true,
        "id": "cd45e94e-5dd5-4919-8431-a5345d2c6ca0",
        "outputId": "0b9ac0e6-7c58-4a67-c657-e2b450428e73"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>term</th>\n",
              "      <th>max_monthly_abstract_df</th>\n",
              "      <th>min_monthly_abstract_df</th>\n",
              "      <th>max_monthly_title_df</th>\n",
              "      <th>min_monthly_title_df</th>\n",
              "      <th>min_semesterly_abstract_df</th>\n",
              "      <th>max_semesterly_abstract_df</th>\n",
              "      <th>min_semesterly_title_df</th>\n",
              "      <th>max_semesterly_title_df</th>\n",
              "      <th>max/min_semesterly_abstract_df</th>\n",
              "      <th>max/min_semesterly_title_df</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>339710</th>\n",
              "      <td>machine learning</td>\n",
              "      <td>0.041916</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.018878</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.036852</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.015741</td>\n",
              "      <td>263.748704</td>\n",
              "      <td>112.656481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>615098</th>\n",
              "      <td>topological insulator</td>\n",
              "      <td>0.040268</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.026116</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.034364</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.017812</td>\n",
              "      <td>180.378007</td>\n",
              "      <td>93.497163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256046</th>\n",
              "      <td>graphene</td>\n",
              "      <td>0.096935</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.085336</td>\n",
              "      <td>0.001078</td>\n",
              "      <td>0.000696</td>\n",
              "      <td>0.082962</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.073019</td>\n",
              "      <td>119.161522</td>\n",
              "      <td>317.996438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345696</th>\n",
              "      <td>majorana</td>\n",
              "      <td>0.030201</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.014824</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.023190</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.009861</td>\n",
              "      <td>109.666883</td>\n",
              "      <td>49.769939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>615605</th>\n",
              "      <td>topological phase</td>\n",
              "      <td>0.037675</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.013383</td>\n",
              "      <td>0.000842</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>0.026414</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.008955</td>\n",
              "      <td>97.018449</td>\n",
              "      <td>44.925006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>658894</th>\n",
              "      <td>weyl semimetal</td>\n",
              "      <td>0.024496</td>\n",
              "      <td>0.000773</td>\n",
              "      <td>0.016571</td>\n",
              "      <td>0.000773</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.013867</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.009541</td>\n",
              "      <td>94.103617</td>\n",
              "      <td>64.743288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371092</th>\n",
              "      <td>moir emission</td>\n",
              "      <td>0.021778</td>\n",
              "      <td>0.000674</td>\n",
              "      <td>0.010821</td>\n",
              "      <td>0.000637</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>0.015111</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.007605</td>\n",
              "      <td>90.240368</td>\n",
              "      <td>54.454118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371090</th>\n",
              "      <td>moir</td>\n",
              "      <td>0.024600</td>\n",
              "      <td>0.000674</td>\n",
              "      <td>0.012731</td>\n",
              "      <td>0.000637</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.017130</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.008889</td>\n",
              "      <td>86.453241</td>\n",
              "      <td>63.644444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>626853</th>\n",
              "      <td>twisted bilayer</td>\n",
              "      <td>0.019446</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.010863</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.012550</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.007774</td>\n",
              "      <td>85.166620</td>\n",
              "      <td>52.752459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54187</th>\n",
              "      <td>bilayer graphene</td>\n",
              "      <td>0.022668</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.015547</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.016109</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.010022</td>\n",
              "      <td>84.558209</td>\n",
              "      <td>52.603072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>575426</th>\n",
              "      <td>superconducting mg</td>\n",
              "      <td>0.019298</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.025765</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.008168</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.009801</td>\n",
              "      <td>81.930302</td>\n",
              "      <td>104.647972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361014</th>\n",
              "      <td>mgb</td>\n",
              "      <td>0.042735</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.031339</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000299</td>\n",
              "      <td>0.020692</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.017152</td>\n",
              "      <td>69.185589</td>\n",
              "      <td>173.614484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>442295</th>\n",
              "      <td>pnictides</td>\n",
              "      <td>0.027563</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.022051</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.020708</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.015077</td>\n",
              "      <td>67.716621</td>\n",
              "      <td>80.723342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324952</th>\n",
              "      <td>learning</td>\n",
              "      <td>0.055689</td>\n",
              "      <td>0.000823</td>\n",
              "      <td>0.032204</td>\n",
              "      <td>0.000673</td>\n",
              "      <td>0.000793</td>\n",
              "      <td>0.052870</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.027222</td>\n",
              "      <td>66.709190</td>\n",
              "      <td>137.390556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>658808</th>\n",
              "      <td>weyl</td>\n",
              "      <td>0.031700</td>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.024496</td>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.022274</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>0.014302</td>\n",
              "      <td>66.511529</td>\n",
              "      <td>85.412687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379762</th>\n",
              "      <td>nanoribbons</td>\n",
              "      <td>0.017774</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.015018</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.010757</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.007958</td>\n",
              "      <td>56.465812</td>\n",
              "      <td>41.769231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515903</th>\n",
              "      <td>semimetal</td>\n",
              "      <td>0.037464</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>0.026756</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.000545</td>\n",
              "      <td>0.030730</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.019192</td>\n",
              "      <td>56.435600</td>\n",
              "      <td>90.031396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306718</th>\n",
              "      <td>iron pnictides</td>\n",
              "      <td>0.021401</td>\n",
              "      <td>0.000519</td>\n",
              "      <td>0.012128</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.009994</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.007993</td>\n",
              "      <td>53.765313</td>\n",
              "      <td>85.338420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339696</th>\n",
              "      <td>machine</td>\n",
              "      <td>0.046707</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.021099</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.000808</td>\n",
              "      <td>0.042130</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.018333</td>\n",
              "      <td>52.114352</td>\n",
              "      <td>98.633333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391508</th>\n",
              "      <td>non hermiticity</td>\n",
              "      <td>0.028609</td>\n",
              "      <td>0.000783</td>\n",
              "      <td>0.018205</td>\n",
              "      <td>0.000654</td>\n",
              "      <td>0.000397</td>\n",
              "      <td>0.019414</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.011808</td>\n",
              "      <td>48.961073</td>\n",
              "      <td>59.560893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188572</th>\n",
              "      <td>engineering</td>\n",
              "      <td>0.068803</td>\n",
              "      <td>0.001361</td>\n",
              "      <td>0.017964</td>\n",
              "      <td>0.001030</td>\n",
              "      <td>0.001769</td>\n",
              "      <td>0.054238</td>\n",
              "      <td>0.000758</td>\n",
              "      <td>0.012745</td>\n",
              "      <td>30.659948</td>\n",
              "      <td>16.810018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386428</th>\n",
              "      <td>neural network</td>\n",
              "      <td>0.036091</td>\n",
              "      <td>0.000693</td>\n",
              "      <td>0.017293</td>\n",
              "      <td>0.000649</td>\n",
              "      <td>0.000978</td>\n",
              "      <td>0.027870</td>\n",
              "      <td>0.000419</td>\n",
              "      <td>0.011949</td>\n",
              "      <td>28.507407</td>\n",
              "      <td>28.519156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>534311</th>\n",
              "      <td>skyrmion</td>\n",
              "      <td>0.023128</td>\n",
              "      <td>0.000818</td>\n",
              "      <td>0.014607</td>\n",
              "      <td>0.000789</td>\n",
              "      <td>0.000598</td>\n",
              "      <td>0.017031</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.010866</td>\n",
              "      <td>28.482160</td>\n",
              "      <td>54.516299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>408401</th>\n",
              "      <td>optical lattice</td>\n",
              "      <td>0.031522</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.000525</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>0.024160</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.012534</td>\n",
              "      <td>26.334242</td>\n",
              "      <td>20.493188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>547455</th>\n",
              "      <td>spin or coupling</td>\n",
              "      <td>0.042289</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.000894</td>\n",
              "      <td>0.001634</td>\n",
              "      <td>0.036721</td>\n",
              "      <td>0.000545</td>\n",
              "      <td>0.011884</td>\n",
              "      <td>22.479162</td>\n",
              "      <td>21.825144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268110</th>\n",
              "      <td>hermiticity</td>\n",
              "      <td>0.031860</td>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.018205</td>\n",
              "      <td>0.000654</td>\n",
              "      <td>0.000991</td>\n",
              "      <td>0.021615</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.012008</td>\n",
              "      <td>21.805344</td>\n",
              "      <td>60.570399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386412</th>\n",
              "      <td>neural</td>\n",
              "      <td>0.037757</td>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.018550</td>\n",
              "      <td>0.000649</td>\n",
              "      <td>0.001388</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>0.000668</td>\n",
              "      <td>0.012962</td>\n",
              "      <td>21.617143</td>\n",
              "      <td>19.414522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625157</th>\n",
              "      <td>tunable</td>\n",
              "      <td>0.047347</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.017424</td>\n",
              "      <td>0.001104</td>\n",
              "      <td>0.001906</td>\n",
              "      <td>0.039828</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>0.012774</td>\n",
              "      <td>20.898257</td>\n",
              "      <td>13.923161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306641</th>\n",
              "      <td>iron</td>\n",
              "      <td>0.036145</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.023346</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.029246</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.017439</td>\n",
              "      <td>19.284905</td>\n",
              "      <td>28.512262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154695</th>\n",
              "      <td>discrete wave superconducting</td>\n",
              "      <td>0.025878</td>\n",
              "      <td>0.000580</td>\n",
              "      <td>0.016636</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000862</td>\n",
              "      <td>0.015902</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>0.009174</td>\n",
              "      <td>18.439668</td>\n",
              "      <td>32.844037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360895</th>\n",
              "      <td>mg</td>\n",
              "      <td>0.057971</td>\n",
              "      <td>0.000823</td>\n",
              "      <td>0.054750</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.002141</td>\n",
              "      <td>0.038116</td>\n",
              "      <td>0.000607</td>\n",
              "      <td>0.032399</td>\n",
              "      <td>17.805608</td>\n",
              "      <td>53.409066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346250</th>\n",
              "      <td>manganites</td>\n",
              "      <td>0.018600</td>\n",
              "      <td>0.000540</td>\n",
              "      <td>0.016058</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000693</td>\n",
              "      <td>0.011009</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.008819</td>\n",
              "      <td>15.883093</td>\n",
              "      <td>29.754467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81229</th>\n",
              "      <td>circuit</td>\n",
              "      <td>0.042541</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.014043</td>\n",
              "      <td>0.000693</td>\n",
              "      <td>0.002527</td>\n",
              "      <td>0.034537</td>\n",
              "      <td>0.000918</td>\n",
              "      <td>0.009630</td>\n",
              "      <td>13.666306</td>\n",
              "      <td>10.484259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139196</th>\n",
              "      <td>design</td>\n",
              "      <td>0.092983</td>\n",
              "      <td>0.003697</td>\n",
              "      <td>0.015901</td>\n",
              "      <td>0.000773</td>\n",
              "      <td>0.006422</td>\n",
              "      <td>0.084907</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.009407</td>\n",
              "      <td>13.221296</td>\n",
              "      <td>30.759532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410846</th>\n",
              "      <td>or coupling</td>\n",
              "      <td>0.045157</td>\n",
              "      <td>0.001429</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.000894</td>\n",
              "      <td>0.002995</td>\n",
              "      <td>0.038393</td>\n",
              "      <td>0.000545</td>\n",
              "      <td>0.012018</td>\n",
              "      <td>12.819657</td>\n",
              "      <td>22.070370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151757</th>\n",
              "      <td>dirac</td>\n",
              "      <td>0.055019</td>\n",
              "      <td>0.001167</td>\n",
              "      <td>0.021665</td>\n",
              "      <td>0.001104</td>\n",
              "      <td>0.003674</td>\n",
              "      <td>0.046899</td>\n",
              "      <td>0.000696</td>\n",
              "      <td>0.017014</td>\n",
              "      <td>12.765454</td>\n",
              "      <td>24.437281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>652157</th>\n",
              "      <td>waals</td>\n",
              "      <td>0.034175</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.017153</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.002089</td>\n",
              "      <td>0.026218</td>\n",
              "      <td>0.000513</td>\n",
              "      <td>0.011848</td>\n",
              "      <td>12.552765</td>\n",
              "      <td>23.109722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137514</th>\n",
              "      <td>derive waals</td>\n",
              "      <td>0.034175</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.017153</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.002089</td>\n",
              "      <td>0.026118</td>\n",
              "      <td>0.000513</td>\n",
              "      <td>0.011848</td>\n",
              "      <td>12.504853</td>\n",
              "      <td>23.109722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>644292</th>\n",
              "      <td>van derive waals</td>\n",
              "      <td>0.034175</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.017153</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.002089</td>\n",
              "      <td>0.026018</td>\n",
              "      <td>0.000513</td>\n",
              "      <td>0.011848</td>\n",
              "      <td>12.456942</td>\n",
              "      <td>23.109722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379842</th>\n",
              "      <td>nanoscale</td>\n",
              "      <td>0.046896</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.019986</td>\n",
              "      <td>0.001506</td>\n",
              "      <td>0.003364</td>\n",
              "      <td>0.041116</td>\n",
              "      <td>0.001835</td>\n",
              "      <td>0.013066</td>\n",
              "      <td>12.222790</td>\n",
              "      <td>7.120859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>644288</th>\n",
              "      <td>van derive</td>\n",
              "      <td>0.034700</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.017153</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.002321</td>\n",
              "      <td>0.026719</td>\n",
              "      <td>0.000513</td>\n",
              "      <td>0.011949</td>\n",
              "      <td>11.513089</td>\n",
              "      <td>23.307241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182792</th>\n",
              "      <td>emergence</td>\n",
              "      <td>0.139760</td>\n",
              "      <td>0.011189</td>\n",
              "      <td>0.019668</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.012252</td>\n",
              "      <td>0.127593</td>\n",
              "      <td>0.001089</td>\n",
              "      <td>0.014630</td>\n",
              "      <td>10.414391</td>\n",
              "      <td>13.433657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>619876</th>\n",
              "      <td>transition metal</td>\n",
              "      <td>0.042784</td>\n",
              "      <td>0.001642</td>\n",
              "      <td>0.015729</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.003364</td>\n",
              "      <td>0.034935</td>\n",
              "      <td>0.001223</td>\n",
              "      <td>0.011229</td>\n",
              "      <td>10.385195</td>\n",
              "      <td>9.180076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>473998</th>\n",
              "      <td>qubit</td>\n",
              "      <td>0.039958</td>\n",
              "      <td>0.003396</td>\n",
              "      <td>0.020882</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.003364</td>\n",
              "      <td>0.033148</td>\n",
              "      <td>0.001223</td>\n",
              "      <td>0.013339</td>\n",
              "      <td>9.854040</td>\n",
              "      <td>10.904406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373238</th>\n",
              "      <td>monolayer</td>\n",
              "      <td>0.052239</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.024902</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.004084</td>\n",
              "      <td>0.040145</td>\n",
              "      <td>0.000817</td>\n",
              "      <td>0.016648</td>\n",
              "      <td>9.830223</td>\n",
              "      <td>20.383179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>626819</th>\n",
              "      <td>twisted</td>\n",
              "      <td>0.037515</td>\n",
              "      <td>0.001131</td>\n",
              "      <td>0.019065</td>\n",
              "      <td>0.000826</td>\n",
              "      <td>0.002974</td>\n",
              "      <td>0.028354</td>\n",
              "      <td>0.000396</td>\n",
              "      <td>0.013559</td>\n",
              "      <td>9.534177</td>\n",
              "      <td>34.216127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380666</th>\n",
              "      <td>nanowires</td>\n",
              "      <td>0.025758</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.021212</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.002178</td>\n",
              "      <td>0.020036</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.013552</td>\n",
              "      <td>9.198945</td>\n",
              "      <td>8.936060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143097</th>\n",
              "      <td>devices</td>\n",
              "      <td>0.125499</td>\n",
              "      <td>0.008392</td>\n",
              "      <td>0.013622</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.011927</td>\n",
              "      <td>0.107427</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.009609</td>\n",
              "      <td>9.007356</td>\n",
              "      <td>6.284588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>631137</th>\n",
              "      <td>ultracold</td>\n",
              "      <td>0.046102</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.020386</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>0.033836</td>\n",
              "      <td>0.001516</td>\n",
              "      <td>0.012855</td>\n",
              "      <td>8.581395</td>\n",
              "      <td>8.477574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>642907</th>\n",
              "      <td>valley</td>\n",
              "      <td>0.026018</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.012099</td>\n",
              "      <td>0.000883</td>\n",
              "      <td>0.002321</td>\n",
              "      <td>0.019265</td>\n",
              "      <td>0.000253</td>\n",
              "      <td>0.007525</td>\n",
              "      <td>8.301274</td>\n",
              "      <td>29.775990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190204</th>\n",
              "      <td>entanglement</td>\n",
              "      <td>0.050907</td>\n",
              "      <td>0.003515</td>\n",
              "      <td>0.026042</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.005199</td>\n",
              "      <td>0.042029</td>\n",
              "      <td>0.002752</td>\n",
              "      <td>0.017492</td>\n",
              "      <td>8.084483</td>\n",
              "      <td>6.355544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>604101</th>\n",
              "      <td>thermoelectric</td>\n",
              "      <td>0.023105</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.015162</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.002115</td>\n",
              "      <td>0.016814</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.008837</td>\n",
              "      <td>7.951313</td>\n",
              "      <td>28.895434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154652</th>\n",
              "      <td>discrete wave</td>\n",
              "      <td>0.038817</td>\n",
              "      <td>0.002075</td>\n",
              "      <td>0.020333</td>\n",
              "      <td>0.000540</td>\n",
              "      <td>0.003932</td>\n",
              "      <td>0.028440</td>\n",
              "      <td>0.000495</td>\n",
              "      <td>0.011621</td>\n",
              "      <td>7.233333</td>\n",
              "      <td>23.471682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441143</th>\n",
              "      <td>plasmon</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.021505</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.002785</td>\n",
              "      <td>0.019723</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.011303</td>\n",
              "      <td>7.082266</td>\n",
              "      <td>36.961938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379467</th>\n",
              "      <td>nanoparticles</td>\n",
              "      <td>0.024006</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.014254</td>\n",
              "      <td>0.001263</td>\n",
              "      <td>0.002307</td>\n",
              "      <td>0.016308</td>\n",
              "      <td>0.001837</td>\n",
              "      <td>0.008617</td>\n",
              "      <td>7.068597</td>\n",
              "      <td>4.690690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625472</th>\n",
              "      <td>tuning</td>\n",
              "      <td>0.077658</td>\n",
              "      <td>0.003750</td>\n",
              "      <td>0.013046</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.009174</td>\n",
              "      <td>0.064352</td>\n",
              "      <td>0.000910</td>\n",
              "      <td>0.008755</td>\n",
              "      <td>7.014352</td>\n",
              "      <td>9.621824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144974</th>\n",
              "      <td>diamond</td>\n",
              "      <td>0.029703</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.022631</td>\n",
              "      <td>0.001081</td>\n",
              "      <td>0.002178</td>\n",
              "      <td>0.014828</td>\n",
              "      <td>0.000769</td>\n",
              "      <td>0.008313</td>\n",
              "      <td>6.807965</td>\n",
              "      <td>10.809331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54120</th>\n",
              "      <td>bilayer</td>\n",
              "      <td>0.044785</td>\n",
              "      <td>0.003396</td>\n",
              "      <td>0.026741</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.005810</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>0.002141</td>\n",
              "      <td>0.018426</td>\n",
              "      <td>6.453947</td>\n",
              "      <td>8.607540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389618</th>\n",
              "      <td>nodal</td>\n",
              "      <td>0.020394</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>0.011955</td>\n",
              "      <td>0.000671</td>\n",
              "      <td>0.002274</td>\n",
              "      <td>0.014311</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>0.007766</td>\n",
              "      <td>6.292101</td>\n",
              "      <td>30.293987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348760</th>\n",
              "      <td>materials</td>\n",
              "      <td>0.267216</td>\n",
              "      <td>0.036398</td>\n",
              "      <td>0.043250</td>\n",
              "      <td>0.003221</td>\n",
              "      <td>0.041590</td>\n",
              "      <td>0.250075</td>\n",
              "      <td>0.005560</td>\n",
              "      <td>0.032082</td>\n",
              "      <td>6.012834</td>\n",
              "      <td>5.770454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472483</th>\n",
              "      <td>quantum spin</td>\n",
              "      <td>0.026618</td>\n",
              "      <td>0.001422</td>\n",
              "      <td>0.013734</td>\n",
              "      <td>0.000904</td>\n",
              "      <td>0.003538</td>\n",
              "      <td>0.020658</td>\n",
              "      <td>0.001387</td>\n",
              "      <td>0.007872</td>\n",
              "      <td>5.838901</td>\n",
              "      <td>5.675851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69268</th>\n",
              "      <td>carbon nanotubes</td>\n",
              "      <td>0.034005</td>\n",
              "      <td>0.001076</td>\n",
              "      <td>0.030435</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.003747</td>\n",
              "      <td>0.021747</td>\n",
              "      <td>0.001782</td>\n",
              "      <td>0.015547</td>\n",
              "      <td>5.804155</td>\n",
              "      <td>8.722809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227064</th>\n",
              "      <td>first principles</td>\n",
              "      <td>0.065934</td>\n",
              "      <td>0.005282</td>\n",
              "      <td>0.018727</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.009099</td>\n",
              "      <td>0.052678</td>\n",
              "      <td>0.002723</td>\n",
              "      <td>0.013143</td>\n",
              "      <td>5.789365</td>\n",
              "      <td>4.827466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>351057</th>\n",
              "      <td>matter</td>\n",
              "      <td>0.079905</td>\n",
              "      <td>0.007728</td>\n",
              "      <td>0.019763</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.012383</td>\n",
              "      <td>0.071190</td>\n",
              "      <td>0.003267</td>\n",
              "      <td>0.011108</td>\n",
              "      <td>5.748945</td>\n",
              "      <td>3.399905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275978</th>\n",
              "      <td>hybrid</td>\n",
              "      <td>0.057092</td>\n",
              "      <td>0.005059</td>\n",
              "      <td>0.016977</td>\n",
              "      <td>0.001112</td>\n",
              "      <td>0.008440</td>\n",
              "      <td>0.047778</td>\n",
              "      <td>0.001634</td>\n",
              "      <td>0.012709</td>\n",
              "      <td>5.660896</td>\n",
              "      <td>7.779963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>614669</th>\n",
              "      <td>topological</td>\n",
              "      <td>0.177503</td>\n",
              "      <td>0.024648</td>\n",
              "      <td>0.076510</td>\n",
              "      <td>0.003326</td>\n",
              "      <td>0.029315</td>\n",
              "      <td>0.163114</td>\n",
              "      <td>0.007118</td>\n",
              "      <td>0.067391</td>\n",
              "      <td>5.564162</td>\n",
              "      <td>9.467361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218316</th>\n",
              "      <td>fets</td>\n",
              "      <td>0.061269</td>\n",
              "      <td>0.002762</td>\n",
              "      <td>0.026042</td>\n",
              "      <td>0.001328</td>\n",
              "      <td>0.008955</td>\n",
              "      <td>0.049565</td>\n",
              "      <td>0.003058</td>\n",
              "      <td>0.017439</td>\n",
              "      <td>5.534718</td>\n",
              "      <td>5.702452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175326</th>\n",
              "      <td>efficient</td>\n",
              "      <td>0.106596</td>\n",
              "      <td>0.010638</td>\n",
              "      <td>0.020043</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.017943</td>\n",
              "      <td>0.098056</td>\n",
              "      <td>0.001392</td>\n",
              "      <td>0.011540</td>\n",
              "      <td>5.464871</td>\n",
              "      <td>8.287335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345033</th>\n",
              "      <td>magnon</td>\n",
              "      <td>0.030473</td>\n",
              "      <td>0.001112</td>\n",
              "      <td>0.015547</td>\n",
              "      <td>0.000971</td>\n",
              "      <td>0.003969</td>\n",
              "      <td>0.021674</td>\n",
              "      <td>0.000690</td>\n",
              "      <td>0.010207</td>\n",
              "      <td>5.460941</td>\n",
              "      <td>14.787601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>547438</th>\n",
              "      <td>spin or</td>\n",
              "      <td>0.071682</td>\n",
              "      <td>0.008772</td>\n",
              "      <td>0.028986</td>\n",
              "      <td>0.001776</td>\n",
              "      <td>0.012524</td>\n",
              "      <td>0.063353</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>0.020205</td>\n",
              "      <td>5.058638</td>\n",
              "      <td>5.124174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526691</th>\n",
              "      <td>silicon</td>\n",
              "      <td>0.031544</td>\n",
              "      <td>0.002660</td>\n",
              "      <td>0.017179</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.004281</td>\n",
              "      <td>0.021533</td>\n",
              "      <td>0.002446</td>\n",
              "      <td>0.010905</td>\n",
              "      <td>5.029551</td>\n",
              "      <td>4.457339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>560774</th>\n",
              "      <td>strain</td>\n",
              "      <td>0.059758</td>\n",
              "      <td>0.005272</td>\n",
              "      <td>0.020915</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.009480</td>\n",
              "      <td>0.047024</td>\n",
              "      <td>0.002123</td>\n",
              "      <td>0.015268</td>\n",
              "      <td>4.960247</td>\n",
              "      <td>7.191224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127706</th>\n",
              "      <td>deep</td>\n",
              "      <td>0.036269</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>0.012079</td>\n",
              "      <td>0.000665</td>\n",
              "      <td>0.006422</td>\n",
              "      <td>0.030300</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.008006</td>\n",
              "      <td>4.718147</td>\n",
              "      <td>42.021415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437228</th>\n",
              "      <td>photon</td>\n",
              "      <td>0.059723</td>\n",
              "      <td>0.003630</td>\n",
              "      <td>0.023256</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.011222</td>\n",
              "      <td>0.051939</td>\n",
              "      <td>0.002426</td>\n",
              "      <td>0.015744</td>\n",
              "      <td>4.628185</td>\n",
              "      <td>6.488655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268468</th>\n",
              "      <td>heterostructures</td>\n",
              "      <td>0.043431</td>\n",
              "      <td>0.002646</td>\n",
              "      <td>0.018458</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.006673</td>\n",
              "      <td>0.030833</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.012778</td>\n",
              "      <td>4.620795</td>\n",
              "      <td>8.425667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388144</th>\n",
              "      <td>ni</td>\n",
              "      <td>0.025554</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.012360</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.003976</td>\n",
              "      <td>0.018241</td>\n",
              "      <td>0.002178</td>\n",
              "      <td>0.008611</td>\n",
              "      <td>4.588248</td>\n",
              "      <td>3.953576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72312</th>\n",
              "      <td>cavity</td>\n",
              "      <td>0.030201</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.014765</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.005173</td>\n",
              "      <td>0.023441</td>\n",
              "      <td>0.001634</td>\n",
              "      <td>0.009327</td>\n",
              "      <td>4.531580</td>\n",
              "      <td>5.709949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177874</th>\n",
              "      <td>electrolyte</td>\n",
              "      <td>0.057160</td>\n",
              "      <td>0.007663</td>\n",
              "      <td>0.016340</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.011009</td>\n",
              "      <td>0.048984</td>\n",
              "      <td>0.003364</td>\n",
              "      <td>0.009519</td>\n",
              "      <td>4.449362</td>\n",
              "      <td>2.829735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346467</th>\n",
              "      <td>many body</td>\n",
              "      <td>0.074389</td>\n",
              "      <td>0.008489</td>\n",
              "      <td>0.020476</td>\n",
              "      <td>0.001078</td>\n",
              "      <td>0.013613</td>\n",
              "      <td>0.060242</td>\n",
              "      <td>0.002538</td>\n",
              "      <td>0.015153</td>\n",
              "      <td>4.425390</td>\n",
              "      <td>5.971555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380444</th>\n",
              "      <td>nanotubes</td>\n",
              "      <td>0.035264</td>\n",
              "      <td>0.001766</td>\n",
              "      <td>0.033696</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.005743</td>\n",
              "      <td>0.024907</td>\n",
              "      <td>0.002674</td>\n",
              "      <td>0.019051</td>\n",
              "      <td>4.336835</td>\n",
              "      <td>7.125872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430311</th>\n",
              "      <td>perovskite</td>\n",
              "      <td>0.022870</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.014294</td>\n",
              "      <td>0.000848</td>\n",
              "      <td>0.003589</td>\n",
              "      <td>0.015547</td>\n",
              "      <td>0.001480</td>\n",
              "      <td>0.008898</td>\n",
              "      <td>4.332183</td>\n",
              "      <td>6.010985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501265</th>\n",
              "      <td>reveal</td>\n",
              "      <td>0.165431</td>\n",
              "      <td>0.016575</td>\n",
              "      <td>0.012451</td>\n",
              "      <td>0.000763</td>\n",
              "      <td>0.032454</td>\n",
              "      <td>0.140278</td>\n",
              "      <td>0.000464</td>\n",
              "      <td>0.007805</td>\n",
              "      <td>4.322391</td>\n",
              "      <td>16.816872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>587499</th>\n",
              "      <td>tc</td>\n",
              "      <td>0.053140</td>\n",
              "      <td>0.005070</td>\n",
              "      <td>0.016717</td>\n",
              "      <td>0.000540</td>\n",
              "      <td>0.008429</td>\n",
              "      <td>0.035938</td>\n",
              "      <td>0.000709</td>\n",
              "      <td>0.009351</td>\n",
              "      <td>4.263436</td>\n",
              "      <td>13.190909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317335</th>\n",
              "      <td>laminar</td>\n",
              "      <td>0.035120</td>\n",
              "      <td>0.002926</td>\n",
              "      <td>0.023593</td>\n",
              "      <td>0.001256</td>\n",
              "      <td>0.005165</td>\n",
              "      <td>0.022018</td>\n",
              "      <td>0.001924</td>\n",
              "      <td>0.011621</td>\n",
              "      <td>4.263357</td>\n",
              "      <td>6.039755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>580706</th>\n",
              "      <td>switching</td>\n",
              "      <td>0.042338</td>\n",
              "      <td>0.003040</td>\n",
              "      <td>0.016230</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.008563</td>\n",
              "      <td>0.035241</td>\n",
              "      <td>0.001837</td>\n",
              "      <td>0.008981</td>\n",
              "      <td>4.115588</td>\n",
              "      <td>4.889294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107188</th>\n",
              "      <td>control</td>\n",
              "      <td>0.138257</td>\n",
              "      <td>0.022099</td>\n",
              "      <td>0.031274</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>0.030275</td>\n",
              "      <td>0.124537</td>\n",
              "      <td>0.004628</td>\n",
              "      <td>0.023016</td>\n",
              "      <td>4.113496</td>\n",
              "      <td>4.972834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192002</th>\n",
              "      <td>epitaxial</td>\n",
              "      <td>0.026241</td>\n",
              "      <td>0.001739</td>\n",
              "      <td>0.014065</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>0.004853</td>\n",
              "      <td>0.019949</td>\n",
              "      <td>0.001625</td>\n",
              "      <td>0.008801</td>\n",
              "      <td>4.110828</td>\n",
              "      <td>5.417453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419231</th>\n",
              "      <td>oxide</td>\n",
              "      <td>0.045332</td>\n",
              "      <td>0.003810</td>\n",
              "      <td>0.021181</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.009402</td>\n",
              "      <td>0.038145</td>\n",
              "      <td>0.002723</td>\n",
              "      <td>0.012787</td>\n",
              "      <td>4.056882</td>\n",
              "      <td>4.696727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>546688</th>\n",
              "      <td>spin glass</td>\n",
              "      <td>0.027883</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.021127</td>\n",
              "      <td>0.000609</td>\n",
              "      <td>0.004456</td>\n",
              "      <td>0.017910</td>\n",
              "      <td>0.001188</td>\n",
              "      <td>0.011829</td>\n",
              "      <td>4.019502</td>\n",
              "      <td>9.955035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>543664</th>\n",
              "      <td>spectroscopy</td>\n",
              "      <td>0.081004</td>\n",
              "      <td>0.011189</td>\n",
              "      <td>0.028470</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>0.018200</td>\n",
              "      <td>0.073148</td>\n",
              "      <td>0.006665</td>\n",
              "      <td>0.019424</td>\n",
              "      <td>4.019027</td>\n",
              "      <td>2.914344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181486</th>\n",
              "      <td>electron transport</td>\n",
              "      <td>0.021916</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.012146</td>\n",
              "      <td>0.000642</td>\n",
              "      <td>0.003670</td>\n",
              "      <td>0.014668</td>\n",
              "      <td>0.001667</td>\n",
              "      <td>0.007593</td>\n",
              "      <td>3.996980</td>\n",
              "      <td>4.555651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380195</th>\n",
              "      <td>nanostructures</td>\n",
              "      <td>0.031642</td>\n",
              "      <td>0.001976</td>\n",
              "      <td>0.016766</td>\n",
              "      <td>0.001235</td>\n",
              "      <td>0.005990</td>\n",
              "      <td>0.023560</td>\n",
              "      <td>0.002123</td>\n",
              "      <td>0.010303</td>\n",
              "      <td>3.933442</td>\n",
              "      <td>4.852784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88691</th>\n",
              "      <td>comment</td>\n",
              "      <td>0.024730</td>\n",
              "      <td>0.001078</td>\n",
              "      <td>0.026408</td>\n",
              "      <td>0.000702</td>\n",
              "      <td>0.003772</td>\n",
              "      <td>0.014559</td>\n",
              "      <td>0.002001</td>\n",
              "      <td>0.017405</td>\n",
              "      <td>3.859766</td>\n",
              "      <td>8.696623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>663208</th>\n",
              "      <td>wires</td>\n",
              "      <td>0.037936</td>\n",
              "      <td>0.003901</td>\n",
              "      <td>0.019676</td>\n",
              "      <td>0.001063</td>\n",
              "      <td>0.006574</td>\n",
              "      <td>0.024654</td>\n",
              "      <td>0.001296</td>\n",
              "      <td>0.012400</td>\n",
              "      <td>3.750256</td>\n",
              "      <td>9.565360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>653071</th>\n",
              "      <td>water</td>\n",
              "      <td>0.031274</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>0.013730</td>\n",
              "      <td>0.000856</td>\n",
              "      <td>0.006065</td>\n",
              "      <td>0.022630</td>\n",
              "      <td>0.000758</td>\n",
              "      <td>0.007593</td>\n",
              "      <td>3.731096</td>\n",
              "      <td>10.015573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>328212</th>\n",
              "      <td>light</td>\n",
              "      <td>0.085609</td>\n",
              "      <td>0.012704</td>\n",
              "      <td>0.016922</td>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.019726</td>\n",
              "      <td>0.073333</td>\n",
              "      <td>0.003481</td>\n",
              "      <td>0.011782</td>\n",
              "      <td>3.717569</td>\n",
              "      <td>3.384585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4123</th>\n",
              "      <td>absolute initial</td>\n",
              "      <td>0.036262</td>\n",
              "      <td>0.004144</td>\n",
              "      <td>0.012418</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.007951</td>\n",
              "      <td>0.029259</td>\n",
              "      <td>0.002526</td>\n",
              "      <td>0.008516</td>\n",
              "      <td>3.679915</td>\n",
              "      <td>3.371441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255925</th>\n",
              "      <td>granular</td>\n",
              "      <td>0.036626</td>\n",
              "      <td>0.003534</td>\n",
              "      <td>0.033296</td>\n",
              "      <td>0.002282</td>\n",
              "      <td>0.006004</td>\n",
              "      <td>0.021909</td>\n",
              "      <td>0.003703</td>\n",
              "      <td>0.016575</td>\n",
              "      <td>3.648933</td>\n",
              "      <td>4.476482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79744</th>\n",
              "      <td>chemical</td>\n",
              "      <td>0.071887</td>\n",
              "      <td>0.008489</td>\n",
              "      <td>0.012563</td>\n",
              "      <td>0.001081</td>\n",
              "      <td>0.017125</td>\n",
              "      <td>0.062444</td>\n",
              "      <td>0.001213</td>\n",
              "      <td>0.008120</td>\n",
              "      <td>3.646267</td>\n",
              "      <td>6.692593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132642</th>\n",
              "      <td>density function</td>\n",
              "      <td>0.075169</td>\n",
              "      <td>0.012802</td>\n",
              "      <td>0.016094</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.016820</td>\n",
              "      <td>0.061194</td>\n",
              "      <td>0.003539</td>\n",
              "      <td>0.009647</td>\n",
              "      <td>3.638272</td>\n",
              "      <td>2.725586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347820</th>\n",
              "      <td>marked</td>\n",
              "      <td>0.031405</td>\n",
              "      <td>0.002523</td>\n",
              "      <td>0.018182</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.005868</td>\n",
              "      <td>0.021236</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.008440</td>\n",
              "      <td>3.618724</td>\n",
              "      <td>85.429349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>456400</th>\n",
              "      <td>probe</td>\n",
              "      <td>0.068109</td>\n",
              "      <td>0.007634</td>\n",
              "      <td>0.022539</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.016013</td>\n",
              "      <td>0.057540</td>\n",
              "      <td>0.003538</td>\n",
              "      <td>0.015441</td>\n",
              "      <td>3.593349</td>\n",
              "      <td>4.364149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276560</th>\n",
              "      <td>hydrogen</td>\n",
              "      <td>0.023555</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.013709</td>\n",
              "      <td>0.001096</td>\n",
              "      <td>0.005307</td>\n",
              "      <td>0.018418</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.008417</td>\n",
              "      <td>3.470414</td>\n",
              "      <td>4.417913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244886</th>\n",
              "      <td>gaas</td>\n",
              "      <td>0.028935</td>\n",
              "      <td>0.003025</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.000526</td>\n",
              "      <td>0.005926</td>\n",
              "      <td>0.020446</td>\n",
              "      <td>0.001389</td>\n",
              "      <td>0.008921</td>\n",
              "      <td>3.450279</td>\n",
              "      <td>6.423473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>618573</th>\n",
              "      <td>transistors</td>\n",
              "      <td>0.024174</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.016291</td>\n",
              "      <td>0.001182</td>\n",
              "      <td>0.004587</td>\n",
              "      <td>0.015796</td>\n",
              "      <td>0.002141</td>\n",
              "      <td>0.008842</td>\n",
              "      <td>3.443585</td>\n",
              "      <td>4.130352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>570765</th>\n",
              "      <td>substrate</td>\n",
              "      <td>0.057540</td>\n",
              "      <td>0.006957</td>\n",
              "      <td>0.012649</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.012844</td>\n",
              "      <td>0.044035</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>0.007515</td>\n",
              "      <td>3.428443</td>\n",
              "      <td>8.191450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316939</th>\n",
              "      <td>ladder</td>\n",
              "      <td>0.022857</td>\n",
              "      <td>0.000838</td>\n",
              "      <td>0.017391</td>\n",
              "      <td>0.000601</td>\n",
              "      <td>0.004870</td>\n",
              "      <td>0.016682</td>\n",
              "      <td>0.001847</td>\n",
              "      <td>0.009706</td>\n",
              "      <td>3.425229</td>\n",
              "      <td>5.254300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475975</th>\n",
              "      <td>raman</td>\n",
              "      <td>0.026914</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>0.014461</td>\n",
              "      <td>0.000582</td>\n",
              "      <td>0.006673</td>\n",
              "      <td>0.022482</td>\n",
              "      <td>0.002730</td>\n",
              "      <td>0.007612</td>\n",
              "      <td>3.369250</td>\n",
              "      <td>2.788455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7600</th>\n",
              "      <td>active</td>\n",
              "      <td>0.080588</td>\n",
              "      <td>0.007105</td>\n",
              "      <td>0.028522</td>\n",
              "      <td>0.001122</td>\n",
              "      <td>0.020251</td>\n",
              "      <td>0.068152</td>\n",
              "      <td>0.001769</td>\n",
              "      <td>0.021574</td>\n",
              "      <td>3.365324</td>\n",
              "      <td>12.195516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247712</th>\n",
              "      <td>gate</td>\n",
              "      <td>0.041958</td>\n",
              "      <td>0.003521</td>\n",
              "      <td>0.015371</td>\n",
              "      <td>0.001116</td>\n",
              "      <td>0.010009</td>\n",
              "      <td>0.033620</td>\n",
              "      <td>0.000303</td>\n",
              "      <td>0.009205</td>\n",
              "      <td>3.358969</td>\n",
              "      <td>30.347938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260597</th>\n",
              "      <td>hall effect</td>\n",
              "      <td>0.037194</td>\n",
              "      <td>0.005263</td>\n",
              "      <td>0.019027</td>\n",
              "      <td>0.001698</td>\n",
              "      <td>0.008203</td>\n",
              "      <td>0.027310</td>\n",
              "      <td>0.004557</td>\n",
              "      <td>0.013026</td>\n",
              "      <td>3.329298</td>\n",
              "      <td>2.858322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362254</th>\n",
              "      <td>microwave</td>\n",
              "      <td>0.039673</td>\n",
              "      <td>0.005690</td>\n",
              "      <td>0.016107</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.008984</td>\n",
              "      <td>0.029873</td>\n",
              "      <td>0.002563</td>\n",
              "      <td>0.009312</td>\n",
              "      <td>3.324969</td>\n",
              "      <td>3.632792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>655054</th>\n",
              "      <td>wave superconducting</td>\n",
              "      <td>0.033272</td>\n",
              "      <td>0.002413</td>\n",
              "      <td>0.018484</td>\n",
              "      <td>0.000548</td>\n",
              "      <td>0.005901</td>\n",
              "      <td>0.019572</td>\n",
              "      <td>0.001101</td>\n",
              "      <td>0.011315</td>\n",
              "      <td>3.316965</td>\n",
              "      <td>10.279149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>627739</th>\n",
              "      <td>two dimensional electron</td>\n",
              "      <td>0.026087</td>\n",
              "      <td>0.001952</td>\n",
              "      <td>0.015385</td>\n",
              "      <td>0.000522</td>\n",
              "      <td>0.005242</td>\n",
              "      <td>0.017370</td>\n",
              "      <td>0.000907</td>\n",
              "      <td>0.010019</td>\n",
              "      <td>3.313364</td>\n",
              "      <td>11.042256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>537024</th>\n",
              "      <td>soft</td>\n",
              "      <td>0.044118</td>\n",
              "      <td>0.003521</td>\n",
              "      <td>0.013866</td>\n",
              "      <td>0.001333</td>\n",
              "      <td>0.010312</td>\n",
              "      <td>0.033985</td>\n",
              "      <td>0.003332</td>\n",
              "      <td>0.009583</td>\n",
              "      <td>3.295582</td>\n",
              "      <td>2.875663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384249</th>\n",
              "      <td>nematic</td>\n",
              "      <td>0.023529</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.013529</td>\n",
              "      <td>0.001104</td>\n",
              "      <td>0.004956</td>\n",
              "      <td>0.016086</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.009319</td>\n",
              "      <td>3.245529</td>\n",
              "      <td>6.094520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472203</th>\n",
              "      <td>quantum phase</td>\n",
              "      <td>0.030635</td>\n",
              "      <td>0.003683</td>\n",
              "      <td>0.014778</td>\n",
              "      <td>0.001295</td>\n",
              "      <td>0.007178</td>\n",
              "      <td>0.023123</td>\n",
              "      <td>0.003332</td>\n",
              "      <td>0.009152</td>\n",
              "      <td>3.221583</td>\n",
              "      <td>2.746315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472235</th>\n",
              "      <td>quantum phase transition</td>\n",
              "      <td>0.026744</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.011803</td>\n",
              "      <td>0.000710</td>\n",
              "      <td>0.005896</td>\n",
              "      <td>0.018982</td>\n",
              "      <td>0.002307</td>\n",
              "      <td>0.008031</td>\n",
              "      <td>3.219492</td>\n",
              "      <td>3.481156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>667036</th>\n",
              "      <td>x ray</td>\n",
              "      <td>0.049743</td>\n",
              "      <td>0.007394</td>\n",
              "      <td>0.016774</td>\n",
              "      <td>0.002361</td>\n",
              "      <td>0.013042</td>\n",
              "      <td>0.041738</td>\n",
              "      <td>0.003670</td>\n",
              "      <td>0.009082</td>\n",
              "      <td>3.200243</td>\n",
              "      <td>2.474850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72922</th>\n",
              "      <td>cell</td>\n",
              "      <td>0.061501</td>\n",
              "      <td>0.005272</td>\n",
              "      <td>0.015280</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>0.016208</td>\n",
              "      <td>0.051636</td>\n",
              "      <td>0.002392</td>\n",
              "      <td>0.009707</td>\n",
              "      <td>3.185853</td>\n",
              "      <td>4.058249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428436</th>\n",
              "      <td>percolation</td>\n",
              "      <td>0.026616</td>\n",
              "      <td>0.003630</td>\n",
              "      <td>0.016345</td>\n",
              "      <td>0.001210</td>\n",
              "      <td>0.006520</td>\n",
              "      <td>0.020764</td>\n",
              "      <td>0.002173</td>\n",
              "      <td>0.009485</td>\n",
              "      <td>3.184428</td>\n",
              "      <td>4.363846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149406</th>\n",
              "      <td>dimensional electron</td>\n",
              "      <td>0.028261</td>\n",
              "      <td>0.001952</td>\n",
              "      <td>0.015385</td>\n",
              "      <td>0.000570</td>\n",
              "      <td>0.006351</td>\n",
              "      <td>0.020183</td>\n",
              "      <td>0.001311</td>\n",
              "      <td>0.010659</td>\n",
              "      <td>3.177778</td>\n",
              "      <td>8.132594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80379</th>\n",
              "      <td>chiral</td>\n",
              "      <td>0.059659</td>\n",
              "      <td>0.006568</td>\n",
              "      <td>0.023810</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.016071</td>\n",
              "      <td>0.050734</td>\n",
              "      <td>0.003267</td>\n",
              "      <td>0.019314</td>\n",
              "      <td>3.156867</td>\n",
              "      <td>5.911546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76908</th>\n",
              "      <td>chaotic</td>\n",
              "      <td>0.034351</td>\n",
              "      <td>0.003610</td>\n",
              "      <td>0.018149</td>\n",
              "      <td>0.000755</td>\n",
              "      <td>0.006259</td>\n",
              "      <td>0.019715</td>\n",
              "      <td>0.003145</td>\n",
              "      <td>0.012232</td>\n",
              "      <td>3.149744</td>\n",
              "      <td>3.889035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56646</th>\n",
              "      <td>body</td>\n",
              "      <td>0.091392</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.026420</td>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.024503</td>\n",
              "      <td>0.076762</td>\n",
              "      <td>0.003383</td>\n",
              "      <td>0.017617</td>\n",
              "      <td>3.132744</td>\n",
              "      <td>5.206899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>651010</th>\n",
              "      <td>vortex</td>\n",
              "      <td>0.061192</td>\n",
              "      <td>0.009422</td>\n",
              "      <td>0.035427</td>\n",
              "      <td>0.002706</td>\n",
              "      <td>0.014259</td>\n",
              "      <td>0.044650</td>\n",
              "      <td>0.005367</td>\n",
              "      <td>0.024771</td>\n",
              "      <td>3.131309</td>\n",
              "      <td>4.615285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472985</th>\n",
              "      <td>quantum well</td>\n",
              "      <td>0.028743</td>\n",
              "      <td>0.003819</td>\n",
              "      <td>0.014563</td>\n",
              "      <td>0.000596</td>\n",
              "      <td>0.005434</td>\n",
              "      <td>0.016997</td>\n",
              "      <td>0.001852</td>\n",
              "      <td>0.008084</td>\n",
              "      <td>3.127999</td>\n",
              "      <td>4.365400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279432</th>\n",
              "      <td>imaging</td>\n",
              "      <td>0.051494</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.014249</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.014099</td>\n",
              "      <td>0.044064</td>\n",
              "      <td>0.001817</td>\n",
              "      <td>0.008943</td>\n",
              "      <td>3.125323</td>\n",
              "      <td>4.923214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>598</th>\n",
              "      <td>2d</td>\n",
              "      <td>0.078843</td>\n",
              "      <td>0.016478</td>\n",
              "      <td>0.026718</td>\n",
              "      <td>0.003617</td>\n",
              "      <td>0.022191</td>\n",
              "      <td>0.068354</td>\n",
              "      <td>0.005148</td>\n",
              "      <td>0.016880</td>\n",
              "      <td>3.080222</td>\n",
              "      <td>3.278927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>455347</th>\n",
              "      <td>principles</td>\n",
              "      <td>0.094849</td>\n",
              "      <td>0.017713</td>\n",
              "      <td>0.021352</td>\n",
              "      <td>0.003306</td>\n",
              "      <td>0.028135</td>\n",
              "      <td>0.086642</td>\n",
              "      <td>0.005173</td>\n",
              "      <td>0.015496</td>\n",
              "      <td>3.079567</td>\n",
              "      <td>2.995682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140396</th>\n",
              "      <td>detection</td>\n",
              "      <td>0.062090</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>0.015584</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.017437</td>\n",
              "      <td>0.053467</td>\n",
              "      <td>0.002446</td>\n",
              "      <td>0.011296</td>\n",
              "      <td>3.066198</td>\n",
              "      <td>4.617361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122516</th>\n",
              "      <td>cuprates</td>\n",
              "      <td>0.054386</td>\n",
              "      <td>0.006653</td>\n",
              "      <td>0.031480</td>\n",
              "      <td>0.001331</td>\n",
              "      <td>0.012082</td>\n",
              "      <td>0.037003</td>\n",
              "      <td>0.003603</td>\n",
              "      <td>0.021407</td>\n",
              "      <td>3.062648</td>\n",
              "      <td>5.942151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>408057</th>\n",
              "      <td>optical</td>\n",
              "      <td>0.120430</td>\n",
              "      <td>0.026643</td>\n",
              "      <td>0.048587</td>\n",
              "      <td>0.005272</td>\n",
              "      <td>0.035780</td>\n",
              "      <td>0.109176</td>\n",
              "      <td>0.013150</td>\n",
              "      <td>0.034777</td>\n",
              "      <td>3.051341</td>\n",
              "      <td>2.644706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364641</th>\n",
              "      <td>mobility</td>\n",
              "      <td>0.039695</td>\n",
              "      <td>0.005333</td>\n",
              "      <td>0.013803</td>\n",
              "      <td>0.000898</td>\n",
              "      <td>0.010009</td>\n",
              "      <td>0.030437</td>\n",
              "      <td>0.002022</td>\n",
              "      <td>0.007732</td>\n",
              "      <td>3.040925</td>\n",
              "      <td>3.824420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290249</th>\n",
              "      <td>information</td>\n",
              "      <td>0.083286</td>\n",
              "      <td>0.013986</td>\n",
              "      <td>0.013942</td>\n",
              "      <td>0.000932</td>\n",
              "      <td>0.023250</td>\n",
              "      <td>0.070556</td>\n",
              "      <td>0.001681</td>\n",
              "      <td>0.008206</td>\n",
              "      <td>3.034656</td>\n",
              "      <td>4.881506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>480101</th>\n",
              "      <td>ray</td>\n",
              "      <td>0.052601</td>\n",
              "      <td>0.009242</td>\n",
              "      <td>0.016774</td>\n",
              "      <td>0.002431</td>\n",
              "      <td>0.014067</td>\n",
              "      <td>0.042645</td>\n",
              "      <td>0.003670</td>\n",
              "      <td>0.009082</td>\n",
              "      <td>3.031534</td>\n",
              "      <td>2.474850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127878</th>\n",
              "      <td>defects</td>\n",
              "      <td>0.061632</td>\n",
              "      <td>0.009242</td>\n",
              "      <td>0.020025</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.017288</td>\n",
              "      <td>0.052315</td>\n",
              "      <td>0.004550</td>\n",
              "      <td>0.015463</td>\n",
              "      <td>3.025999</td>\n",
              "      <td>3.398759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11606</th>\n",
              "      <td>aging</td>\n",
              "      <td>0.032298</td>\n",
              "      <td>0.002232</td>\n",
              "      <td>0.014286</td>\n",
              "      <td>0.000580</td>\n",
              "      <td>0.006285</td>\n",
              "      <td>0.018973</td>\n",
              "      <td>0.001505</td>\n",
              "      <td>0.007824</td>\n",
              "      <td>3.018736</td>\n",
              "      <td>5.198790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165022</th>\n",
              "      <td>driven</td>\n",
              "      <td>0.121597</td>\n",
              "      <td>0.023256</td>\n",
              "      <td>0.036442</td>\n",
              "      <td>0.005282</td>\n",
              "      <td>0.035128</td>\n",
              "      <td>0.105823</td>\n",
              "      <td>0.010659</td>\n",
              "      <td>0.027443</td>\n",
              "      <td>3.012523</td>\n",
              "      <td>2.574706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>648223</th>\n",
              "      <td>via</td>\n",
              "      <td>0.108721</td>\n",
              "      <td>0.023776</td>\n",
              "      <td>0.027371</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.032757</td>\n",
              "      <td>0.098202</td>\n",
              "      <td>0.002141</td>\n",
              "      <td>0.022616</td>\n",
              "      <td>2.997887</td>\n",
              "      <td>10.564824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171437</th>\n",
              "      <td>edge</td>\n",
              "      <td>0.076555</td>\n",
              "      <td>0.011091</td>\n",
              "      <td>0.021850</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.020795</td>\n",
              "      <td>0.062103</td>\n",
              "      <td>0.003812</td>\n",
              "      <td>0.015670</td>\n",
              "      <td>2.986425</td>\n",
              "      <td>4.111260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179530</th>\n",
              "      <td>electron gain</td>\n",
              "      <td>0.023715</td>\n",
              "      <td>0.002941</td>\n",
              "      <td>0.016327</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>0.005556</td>\n",
              "      <td>0.016514</td>\n",
              "      <td>0.001296</td>\n",
              "      <td>0.008740</td>\n",
              "      <td>2.972477</td>\n",
              "      <td>6.742394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188896</th>\n",
              "      <td>enhanced</td>\n",
              "      <td>0.111307</td>\n",
              "      <td>0.020333</td>\n",
              "      <td>0.024825</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.032454</td>\n",
              "      <td>0.095833</td>\n",
              "      <td>0.005156</td>\n",
              "      <td>0.017626</td>\n",
              "      <td>2.952921</td>\n",
              "      <td>3.418312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357692</th>\n",
              "      <td>mesoscopic</td>\n",
              "      <td>0.035524</td>\n",
              "      <td>0.004624</td>\n",
              "      <td>0.021429</td>\n",
              "      <td>0.000570</td>\n",
              "      <td>0.007212</td>\n",
              "      <td>0.020764</td>\n",
              "      <td>0.001401</td>\n",
              "      <td>0.012681</td>\n",
              "      <td>2.878985</td>\n",
              "      <td>9.051374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69167</th>\n",
              "      <td>carbon</td>\n",
              "      <td>0.045340</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.032609</td>\n",
              "      <td>0.002221</td>\n",
              "      <td>0.010346</td>\n",
              "      <td>0.029740</td>\n",
              "      <td>0.005248</td>\n",
              "      <td>0.018216</td>\n",
              "      <td>2.874584</td>\n",
              "      <td>3.470934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247509</th>\n",
              "      <td>gases</td>\n",
              "      <td>0.030492</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.021907</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.007339</td>\n",
              "      <td>0.020882</td>\n",
              "      <td>0.004003</td>\n",
              "      <td>0.013654</td>\n",
              "      <td>2.845164</td>\n",
              "      <td>3.411086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346410</th>\n",
              "      <td>many</td>\n",
              "      <td>0.193319</td>\n",
              "      <td>0.047538</td>\n",
              "      <td>0.026298</td>\n",
              "      <td>0.003373</td>\n",
              "      <td>0.061774</td>\n",
              "      <td>0.175323</td>\n",
              "      <td>0.007079</td>\n",
              "      <td>0.021533</td>\n",
              "      <td>2.838145</td>\n",
              "      <td>3.041985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>562728</th>\n",
              "      <td>stripe</td>\n",
              "      <td>0.035088</td>\n",
              "      <td>0.005690</td>\n",
              "      <td>0.022857</td>\n",
              "      <td>0.000583</td>\n",
              "      <td>0.009211</td>\n",
              "      <td>0.025994</td>\n",
              "      <td>0.001664</td>\n",
              "      <td>0.014985</td>\n",
              "      <td>2.822018</td>\n",
              "      <td>9.004811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>537376</th>\n",
              "      <td>solid</td>\n",
              "      <td>0.072604</td>\n",
              "      <td>0.019324</td>\n",
              "      <td>0.024194</td>\n",
              "      <td>0.003233</td>\n",
              "      <td>0.023142</td>\n",
              "      <td>0.063291</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>0.014210</td>\n",
              "      <td>2.734922</td>\n",
              "      <td>2.269267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>470795</th>\n",
              "      <td>quantum critical</td>\n",
              "      <td>0.026627</td>\n",
              "      <td>0.003521</td>\n",
              "      <td>0.016765</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.006534</td>\n",
              "      <td>0.017774</td>\n",
              "      <td>0.002051</td>\n",
              "      <td>0.010009</td>\n",
              "      <td>2.720154</td>\n",
              "      <td>4.880457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550408</th>\n",
              "      <td>sr</td>\n",
              "      <td>0.042975</td>\n",
              "      <td>0.004677</td>\n",
              "      <td>0.024096</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.009450</td>\n",
              "      <td>0.025634</td>\n",
              "      <td>0.003068</td>\n",
              "      <td>0.012996</td>\n",
              "      <td>2.712591</td>\n",
              "      <td>4.235674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78721</th>\n",
              "      <td>charge density</td>\n",
              "      <td>0.031634</td>\n",
              "      <td>0.005442</td>\n",
              "      <td>0.012288</td>\n",
              "      <td>0.000829</td>\n",
              "      <td>0.008286</td>\n",
              "      <td>0.022416</td>\n",
              "      <td>0.001187</td>\n",
              "      <td>0.007605</td>\n",
              "      <td>2.705191</td>\n",
              "      <td>6.405584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145284</th>\n",
              "      <td>dielectric</td>\n",
              "      <td>0.041434</td>\n",
              "      <td>0.006568</td>\n",
              "      <td>0.015810</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.010616</td>\n",
              "      <td>0.028595</td>\n",
              "      <td>0.002730</td>\n",
              "      <td>0.007645</td>\n",
              "      <td>2.693610</td>\n",
              "      <td>2.800714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148560</th>\n",
              "      <td>dilute</td>\n",
              "      <td>0.034139</td>\n",
              "      <td>0.008028</td>\n",
              "      <td>0.020236</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.009823</td>\n",
              "      <td>0.026224</td>\n",
              "      <td>0.002173</td>\n",
              "      <td>0.013330</td>\n",
              "      <td>2.669730</td>\n",
              "      <td>6.132973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217143</th>\n",
              "      <td>ferroelectric</td>\n",
              "      <td>0.030227</td>\n",
              "      <td>0.003683</td>\n",
              "      <td>0.016332</td>\n",
              "      <td>0.001125</td>\n",
              "      <td>0.007582</td>\n",
              "      <td>0.020214</td>\n",
              "      <td>0.003267</td>\n",
              "      <td>0.011008</td>\n",
              "      <td>2.666246</td>\n",
              "      <td>3.369275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>607275</th>\n",
              "      <td>thin films</td>\n",
              "      <td>0.046312</td>\n",
              "      <td>0.009217</td>\n",
              "      <td>0.024701</td>\n",
              "      <td>0.001686</td>\n",
              "      <td>0.013150</td>\n",
              "      <td>0.034935</td>\n",
              "      <td>0.004893</td>\n",
              "      <td>0.017182</td>\n",
              "      <td>2.656678</td>\n",
              "      <td>3.511598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63797</th>\n",
              "      <td>bulk</td>\n",
              "      <td>0.100142</td>\n",
              "      <td>0.020279</td>\n",
              "      <td>0.015587</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>0.032150</td>\n",
              "      <td>0.085089</td>\n",
              "      <td>0.002752</td>\n",
              "      <td>0.010168</td>\n",
              "      <td>2.646596</td>\n",
              "      <td>3.694547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91070</th>\n",
              "      <td>complex</td>\n",
              "      <td>0.116366</td>\n",
              "      <td>0.025806</td>\n",
              "      <td>0.023307</td>\n",
              "      <td>0.003364</td>\n",
              "      <td>0.036700</td>\n",
              "      <td>0.096852</td>\n",
              "      <td>0.005460</td>\n",
              "      <td>0.016455</td>\n",
              "      <td>2.639013</td>\n",
              "      <td>3.014043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438035</th>\n",
              "      <td>physical</td>\n",
              "      <td>0.173792</td>\n",
              "      <td>0.053824</td>\n",
              "      <td>0.017787</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.062891</td>\n",
              "      <td>0.164907</td>\n",
              "      <td>0.005526</td>\n",
              "      <td>0.011111</td>\n",
              "      <td>2.622099</td>\n",
              "      <td>2.010774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180988</th>\n",
              "      <td>electron structure</td>\n",
              "      <td>0.051795</td>\n",
              "      <td>0.003831</td>\n",
              "      <td>0.017143</td>\n",
              "      <td>0.001892</td>\n",
              "      <td>0.013761</td>\n",
              "      <td>0.035833</td>\n",
              "      <td>0.004659</td>\n",
              "      <td>0.011798</td>\n",
              "      <td>2.603889</td>\n",
              "      <td>2.532137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505039</th>\n",
              "      <td>role</td>\n",
              "      <td>0.094049</td>\n",
              "      <td>0.023140</td>\n",
              "      <td>0.021090</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.031193</td>\n",
              "      <td>0.081204</td>\n",
              "      <td>0.005570</td>\n",
              "      <td>0.011693</td>\n",
              "      <td>2.603295</td>\n",
              "      <td>2.099295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481071</th>\n",
              "      <td>real</td>\n",
              "      <td>0.173568</td>\n",
              "      <td>0.047934</td>\n",
              "      <td>0.018939</td>\n",
              "      <td>0.001837</td>\n",
              "      <td>0.059894</td>\n",
              "      <td>0.155342</td>\n",
              "      <td>0.003429</td>\n",
              "      <td>0.010867</td>\n",
              "      <td>2.593618</td>\n",
              "      <td>3.169060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316132</th>\n",
              "      <td>kondo</td>\n",
              "      <td>0.029954</td>\n",
              "      <td>0.005229</td>\n",
              "      <td>0.021802</td>\n",
              "      <td>0.001744</td>\n",
              "      <td>0.008148</td>\n",
              "      <td>0.021019</td>\n",
              "      <td>0.003840</td>\n",
              "      <td>0.014068</td>\n",
              "      <td>2.579555</td>\n",
              "      <td>3.663451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40439</th>\n",
              "      <td>band</td>\n",
              "      <td>0.159506</td>\n",
              "      <td>0.048750</td>\n",
              "      <td>0.030056</td>\n",
              "      <td>0.004608</td>\n",
              "      <td>0.056112</td>\n",
              "      <td>0.144725</td>\n",
              "      <td>0.007583</td>\n",
              "      <td>0.024017</td>\n",
              "      <td>2.579242</td>\n",
              "      <td>3.167337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58102</th>\n",
              "      <td>bose einstein condensate</td>\n",
              "      <td>0.052503</td>\n",
              "      <td>0.010172</td>\n",
              "      <td>0.044362</td>\n",
              "      <td>0.003778</td>\n",
              "      <td>0.013519</td>\n",
              "      <td>0.034747</td>\n",
              "      <td>0.006019</td>\n",
              "      <td>0.032095</td>\n",
              "      <td>2.570355</td>\n",
              "      <td>5.332711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176483</th>\n",
              "      <td>einstein condensate</td>\n",
              "      <td>0.052503</td>\n",
              "      <td>0.010172</td>\n",
              "      <td>0.044362</td>\n",
              "      <td>0.003778</td>\n",
              "      <td>0.013519</td>\n",
              "      <td>0.034747</td>\n",
              "      <td>0.006019</td>\n",
              "      <td>0.032095</td>\n",
              "      <td>2.570355</td>\n",
              "      <td>5.332711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>525634</th>\n",
              "      <td>signatures</td>\n",
              "      <td>0.063320</td>\n",
              "      <td>0.015267</td>\n",
              "      <td>0.013350</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.021535</td>\n",
              "      <td>0.054732</td>\n",
              "      <td>0.002785</td>\n",
              "      <td>0.008615</td>\n",
              "      <td>2.541581</td>\n",
              "      <td>3.093400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58101</th>\n",
              "      <td>bose einstein</td>\n",
              "      <td>0.054945</td>\n",
              "      <td>0.011042</td>\n",
              "      <td>0.044362</td>\n",
              "      <td>0.003778</td>\n",
              "      <td>0.014259</td>\n",
              "      <td>0.036240</td>\n",
              "      <td>0.006111</td>\n",
              "      <td>0.033106</td>\n",
              "      <td>2.541479</td>\n",
              "      <td>5.417327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319821</th>\n",
              "      <td>laser</td>\n",
              "      <td>0.043665</td>\n",
              "      <td>0.006791</td>\n",
              "      <td>0.015748</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>0.013843</td>\n",
              "      <td>0.035179</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>0.009647</td>\n",
              "      <td>2.541341</td>\n",
              "      <td>2.446603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322601</th>\n",
              "      <td>layer</td>\n",
              "      <td>0.134362</td>\n",
              "      <td>0.030560</td>\n",
              "      <td>0.031746</td>\n",
              "      <td>0.004854</td>\n",
              "      <td>0.045193</td>\n",
              "      <td>0.114639</td>\n",
              "      <td>0.009283</td>\n",
              "      <td>0.022759</td>\n",
              "      <td>2.536675</td>\n",
              "      <td>2.451735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>576646</th>\n",
              "      <td>superfluid</td>\n",
              "      <td>0.047149</td>\n",
              "      <td>0.009881</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>0.001976</td>\n",
              "      <td>0.013761</td>\n",
              "      <td>0.034554</td>\n",
              "      <td>0.006004</td>\n",
              "      <td>0.015876</td>\n",
              "      <td>2.510895</td>\n",
              "      <td>2.644145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133773</th>\n",
              "      <td>density wave</td>\n",
              "      <td>0.032907</td>\n",
              "      <td>0.003738</td>\n",
              "      <td>0.012552</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.010009</td>\n",
              "      <td>0.025117</td>\n",
              "      <td>0.002765</td>\n",
              "      <td>0.009013</td>\n",
              "      <td>2.509438</td>\n",
              "      <td>3.260007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298431</th>\n",
              "      <td>interface</td>\n",
              "      <td>0.078086</td>\n",
              "      <td>0.019324</td>\n",
              "      <td>0.025819</td>\n",
              "      <td>0.003610</td>\n",
              "      <td>0.026911</td>\n",
              "      <td>0.067528</td>\n",
              "      <td>0.008101</td>\n",
              "      <td>0.020333</td>\n",
              "      <td>2.509291</td>\n",
              "      <td>2.509995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294150</th>\n",
              "      <td>insulator transition</td>\n",
              "      <td>0.025692</td>\n",
              "      <td>0.003155</td>\n",
              "      <td>0.013554</td>\n",
              "      <td>0.000621</td>\n",
              "      <td>0.006204</td>\n",
              "      <td>0.015469</td>\n",
              "      <td>0.001794</td>\n",
              "      <td>0.008440</td>\n",
              "      <td>2.493190</td>\n",
              "      <td>4.703406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92372</th>\n",
              "      <td>composition</td>\n",
              "      <td>0.070736</td>\n",
              "      <td>0.013410</td>\n",
              "      <td>0.015094</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>0.023242</td>\n",
              "      <td>0.057840</td>\n",
              "      <td>0.002947</td>\n",
              "      <td>0.008399</td>\n",
              "      <td>2.488663</td>\n",
              "      <td>2.849930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>408987</th>\n",
              "      <td>optimal</td>\n",
              "      <td>0.080683</td>\n",
              "      <td>0.017831</td>\n",
              "      <td>0.017800</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.026147</td>\n",
              "      <td>0.064759</td>\n",
              "      <td>0.003812</td>\n",
              "      <td>0.013566</td>\n",
              "      <td>2.476710</td>\n",
              "      <td>3.559058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116991</th>\n",
              "      <td>covalent</td>\n",
              "      <td>0.043787</td>\n",
              "      <td>0.006957</td>\n",
              "      <td>0.018240</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.014067</td>\n",
              "      <td>0.034593</td>\n",
              "      <td>0.002446</td>\n",
              "      <td>0.011734</td>\n",
              "      <td>2.459094</td>\n",
              "      <td>4.796376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464332</th>\n",
              "      <td>protein</td>\n",
              "      <td>0.023839</td>\n",
              "      <td>0.002232</td>\n",
              "      <td>0.014519</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>0.005920</td>\n",
              "      <td>0.014496</td>\n",
              "      <td>0.001544</td>\n",
              "      <td>0.008493</td>\n",
              "      <td>2.448537</td>\n",
              "      <td>5.498938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370482</th>\n",
              "      <td>modulation</td>\n",
              "      <td>0.066090</td>\n",
              "      <td>0.010542</td>\n",
              "      <td>0.014967</td>\n",
              "      <td>0.001754</td>\n",
              "      <td>0.022239</td>\n",
              "      <td>0.054236</td>\n",
              "      <td>0.003267</td>\n",
              "      <td>0.009152</td>\n",
              "      <td>2.438770</td>\n",
              "      <td>2.801286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61926</th>\n",
              "      <td>breaking</td>\n",
              "      <td>0.080511</td>\n",
              "      <td>0.017956</td>\n",
              "      <td>0.018205</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.029705</td>\n",
              "      <td>0.071944</td>\n",
              "      <td>0.004652</td>\n",
              "      <td>0.010741</td>\n",
              "      <td>2.421942</td>\n",
              "      <td>2.308771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361494</th>\n",
              "      <td>microscopic</td>\n",
              "      <td>0.106979</td>\n",
              "      <td>0.024818</td>\n",
              "      <td>0.018301</td>\n",
              "      <td>0.002361</td>\n",
              "      <td>0.037402</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.005990</td>\n",
              "      <td>0.012860</td>\n",
              "      <td>2.406284</td>\n",
              "      <td>2.147058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532667</th>\n",
              "      <td>sitter</td>\n",
              "      <td>0.030871</td>\n",
              "      <td>0.006211</td>\n",
              "      <td>0.019767</td>\n",
              "      <td>0.001120</td>\n",
              "      <td>0.009786</td>\n",
              "      <td>0.023531</td>\n",
              "      <td>0.004630</td>\n",
              "      <td>0.010009</td>\n",
              "      <td>2.404560</td>\n",
              "      <td>2.161864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165951</th>\n",
              "      <td>droplets</td>\n",
              "      <td>0.030814</td>\n",
              "      <td>0.004908</td>\n",
              "      <td>0.012433</td>\n",
              "      <td>0.000712</td>\n",
              "      <td>0.009712</td>\n",
              "      <td>0.023328</td>\n",
              "      <td>0.001066</td>\n",
              "      <td>0.008175</td>\n",
              "      <td>2.401854</td>\n",
              "      <td>7.669465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356451</th>\n",
              "      <td>mediated</td>\n",
              "      <td>0.042061</td>\n",
              "      <td>0.007968</td>\n",
              "      <td>0.021277</td>\n",
              "      <td>0.001227</td>\n",
              "      <td>0.014266</td>\n",
              "      <td>0.034224</td>\n",
              "      <td>0.005544</td>\n",
              "      <td>0.011251</td>\n",
              "      <td>2.399004</td>\n",
              "      <td>2.029509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377146</th>\n",
              "      <td>multi</td>\n",
              "      <td>0.169994</td>\n",
              "      <td>0.055453</td>\n",
              "      <td>0.045064</td>\n",
              "      <td>0.007905</td>\n",
              "      <td>0.061268</td>\n",
              "      <td>0.146944</td>\n",
              "      <td>0.017870</td>\n",
              "      <td>0.034430</td>\n",
              "      <td>2.398395</td>\n",
              "      <td>1.926760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248111</th>\n",
              "      <td>gauge</td>\n",
              "      <td>0.030217</td>\n",
              "      <td>0.005312</td>\n",
              "      <td>0.012324</td>\n",
              "      <td>0.000842</td>\n",
              "      <td>0.010105</td>\n",
              "      <td>0.024167</td>\n",
              "      <td>0.001585</td>\n",
              "      <td>0.007595</td>\n",
              "      <td>2.391552</td>\n",
              "      <td>4.791456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>607207</th>\n",
              "      <td>thin</td>\n",
              "      <td>0.072041</td>\n",
              "      <td>0.018416</td>\n",
              "      <td>0.031414</td>\n",
              "      <td>0.005059</td>\n",
              "      <td>0.025174</td>\n",
              "      <td>0.059942</td>\n",
              "      <td>0.008257</td>\n",
              "      <td>0.022227</td>\n",
              "      <td>2.381067</td>\n",
              "      <td>2.691981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515231</th>\n",
              "      <td>semiconductor</td>\n",
              "      <td>0.067974</td>\n",
              "      <td>0.018895</td>\n",
              "      <td>0.030624</td>\n",
              "      <td>0.005525</td>\n",
              "      <td>0.024503</td>\n",
              "      <td>0.057881</td>\n",
              "      <td>0.011081</td>\n",
              "      <td>0.020024</td>\n",
              "      <td>2.362205</td>\n",
              "      <td>1.807065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>609566</th>\n",
              "      <td>through</td>\n",
              "      <td>0.174323</td>\n",
              "      <td>0.049236</td>\n",
              "      <td>0.026265</td>\n",
              "      <td>0.004608</td>\n",
              "      <td>0.065454</td>\n",
              "      <td>0.154352</td>\n",
              "      <td>0.006116</td>\n",
              "      <td>0.017931</td>\n",
              "      <td>2.358186</td>\n",
              "      <td>2.931640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391142</th>\n",
              "      <td>non equilibrium</td>\n",
              "      <td>0.036417</td>\n",
              "      <td>0.006452</td>\n",
              "      <td>0.016279</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.011927</td>\n",
              "      <td>0.028080</td>\n",
              "      <td>0.002326</td>\n",
              "      <td>0.007732</td>\n",
              "      <td>2.354368</td>\n",
              "      <td>3.324039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42449</th>\n",
              "      <td>based</td>\n",
              "      <td>0.267487</td>\n",
              "      <td>0.086667</td>\n",
              "      <td>0.043530</td>\n",
              "      <td>0.003641</td>\n",
              "      <td>0.102856</td>\n",
              "      <td>0.242103</td>\n",
              "      <td>0.006728</td>\n",
              "      <td>0.032503</td>\n",
              "      <td>2.353814</td>\n",
              "      <td>4.831196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67903</th>\n",
              "      <td>cancer</td>\n",
              "      <td>0.028099</td>\n",
              "      <td>0.002013</td>\n",
              "      <td>0.014876</td>\n",
              "      <td>0.000616</td>\n",
              "      <td>0.005600</td>\n",
              "      <td>0.013150</td>\n",
              "      <td>0.001620</td>\n",
              "      <td>0.007645</td>\n",
              "      <td>2.348344</td>\n",
              "      <td>4.718559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376283</th>\n",
              "      <td>mott</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.004559</td>\n",
              "      <td>0.016373</td>\n",
              "      <td>0.000582</td>\n",
              "      <td>0.010361</td>\n",
              "      <td>0.024317</td>\n",
              "      <td>0.003538</td>\n",
              "      <td>0.010963</td>\n",
              "      <td>2.346918</td>\n",
              "      <td>3.098536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58080</th>\n",
              "      <td>bose</td>\n",
              "      <td>0.072039</td>\n",
              "      <td>0.017943</td>\n",
              "      <td>0.064336</td>\n",
              "      <td>0.009679</td>\n",
              "      <td>0.023241</td>\n",
              "      <td>0.054487</td>\n",
              "      <td>0.011667</td>\n",
              "      <td>0.046753</td>\n",
              "      <td>2.344442</td>\n",
              "      <td>4.007365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426662</th>\n",
              "      <td>patterns</td>\n",
              "      <td>0.055882</td>\n",
              "      <td>0.011858</td>\n",
              "      <td>0.014239</td>\n",
              "      <td>0.001976</td>\n",
              "      <td>0.021101</td>\n",
              "      <td>0.049335</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>0.008706</td>\n",
              "      <td>2.338028</td>\n",
              "      <td>2.207999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87577</th>\n",
              "      <td>colloidal</td>\n",
              "      <td>0.027428</td>\n",
              "      <td>0.003306</td>\n",
              "      <td>0.016781</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.007886</td>\n",
              "      <td>0.018339</td>\n",
              "      <td>0.003364</td>\n",
              "      <td>0.010553</td>\n",
              "      <td>2.325539</td>\n",
              "      <td>3.137241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505961</th>\n",
              "      <td>rotation</td>\n",
              "      <td>0.060847</td>\n",
              "      <td>0.011429</td>\n",
              "      <td>0.016863</td>\n",
              "      <td>0.001754</td>\n",
              "      <td>0.019412</td>\n",
              "      <td>0.044931</td>\n",
              "      <td>0.004558</td>\n",
              "      <td>0.008965</td>\n",
              "      <td>2.314672</td>\n",
              "      <td>1.966784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443734</th>\n",
              "      <td>polarization</td>\n",
              "      <td>0.108564</td>\n",
              "      <td>0.033956</td>\n",
              "      <td>0.033040</td>\n",
              "      <td>0.004249</td>\n",
              "      <td>0.043017</td>\n",
              "      <td>0.099392</td>\n",
              "      <td>0.007951</td>\n",
              "      <td>0.026208</td>\n",
              "      <td>2.310547</td>\n",
              "      <td>3.296182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466359</th>\n",
              "      <td>pseudogap</td>\n",
              "      <td>0.034639</td>\n",
              "      <td>0.008007</td>\n",
              "      <td>0.014164</td>\n",
              "      <td>0.001086</td>\n",
              "      <td>0.011318</td>\n",
              "      <td>0.026030</td>\n",
              "      <td>0.002021</td>\n",
              "      <td>0.008845</td>\n",
              "      <td>2.299921</td>\n",
              "      <td>4.376548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>632178</th>\n",
              "      <td>under</td>\n",
              "      <td>0.315665</td>\n",
              "      <td>0.109155</td>\n",
              "      <td>0.033577</td>\n",
              "      <td>0.007042</td>\n",
              "      <td>0.125265</td>\n",
              "      <td>0.288056</td>\n",
              "      <td>0.009706</td>\n",
              "      <td>0.020552</td>\n",
              "      <td>2.299562</td>\n",
              "      <td>2.117483</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 term  max_monthly_abstract_df  \\\n",
              "339710               machine learning                 0.041916   \n",
              "615098          topological insulator                 0.040268   \n",
              "256046                       graphene                 0.096935   \n",
              "345696                       majorana                 0.030201   \n",
              "615605              topological phase                 0.037675   \n",
              "658894                 weyl semimetal                 0.024496   \n",
              "371092                  moir emission                 0.021778   \n",
              "371090                           moir                 0.024600   \n",
              "626853                twisted bilayer                 0.019446   \n",
              "54187                bilayer graphene                 0.022668   \n",
              "575426             superconducting mg                 0.019298   \n",
              "361014                            mgb                 0.042735   \n",
              "442295                      pnictides                 0.027563   \n",
              "324952                       learning                 0.055689   \n",
              "658808                           weyl                 0.031700   \n",
              "379762                    nanoribbons                 0.017774   \n",
              "515903                      semimetal                 0.037464   \n",
              "306718                 iron pnictides                 0.021401   \n",
              "339696                        machine                 0.046707   \n",
              "391508                non hermiticity                 0.028609   \n",
              "188572                    engineering                 0.068803   \n",
              "386428                 neural network                 0.036091   \n",
              "534311                       skyrmion                 0.023128   \n",
              "408401                optical lattice                 0.031522   \n",
              "547455               spin or coupling                 0.042289   \n",
              "268110                    hermiticity                 0.031860   \n",
              "386412                         neural                 0.037757   \n",
              "625157                        tunable                 0.047347   \n",
              "306641                           iron                 0.036145   \n",
              "154695  discrete wave superconducting                 0.025878   \n",
              "360895                             mg                 0.057971   \n",
              "346250                     manganites                 0.018600   \n",
              "81229                         circuit                 0.042541   \n",
              "139196                         design                 0.092983   \n",
              "410846                    or coupling                 0.045157   \n",
              "151757                          dirac                 0.055019   \n",
              "652157                          waals                 0.034175   \n",
              "137514                   derive waals                 0.034175   \n",
              "644292               van derive waals                 0.034175   \n",
              "379842                      nanoscale                 0.046896   \n",
              "644288                     van derive                 0.034700   \n",
              "182792                      emergence                 0.139760   \n",
              "619876               transition metal                 0.042784   \n",
              "473998                          qubit                 0.039958   \n",
              "373238                      monolayer                 0.052239   \n",
              "626819                        twisted                 0.037515   \n",
              "380666                      nanowires                 0.025758   \n",
              "143097                        devices                 0.125499   \n",
              "631137                      ultracold                 0.046102   \n",
              "642907                         valley                 0.026018   \n",
              "190204                   entanglement                 0.050907   \n",
              "604101                 thermoelectric                 0.023105   \n",
              "154652                  discrete wave                 0.038817   \n",
              "441143                        plasmon                 0.032258   \n",
              "379467                  nanoparticles                 0.024006   \n",
              "625472                         tuning                 0.077658   \n",
              "144974                        diamond                 0.029703   \n",
              "54120                         bilayer                 0.044785   \n",
              "389618                          nodal                 0.020394   \n",
              "348760                      materials                 0.267216   \n",
              "472483                   quantum spin                 0.026618   \n",
              "69268                carbon nanotubes                 0.034005   \n",
              "227064               first principles                 0.065934   \n",
              "351057                         matter                 0.079905   \n",
              "275978                         hybrid                 0.057092   \n",
              "614669                    topological                 0.177503   \n",
              "218316                           fets                 0.061269   \n",
              "175326                      efficient                 0.106596   \n",
              "345033                         magnon                 0.030473   \n",
              "547438                        spin or                 0.071682   \n",
              "526691                        silicon                 0.031544   \n",
              "560774                         strain                 0.059758   \n",
              "127706                           deep                 0.036269   \n",
              "437228                         photon                 0.059723   \n",
              "268468               heterostructures                 0.043431   \n",
              "388144                             ni                 0.025554   \n",
              "72312                          cavity                 0.030201   \n",
              "177874                    electrolyte                 0.057160   \n",
              "346467                      many body                 0.074389   \n",
              "380444                      nanotubes                 0.035264   \n",
              "430311                     perovskite                 0.022870   \n",
              "501265                         reveal                 0.165431   \n",
              "587499                             tc                 0.053140   \n",
              "317335                        laminar                 0.035120   \n",
              "580706                      switching                 0.042338   \n",
              "107188                        control                 0.138257   \n",
              "192002                      epitaxial                 0.026241   \n",
              "419231                          oxide                 0.045332   \n",
              "546688                     spin glass                 0.027883   \n",
              "543664                   spectroscopy                 0.081004   \n",
              "181486             electron transport                 0.021916   \n",
              "380195                 nanostructures                 0.031642   \n",
              "88691                         comment                 0.024730   \n",
              "663208                          wires                 0.037936   \n",
              "653071                          water                 0.031274   \n",
              "328212                          light                 0.085609   \n",
              "4123                 absolute initial                 0.036262   \n",
              "255925                       granular                 0.036626   \n",
              "79744                        chemical                 0.071887   \n",
              "132642               density function                 0.075169   \n",
              "347820                         marked                 0.031405   \n",
              "456400                          probe                 0.068109   \n",
              "276560                       hydrogen                 0.023555   \n",
              "244886                           gaas                 0.028935   \n",
              "618573                    transistors                 0.024174   \n",
              "570765                      substrate                 0.057540   \n",
              "316939                         ladder                 0.022857   \n",
              "475975                          raman                 0.026914   \n",
              "7600                           active                 0.080588   \n",
              "247712                           gate                 0.041958   \n",
              "260597                    hall effect                 0.037194   \n",
              "362254                      microwave                 0.039673   \n",
              "655054           wave superconducting                 0.033272   \n",
              "627739       two dimensional electron                 0.026087   \n",
              "537024                           soft                 0.044118   \n",
              "384249                        nematic                 0.023529   \n",
              "472203                  quantum phase                 0.030635   \n",
              "472235       quantum phase transition                 0.026744   \n",
              "667036                          x ray                 0.049743   \n",
              "72922                            cell                 0.061501   \n",
              "428436                    percolation                 0.026616   \n",
              "149406           dimensional electron                 0.028261   \n",
              "80379                          chiral                 0.059659   \n",
              "76908                         chaotic                 0.034351   \n",
              "56646                            body                 0.091392   \n",
              "651010                         vortex                 0.061192   \n",
              "472985                   quantum well                 0.028743   \n",
              "279432                        imaging                 0.051494   \n",
              "598                                2d                 0.078843   \n",
              "455347                     principles                 0.094849   \n",
              "140396                      detection                 0.062090   \n",
              "122516                       cuprates                 0.054386   \n",
              "408057                        optical                 0.120430   \n",
              "364641                       mobility                 0.039695   \n",
              "290249                    information                 0.083286   \n",
              "480101                            ray                 0.052601   \n",
              "127878                        defects                 0.061632   \n",
              "11606                           aging                 0.032298   \n",
              "165022                         driven                 0.121597   \n",
              "648223                            via                 0.108721   \n",
              "171437                           edge                 0.076555   \n",
              "179530                  electron gain                 0.023715   \n",
              "188896                       enhanced                 0.111307   \n",
              "357692                     mesoscopic                 0.035524   \n",
              "69167                          carbon                 0.045340   \n",
              "247509                          gases                 0.030492   \n",
              "346410                           many                 0.193319   \n",
              "562728                         stripe                 0.035088   \n",
              "537376                          solid                 0.072604   \n",
              "470795               quantum critical                 0.026627   \n",
              "550408                             sr                 0.042975   \n",
              "78721                  charge density                 0.031634   \n",
              "145284                     dielectric                 0.041434   \n",
              "148560                         dilute                 0.034139   \n",
              "217143                  ferroelectric                 0.030227   \n",
              "607275                     thin films                 0.046312   \n",
              "63797                            bulk                 0.100142   \n",
              "91070                         complex                 0.116366   \n",
              "438035                       physical                 0.173792   \n",
              "180988             electron structure                 0.051795   \n",
              "505039                           role                 0.094049   \n",
              "481071                           real                 0.173568   \n",
              "316132                          kondo                 0.029954   \n",
              "40439                            band                 0.159506   \n",
              "58102        bose einstein condensate                 0.052503   \n",
              "176483            einstein condensate                 0.052503   \n",
              "525634                     signatures                 0.063320   \n",
              "58101                   bose einstein                 0.054945   \n",
              "319821                          laser                 0.043665   \n",
              "322601                          layer                 0.134362   \n",
              "576646                     superfluid                 0.047149   \n",
              "133773                   density wave                 0.032907   \n",
              "298431                      interface                 0.078086   \n",
              "294150           insulator transition                 0.025692   \n",
              "92372                     composition                 0.070736   \n",
              "408987                        optimal                 0.080683   \n",
              "116991                       covalent                 0.043787   \n",
              "464332                        protein                 0.023839   \n",
              "370482                     modulation                 0.066090   \n",
              "61926                        breaking                 0.080511   \n",
              "361494                    microscopic                 0.106979   \n",
              "532667                         sitter                 0.030871   \n",
              "165951                       droplets                 0.030814   \n",
              "356451                       mediated                 0.042061   \n",
              "377146                          multi                 0.169994   \n",
              "248111                          gauge                 0.030217   \n",
              "607207                           thin                 0.072041   \n",
              "515231                  semiconductor                 0.067974   \n",
              "609566                        through                 0.174323   \n",
              "391142                non equilibrium                 0.036417   \n",
              "42449                           based                 0.267487   \n",
              "67903                          cancer                 0.028099   \n",
              "376283                           mott                 0.031250   \n",
              "58080                            bose                 0.072039   \n",
              "426662                       patterns                 0.055882   \n",
              "87577                       colloidal                 0.027428   \n",
              "505961                       rotation                 0.060847   \n",
              "443734                   polarization                 0.108564   \n",
              "466359                      pseudogap                 0.034639   \n",
              "632178                          under                 0.315665   \n",
              "\n",
              "        min_monthly_abstract_df  max_monthly_title_df  min_monthly_title_df  \\\n",
              "339710                 0.000672              0.018878              0.000672   \n",
              "615098                 0.000986              0.026116              0.000914   \n",
              "256046                 0.001109              0.085336              0.001078   \n",
              "345696                 0.000914              0.014824              0.000914   \n",
              "615605                 0.000955              0.013383              0.000842   \n",
              "658894                 0.000773              0.016571              0.000773   \n",
              "371092                 0.000674              0.010821              0.000637   \n",
              "371090                 0.000674              0.012731              0.000637   \n",
              "626853                 0.000612              0.010863              0.000612   \n",
              "54187                  0.001057              0.015547              0.001057   \n",
              "575426                 0.000505              0.025765              0.000505   \n",
              "361014                 0.000505              0.031339              0.000505   \n",
              "442295                 0.001049              0.022051              0.000505   \n",
              "324952                 0.000823              0.032204              0.000673   \n",
              "658808                 0.000814              0.024496              0.000814   \n",
              "379762                 0.001049              0.015018              0.000589   \n",
              "515903                 0.000895              0.026756              0.000806   \n",
              "306718                 0.000519              0.012128              0.000505   \n",
              "339696                 0.000672              0.021099              0.000672   \n",
              "391508                 0.000783              0.018205              0.000654   \n",
              "188572                 0.001361              0.017964              0.001030   \n",
              "386428                 0.000693              0.017293              0.000649   \n",
              "534311                 0.000818              0.014607              0.000789   \n",
              "408401                 0.001416              0.021739              0.000525   \n",
              "547455                 0.001399              0.019231              0.000894   \n",
              "268110                 0.000797              0.018205              0.000654   \n",
              "386412                 0.000814              0.018550              0.000649   \n",
              "625157                 0.001425              0.017424              0.001104   \n",
              "306641                 0.001110              0.023346              0.001049   \n",
              "154695                 0.000580              0.016636              0.000505   \n",
              "360895                 0.000823              0.054750              0.000505   \n",
              "346250                 0.000540              0.016058              0.000505   \n",
              "81229                  0.001399              0.014043              0.000693   \n",
              "139196                 0.003697              0.015901              0.000773   \n",
              "410846                 0.001429              0.019231              0.000894   \n",
              "151757                 0.001167              0.021665              0.001104   \n",
              "652157                 0.001045              0.017153              0.000806   \n",
              "137514                 0.001045              0.017153              0.000806   \n",
              "644292                 0.001045              0.017153              0.000806   \n",
              "379842                 0.001842              0.019986              0.001506   \n",
              "644288                 0.001045              0.017153              0.000806   \n",
              "182792                 0.011189              0.019668              0.001049   \n",
              "619876                 0.001642              0.015729              0.000962   \n",
              "473998                 0.003396              0.020882              0.001399   \n",
              "373238                 0.001453              0.024902              0.001057   \n",
              "626819                 0.001131              0.019065              0.000826   \n",
              "380666                 0.001307              0.021212              0.001307   \n",
              "143097                 0.008392              0.013622              0.001160   \n",
              "631137                 0.001815              0.020386              0.001381   \n",
              "642907                 0.001221              0.012099              0.000883   \n",
              "190204                 0.003515              0.026042              0.001425   \n",
              "604101                 0.001152              0.015162              0.001045   \n",
              "154652                 0.002075              0.020333              0.000540   \n",
              "441143                 0.001307              0.021505              0.001057   \n",
              "379467                 0.001221              0.014254              0.001263   \n",
              "625472                 0.003750              0.013046              0.000986   \n",
              "144974                 0.001250              0.022631              0.001081   \n",
              "54120                  0.003396              0.026741              0.001908   \n",
              "389618                 0.001072              0.011955              0.000671   \n",
              "348760                 0.036398              0.043250              0.003221   \n",
              "472483                 0.001422              0.013734              0.000904   \n",
              "69268                  0.001076              0.030435              0.000589   \n",
              "227064                 0.005282              0.018727              0.001250   \n",
              "351057                 0.007728              0.019763              0.001416   \n",
              "275978                 0.005059              0.016977              0.001112   \n",
              "614669                 0.024648              0.076510              0.003326   \n",
              "218316                 0.002762              0.026042              0.001328   \n",
              "175326                 0.010638              0.020043              0.001250   \n",
              "345033                 0.001112              0.015547              0.000971   \n",
              "547438                 0.008772              0.028986              0.001776   \n",
              "526691                 0.002660              0.017179              0.001110   \n",
              "560774                 0.005272              0.020915              0.001221   \n",
              "127706                 0.002721              0.012079              0.000665   \n",
              "437228                 0.003630              0.023256              0.001761   \n",
              "268468                 0.002646              0.018458              0.000986   \n",
              "388144                 0.001916              0.012360              0.001012   \n",
              "72312                  0.001425              0.014765              0.001181   \n",
              "177874                 0.007663              0.016340              0.001190   \n",
              "346467                 0.008489              0.020476              0.001078   \n",
              "380444                 0.001766              0.033696              0.000589   \n",
              "430311                 0.001399              0.014294              0.000848   \n",
              "501265                 0.016575              0.012451              0.000763   \n",
              "587499                 0.005070              0.016717              0.000540   \n",
              "317335                 0.002926              0.023593              0.001256   \n",
              "580706                 0.003040              0.016230              0.001160   \n",
              "107188                 0.022099              0.031274              0.001488   \n",
              "192002                 0.001739              0.014065              0.001205   \n",
              "419231                 0.003810              0.021181              0.001610   \n",
              "546688                 0.001815              0.021127              0.000609   \n",
              "543664                 0.011189              0.028470              0.003284   \n",
              "181486                 0.001848              0.012146              0.000642   \n",
              "380195                 0.001976              0.016766              0.001235   \n",
              "88691                  0.001078              0.026408              0.000702   \n",
              "663208                 0.003901              0.019676              0.001063   \n",
              "653071                 0.001520              0.013730              0.000856   \n",
              "328212                 0.012704              0.016922              0.001073   \n",
              "4123                   0.004144              0.012418              0.001908   \n",
              "255925                 0.003534              0.033296              0.002282   \n",
              "79744                  0.008489              0.012563              0.001081   \n",
              "132642                 0.012802              0.016094              0.001057   \n",
              "347820                 0.002523              0.018182              0.000505   \n",
              "456400                 0.007634              0.022539              0.001416   \n",
              "276560                 0.001399              0.013709              0.001096   \n",
              "244886                 0.003025              0.018519              0.000526   \n",
              "618573                 0.001761              0.016291              0.001182   \n",
              "570765                 0.006957              0.012649              0.001190   \n",
              "316939                 0.000838              0.017391              0.000601   \n",
              "475975                 0.003284              0.014461              0.000582   \n",
              "7600                   0.007105              0.028522              0.001122   \n",
              "247712                 0.003521              0.015371              0.001116   \n",
              "260597                 0.005263              0.019027              0.001698   \n",
              "362254                 0.005690              0.016107              0.001610   \n",
              "655054                 0.002413              0.018484              0.000548   \n",
              "627739                 0.001952              0.015385              0.000522   \n",
              "537024                 0.003521              0.013866              0.001333   \n",
              "384249                 0.001916              0.013529              0.001104   \n",
              "472203                 0.003683              0.014778              0.001295   \n",
              "472235                 0.001842              0.011803              0.000710   \n",
              "667036                 0.007394              0.016774              0.002361   \n",
              "72922                  0.005272              0.015280              0.001072   \n",
              "428436                 0.003630              0.016345              0.001210   \n",
              "149406                 0.001952              0.015385              0.000570   \n",
              "80379                  0.006568              0.023810              0.001195   \n",
              "76908                  0.003610              0.018149              0.000755   \n",
              "56646                  0.018519              0.026420              0.001323   \n",
              "651010                 0.009422              0.035427              0.002706   \n",
              "472985                 0.003819              0.014563              0.000596   \n",
              "279432                 0.008499              0.014249              0.001399   \n",
              "598                    0.016478              0.026718              0.003617   \n",
              "455347                 0.017713              0.021352              0.003306   \n",
              "140396                 0.009208              0.015584              0.001399   \n",
              "122516                 0.006653              0.031480              0.001331   \n",
              "408057                 0.026643              0.048587              0.005272   \n",
              "364641                 0.005333              0.013803              0.000898   \n",
              "290249                 0.013986              0.013942              0.000932   \n",
              "480101                 0.009242              0.016774              0.002431   \n",
              "127878                 0.009242              0.020025              0.001905   \n",
              "11606                  0.002232              0.014286              0.000580   \n",
              "165022                 0.023256              0.036442              0.005282   \n",
              "648223                 0.023776              0.027371              0.001290   \n",
              "171437                 0.011091              0.021850              0.001049   \n",
              "179530                 0.002941              0.016327              0.000509   \n",
              "188896                 0.020333              0.024825              0.001399   \n",
              "357692                 0.004624              0.021429              0.000570   \n",
              "69167                  0.001908              0.032609              0.002221   \n",
              "247509                 0.001517              0.021907              0.001416   \n",
              "346410                 0.047538              0.026298              0.003373   \n",
              "562728                 0.005690              0.022857              0.000583   \n",
              "537376                 0.019324              0.024194              0.003233   \n",
              "470795                 0.003521              0.016765              0.001152   \n",
              "550408                 0.004677              0.024096              0.000756   \n",
              "78721                  0.005442              0.012288              0.000829   \n",
              "145284                 0.006568              0.015810              0.001152   \n",
              "148560                 0.008028              0.020236              0.001010   \n",
              "217143                 0.003683              0.016332              0.001125   \n",
              "607275                 0.009217              0.024701              0.001686   \n",
              "63797                  0.020279              0.015587              0.001488   \n",
              "91070                  0.025806              0.023307              0.003364   \n",
              "438035                 0.053824              0.017787              0.001416   \n",
              "180988                 0.003831              0.017143              0.001892   \n",
              "505039                 0.023140              0.021090              0.001908   \n",
              "481071                 0.047934              0.018939              0.001837   \n",
              "316132                 0.005229              0.021802              0.001744   \n",
              "40439                  0.048750              0.030056              0.004608   \n",
              "58102                  0.010172              0.044362              0.003778   \n",
              "176483                 0.010172              0.044362              0.003778   \n",
              "525634                 0.015267              0.013350              0.001190   \n",
              "58101                  0.011042              0.044362              0.003778   \n",
              "319821                 0.006791              0.015748              0.001520   \n",
              "322601                 0.030560              0.031746              0.004854   \n",
              "576646                 0.009881              0.026316              0.001976   \n",
              "133773                 0.003738              0.012552              0.001460   \n",
              "298431                 0.019324              0.025819              0.003610   \n",
              "294150                 0.003155              0.013554              0.000621   \n",
              "92372                  0.013410              0.015094              0.000895   \n",
              "408987                 0.017831              0.017800              0.001761   \n",
              "116991                 0.006957              0.018240              0.001399   \n",
              "464332                 0.002232              0.014519              0.000631   \n",
              "370482                 0.010542              0.014967              0.001754   \n",
              "61926                  0.017956              0.018205              0.001221   \n",
              "361494                 0.024818              0.018301              0.002361   \n",
              "532667                 0.006211              0.019767              0.001120   \n",
              "165951                 0.004908              0.012433              0.000712   \n",
              "356451                 0.007968              0.021277              0.001227   \n",
              "377146                 0.055453              0.045064              0.007905   \n",
              "248111                 0.005312              0.012324              0.000842   \n",
              "607207                 0.018416              0.031414              0.005059   \n",
              "515231                 0.018895              0.030624              0.005525   \n",
              "609566                 0.049236              0.026265              0.004608   \n",
              "391142                 0.006452              0.016279              0.001049   \n",
              "42449                  0.086667              0.043530              0.003641   \n",
              "67903                  0.002013              0.014876              0.000616   \n",
              "376283                 0.004559              0.016373              0.000582   \n",
              "58080                  0.017943              0.064336              0.009679   \n",
              "426662                 0.011858              0.014239              0.001976   \n",
              "87577                  0.003306              0.016781              0.001848   \n",
              "505961                 0.011429              0.016863              0.001754   \n",
              "443734                 0.033956              0.033040              0.004249   \n",
              "466359                 0.008007              0.014164              0.001086   \n",
              "632178                 0.109155              0.033577              0.007042   \n",
              "\n",
              "        min_semesterly_abstract_df  max_semesterly_abstract_df  \\\n",
              "339710                    0.000140                    0.036852   \n",
              "615098                    0.000191                    0.034364   \n",
              "256046                    0.000696                    0.082962   \n",
              "345696                    0.000211                    0.023190   \n",
              "615605                    0.000272                    0.026414   \n",
              "658894                    0.000147                    0.013867   \n",
              "371092                    0.000167                    0.015111   \n",
              "371090                    0.000198                    0.017130   \n",
              "626853                    0.000147                    0.012550   \n",
              "54187                     0.000191                    0.016109   \n",
              "575426                    0.000100                    0.008168   \n",
              "361014                    0.000299                    0.020692   \n",
              "442295                    0.000306                    0.020708   \n",
              "324952                    0.000793                    0.052870   \n",
              "658808                    0.000335                    0.022274   \n",
              "379762                    0.000191                    0.010757   \n",
              "515903                    0.000545                    0.030730   \n",
              "306718                    0.000186                    0.009994   \n",
              "339696                    0.000808                    0.042130   \n",
              "391508                    0.000397                    0.019414   \n",
              "188572                    0.001769                    0.054238   \n",
              "386428                    0.000978                    0.027870   \n",
              "534311                    0.000598                    0.017031   \n",
              "408401                    0.000917                    0.024160   \n",
              "547455                    0.001634                    0.036721   \n",
              "268110                    0.000991                    0.021615   \n",
              "386412                    0.001388                    0.030000   \n",
              "625157                    0.001906                    0.039828   \n",
              "306641                    0.001517                    0.029246   \n",
              "154695                    0.000862                    0.015902   \n",
              "360895                    0.002141                    0.038116   \n",
              "346250                    0.000693                    0.011009   \n",
              "81229                     0.002527                    0.034537   \n",
              "139196                    0.006422                    0.084907   \n",
              "410846                    0.002995                    0.038393   \n",
              "151757                    0.003674                    0.046899   \n",
              "652157                    0.002089                    0.026218   \n",
              "137514                    0.002089                    0.026118   \n",
              "644292                    0.002089                    0.026018   \n",
              "379842                    0.003364                    0.041116   \n",
              "644288                    0.002321                    0.026719   \n",
              "182792                    0.012252                    0.127593   \n",
              "619876                    0.003364                    0.034935   \n",
              "473998                    0.003364                    0.033148   \n",
              "373238                    0.004084                    0.040145   \n",
              "626819                    0.002974                    0.028354   \n",
              "380666                    0.002178                    0.020036   \n",
              "143097                    0.011927                    0.107427   \n",
              "631137                    0.003943                    0.033836   \n",
              "642907                    0.002321                    0.019265   \n",
              "190204                    0.005199                    0.042029   \n",
              "604101                    0.002115                    0.016814   \n",
              "154652                    0.003932                    0.028440   \n",
              "441143                    0.002785                    0.019723   \n",
              "379467                    0.002307                    0.016308   \n",
              "625472                    0.009174                    0.064352   \n",
              "144974                    0.002178                    0.014828   \n",
              "54120                     0.005810                    0.037500   \n",
              "389618                    0.002274                    0.014311   \n",
              "348760                    0.041590                    0.250075   \n",
              "472483                    0.003538                    0.020658   \n",
              "69268                     0.003747                    0.021747   \n",
              "227064                    0.009099                    0.052678   \n",
              "351057                    0.012383                    0.071190   \n",
              "275978                    0.008440                    0.047778   \n",
              "614669                    0.029315                    0.163114   \n",
              "218316                    0.008955                    0.049565   \n",
              "175326                    0.017943                    0.098056   \n",
              "345033                    0.003969                    0.021674   \n",
              "547438                    0.012524                    0.063353   \n",
              "526691                    0.004281                    0.021533   \n",
              "560774                    0.009480                    0.047024   \n",
              "127706                    0.006422                    0.030300   \n",
              "437228                    0.011222                    0.051939   \n",
              "268468                    0.006673                    0.030833   \n",
              "388144                    0.003976                    0.018241   \n",
              "72312                     0.005173                    0.023441   \n",
              "177874                    0.011009                    0.048984   \n",
              "346467                    0.013613                    0.060242   \n",
              "380444                    0.005743                    0.024907   \n",
              "430311                    0.003589                    0.015547   \n",
              "501265                    0.032454                    0.140278   \n",
              "587499                    0.008429                    0.035938   \n",
              "317335                    0.005165                    0.022018   \n",
              "580706                    0.008563                    0.035241   \n",
              "107188                    0.030275                    0.124537   \n",
              "192002                    0.004853                    0.019949   \n",
              "419231                    0.009402                    0.038145   \n",
              "546688                    0.004456                    0.017910   \n",
              "543664                    0.018200                    0.073148   \n",
              "181486                    0.003670                    0.014668   \n",
              "380195                    0.005990                    0.023560   \n",
              "88691                     0.003772                    0.014559   \n",
              "663208                    0.006574                    0.024654   \n",
              "653071                    0.006065                    0.022630   \n",
              "328212                    0.019726                    0.073333   \n",
              "4123                      0.007951                    0.029259   \n",
              "255925                    0.006004                    0.021909   \n",
              "79744                     0.017125                    0.062444   \n",
              "132642                    0.016820                    0.061194   \n",
              "347820                    0.005868                    0.021236   \n",
              "456400                    0.016013                    0.057540   \n",
              "276560                    0.005307                    0.018418   \n",
              "244886                    0.005926                    0.020446   \n",
              "618573                    0.004587                    0.015796   \n",
              "570765                    0.012844                    0.044035   \n",
              "316939                    0.004870                    0.016682   \n",
              "475975                    0.006673                    0.022482   \n",
              "7600                      0.020251                    0.068152   \n",
              "247712                    0.010009                    0.033620   \n",
              "260597                    0.008203                    0.027310   \n",
              "362254                    0.008984                    0.029873   \n",
              "655054                    0.005901                    0.019572   \n",
              "627739                    0.005242                    0.017370   \n",
              "537024                    0.010312                    0.033985   \n",
              "384249                    0.004956                    0.016086   \n",
              "472203                    0.007178                    0.023123   \n",
              "472235                    0.005896                    0.018982   \n",
              "667036                    0.013042                    0.041738   \n",
              "72922                     0.016208                    0.051636   \n",
              "428436                    0.006520                    0.020764   \n",
              "149406                    0.006351                    0.020183   \n",
              "80379                     0.016071                    0.050734   \n",
              "76908                     0.006259                    0.019715   \n",
              "56646                     0.024503                    0.076762   \n",
              "651010                    0.014259                    0.044650   \n",
              "472985                    0.005434                    0.016997   \n",
              "279432                    0.014099                    0.044064   \n",
              "598                       0.022191                    0.068354   \n",
              "455347                    0.028135                    0.086642   \n",
              "140396                    0.017437                    0.053467   \n",
              "122516                    0.012082                    0.037003   \n",
              "408057                    0.035780                    0.109176   \n",
              "364641                    0.010009                    0.030437   \n",
              "290249                    0.023250                    0.070556   \n",
              "480101                    0.014067                    0.042645   \n",
              "127878                    0.017288                    0.052315   \n",
              "11606                     0.006285                    0.018973   \n",
              "165022                    0.035128                    0.105823   \n",
              "648223                    0.032757                    0.098202   \n",
              "171437                    0.020795                    0.062103   \n",
              "179530                    0.005556                    0.016514   \n",
              "188896                    0.032454                    0.095833   \n",
              "357692                    0.007212                    0.020764   \n",
              "69167                     0.010346                    0.029740   \n",
              "247509                    0.007339                    0.020882   \n",
              "346410                    0.061774                    0.175323   \n",
              "562728                    0.009211                    0.025994   \n",
              "537376                    0.023142                    0.063291   \n",
              "470795                    0.006534                    0.017774   \n",
              "550408                    0.009450                    0.025634   \n",
              "78721                     0.008286                    0.022416   \n",
              "145284                    0.010616                    0.028595   \n",
              "148560                    0.009823                    0.026224   \n",
              "217143                    0.007582                    0.020214   \n",
              "607275                    0.013150                    0.034935   \n",
              "63797                     0.032150                    0.085089   \n",
              "91070                     0.036700                    0.096852   \n",
              "438035                    0.062891                    0.164907   \n",
              "180988                    0.013761                    0.035833   \n",
              "505039                    0.031193                    0.081204   \n",
              "481071                    0.059894                    0.155342   \n",
              "316132                    0.008148                    0.021019   \n",
              "40439                     0.056112                    0.144725   \n",
              "58102                     0.013519                    0.034747   \n",
              "176483                    0.013519                    0.034747   \n",
              "525634                    0.021535                    0.054732   \n",
              "58101                     0.014259                    0.036240   \n",
              "319821                    0.013843                    0.035179   \n",
              "322601                    0.045193                    0.114639   \n",
              "576646                    0.013761                    0.034554   \n",
              "133773                    0.010009                    0.025117   \n",
              "298431                    0.026911                    0.067528   \n",
              "294150                    0.006204                    0.015469   \n",
              "92372                     0.023242                    0.057840   \n",
              "408987                    0.026147                    0.064759   \n",
              "116991                    0.014067                    0.034593   \n",
              "464332                    0.005920                    0.014496   \n",
              "370482                    0.022239                    0.054236   \n",
              "61926                     0.029705                    0.071944   \n",
              "361494                    0.037402                    0.090000   \n",
              "532667                    0.009786                    0.023531   \n",
              "165951                    0.009712                    0.023328   \n",
              "356451                    0.014266                    0.034224   \n",
              "377146                    0.061268                    0.146944   \n",
              "248111                    0.010105                    0.024167   \n",
              "607207                    0.025174                    0.059942   \n",
              "515231                    0.024503                    0.057881   \n",
              "609566                    0.065454                    0.154352   \n",
              "391142                    0.011927                    0.028080   \n",
              "42449                     0.102856                    0.242103   \n",
              "67903                     0.005600                    0.013150   \n",
              "376283                    0.010361                    0.024317   \n",
              "58080                     0.023241                    0.054487   \n",
              "426662                    0.021101                    0.049335   \n",
              "87577                     0.007886                    0.018339   \n",
              "505961                    0.019412                    0.044931   \n",
              "443734                    0.043017                    0.099392   \n",
              "466359                    0.011318                    0.026030   \n",
              "632178                    0.125265                    0.288056   \n",
              "\n",
              "        min_semesterly_title_df  max_semesterly_title_df  \\\n",
              "339710                 0.000140                 0.015741   \n",
              "615098                 0.000191                 0.017812   \n",
              "256046                 0.000230                 0.073019   \n",
              "345696                 0.000198                 0.009861   \n",
              "615605                 0.000199                 0.008955   \n",
              "658894                 0.000147                 0.009541   \n",
              "371092                 0.000140                 0.007605   \n",
              "371090                 0.000140                 0.008889   \n",
              "626853                 0.000147                 0.007774   \n",
              "54187                  0.000191                 0.010022   \n",
              "575426                 0.000094                 0.009801   \n",
              "361014                 0.000099                 0.017152   \n",
              "442295                 0.000187                 0.015077   \n",
              "324952                 0.000198                 0.027222   \n",
              "658808                 0.000167                 0.014302   \n",
              "379762                 0.000191                 0.007958   \n",
              "515903                 0.000213                 0.019192   \n",
              "306718                 0.000094                 0.007993   \n",
              "339696                 0.000186                 0.018333   \n",
              "391508                 0.000198                 0.011808   \n",
              "188572                 0.000758                 0.012745   \n",
              "386428                 0.000419                 0.011949   \n",
              "534311                 0.000199                 0.010866   \n",
              "408401                 0.000612                 0.012534   \n",
              "547455                 0.000545                 0.011884   \n",
              "268110                 0.000198                 0.012008   \n",
              "386412                 0.000668                 0.012962   \n",
              "625157                 0.000917                 0.012774   \n",
              "306641                 0.000612                 0.017439   \n",
              "154695                 0.000279                 0.009174   \n",
              "360895                 0.000607                 0.032399   \n",
              "346250                 0.000296                 0.008819   \n",
              "81229                  0.000918                 0.009630   \n",
              "139196                 0.000306                 0.009407   \n",
              "410846                 0.000545                 0.012018   \n",
              "151757                 0.000696                 0.017014   \n",
              "652157                 0.000513                 0.011848   \n",
              "137514                 0.000513                 0.011848   \n",
              "644292                 0.000513                 0.011848   \n",
              "379842                 0.001835                 0.013066   \n",
              "644288                 0.000513                 0.011949   \n",
              "182792                 0.001089                 0.014630   \n",
              "619876                 0.001223                 0.011229   \n",
              "473998                 0.001223                 0.013339   \n",
              "373238                 0.000817                 0.016648   \n",
              "626819                 0.000396                 0.013559   \n",
              "380666                 0.001517                 0.013552   \n",
              "143097                 0.001529                 0.009609   \n",
              "631137                 0.001516                 0.012855   \n",
              "642907                 0.000253                 0.007525   \n",
              "190204                 0.002752                 0.017492   \n",
              "604101                 0.000306                 0.008837   \n",
              "154652                 0.000495                 0.011621   \n",
              "441143                 0.000306                 0.011303   \n",
              "379467                 0.001837                 0.008617   \n",
              "625472                 0.000910                 0.008755   \n",
              "144974                 0.000769                 0.008313   \n",
              "54120                  0.002141                 0.018426   \n",
              "389618                 0.000256                 0.007766   \n",
              "348760                 0.005560                 0.032082   \n",
              "472483                 0.001387                 0.007872   \n",
              "69268                  0.001782                 0.015547   \n",
              "227064                 0.002723                 0.013143   \n",
              "351057                 0.003267                 0.011108   \n",
              "275978                 0.001634                 0.012709   \n",
              "614669                 0.007118                 0.067391   \n",
              "218316                 0.003058                 0.017439   \n",
              "175326                 0.001392                 0.011540   \n",
              "345033                 0.000690                 0.010207   \n",
              "547438                 0.003943                 0.020205   \n",
              "526691                 0.002446                 0.010905   \n",
              "560774                 0.002123                 0.015268   \n",
              "127706                 0.000191                 0.008006   \n",
              "437228                 0.002426                 0.015744   \n",
              "268468                 0.001517                 0.012778   \n",
              "388144                 0.002178                 0.008611   \n",
              "72312                  0.001634                 0.009327   \n",
              "177874                 0.003364                 0.009519   \n",
              "346467                 0.002538                 0.015153   \n",
              "380444                 0.002674                 0.019051   \n",
              "430311                 0.001480                 0.008898   \n",
              "501265                 0.000464                 0.007805   \n",
              "587499                 0.000709                 0.009351   \n",
              "317335                 0.001924                 0.011621   \n",
              "580706                 0.001837                 0.008981   \n",
              "107188                 0.004628                 0.023016   \n",
              "192002                 0.001625                 0.008801   \n",
              "419231                 0.002723                 0.012787   \n",
              "546688                 0.001188                 0.011829   \n",
              "543664                 0.006665                 0.019424   \n",
              "181486                 0.001667                 0.007593   \n",
              "380195                 0.002123                 0.010303   \n",
              "88691                  0.002001                 0.017405   \n",
              "663208                 0.001296                 0.012400   \n",
              "653071                 0.000758                 0.007593   \n",
              "328212                 0.003481                 0.011782   \n",
              "4123                   0.002526                 0.008516   \n",
              "255925                 0.003703                 0.016575   \n",
              "79744                  0.001213                 0.008120   \n",
              "132642                 0.003539                 0.009647   \n",
              "347820                 0.000099                 0.008440   \n",
              "456400                 0.003538                 0.015441   \n",
              "276560                 0.001905                 0.008417   \n",
              "244886                 0.001389                 0.008921   \n",
              "618573                 0.002141                 0.008842   \n",
              "570765                 0.000917                 0.007515   \n",
              "316939                 0.001847                 0.009706   \n",
              "475975                 0.002730                 0.007612   \n",
              "7600                   0.001769                 0.021574   \n",
              "247712                 0.000303                 0.009205   \n",
              "260597                 0.004557                 0.013026   \n",
              "362254                 0.002563                 0.009312   \n",
              "655054                 0.001101                 0.011315   \n",
              "627739                 0.000907                 0.010019   \n",
              "537024                 0.003332                 0.009583   \n",
              "384249                 0.001529                 0.009319   \n",
              "472203                 0.003332                 0.009152   \n",
              "472235                 0.002307                 0.008031   \n",
              "667036                 0.003670                 0.009082   \n",
              "72922                  0.002392                 0.009707   \n",
              "428436                 0.002173                 0.009485   \n",
              "149406                 0.001311                 0.010659   \n",
              "80379                  0.003267                 0.019314   \n",
              "76908                  0.003145                 0.012232   \n",
              "56646                  0.003383                 0.017617   \n",
              "651010                 0.005367                 0.024771   \n",
              "472985                 0.001852                 0.008084   \n",
              "279432                 0.001817                 0.008943   \n",
              "598                    0.005148                 0.016880   \n",
              "455347                 0.005173                 0.015496   \n",
              "140396                 0.002446                 0.011296   \n",
              "122516                 0.003603                 0.021407   \n",
              "408057                 0.013150                 0.034777   \n",
              "364641                 0.002022                 0.007732   \n",
              "290249                 0.001681                 0.008206   \n",
              "480101                 0.003670                 0.009082   \n",
              "127878                 0.004550                 0.015463   \n",
              "11606                  0.001505                 0.007824   \n",
              "165022                 0.010659                 0.027443   \n",
              "648223                 0.002141                 0.022616   \n",
              "171437                 0.003812                 0.015670   \n",
              "179530                 0.001296                 0.008740   \n",
              "188896                 0.005156                 0.017626   \n",
              "357692                 0.001401                 0.012681   \n",
              "69167                  0.005248                 0.018216   \n",
              "247509                 0.004003                 0.013654   \n",
              "346410                 0.007079                 0.021533   \n",
              "562728                 0.001664                 0.014985   \n",
              "537376                 0.006262                 0.014210   \n",
              "470795                 0.002051                 0.010009   \n",
              "550408                 0.003068                 0.012996   \n",
              "78721                  0.001187                 0.007605   \n",
              "145284                 0.002730                 0.007645   \n",
              "148560                 0.002173                 0.013330   \n",
              "217143                 0.003267                 0.011008   \n",
              "607275                 0.004893                 0.017182   \n",
              "63797                  0.002752                 0.010168   \n",
              "91070                  0.005460                 0.016455   \n",
              "438035                 0.005526                 0.011111   \n",
              "180988                 0.004659                 0.011798   \n",
              "505039                 0.005570                 0.011693   \n",
              "481071                 0.003429                 0.010867   \n",
              "316132                 0.003840                 0.014068   \n",
              "40439                  0.007583                 0.024017   \n",
              "58102                  0.006019                 0.032095   \n",
              "176483                 0.006019                 0.032095   \n",
              "525634                 0.002785                 0.008615   \n",
              "58101                  0.006111                 0.033106   \n",
              "319821                 0.003943                 0.009647   \n",
              "322601                 0.009283                 0.022759   \n",
              "576646                 0.006004                 0.015876   \n",
              "133773                 0.002765                 0.009013   \n",
              "298431                 0.008101                 0.020333   \n",
              "294150                 0.001794                 0.008440   \n",
              "92372                  0.002947                 0.008399   \n",
              "408987                 0.003812                 0.013566   \n",
              "116991                 0.002446                 0.011734   \n",
              "464332                 0.001544                 0.008493   \n",
              "370482                 0.003267                 0.009152   \n",
              "61926                  0.004652                 0.010741   \n",
              "361494                 0.005990                 0.012860   \n",
              "532667                 0.004630                 0.010009   \n",
              "165951                 0.001066                 0.008175   \n",
              "356451                 0.005544                 0.011251   \n",
              "377146                 0.017870                 0.034430   \n",
              "248111                 0.001585                 0.007595   \n",
              "607207                 0.008257                 0.022227   \n",
              "515231                 0.011081                 0.020024   \n",
              "609566                 0.006116                 0.017931   \n",
              "391142                 0.002326                 0.007732   \n",
              "42449                  0.006728                 0.032503   \n",
              "67903                  0.001620                 0.007645   \n",
              "376283                 0.003538                 0.010963   \n",
              "58080                  0.011667                 0.046753   \n",
              "426662                 0.003943                 0.008706   \n",
              "87577                  0.003364                 0.010553   \n",
              "505961                 0.004558                 0.008965   \n",
              "443734                 0.007951                 0.026208   \n",
              "466359                 0.002021                 0.008845   \n",
              "632178                 0.009706                 0.020552   \n",
              "\n",
              "        max/min_semesterly_abstract_df  max/min_semesterly_title_df  \n",
              "339710                      263.748704                   112.656481  \n",
              "615098                      180.378007                    93.497163  \n",
              "256046                      119.161522                   317.996438  \n",
              "345696                      109.666883                    49.769939  \n",
              "615605                       97.018449                    44.925006  \n",
              "658894                       94.103617                    64.743288  \n",
              "371092                       90.240368                    54.454118  \n",
              "371090                       86.453241                    63.644444  \n",
              "626853                       85.166620                    52.752459  \n",
              "54187                        84.558209                    52.603072  \n",
              "575426                       81.930302                   104.647972  \n",
              "361014                       69.185589                   173.614484  \n",
              "442295                       67.716621                    80.723342  \n",
              "324952                       66.709190                   137.390556  \n",
              "658808                       66.511529                    85.412687  \n",
              "379762                       56.465812                    41.769231  \n",
              "515903                       56.435600                    90.031396  \n",
              "306718                       53.765313                    85.338420  \n",
              "339696                       52.114352                    98.633333  \n",
              "391508                       48.961073                    59.560893  \n",
              "188572                       30.659948                    16.810018  \n",
              "386428                       28.507407                    28.519156  \n",
              "534311                       28.482160                    54.516299  \n",
              "408401                       26.334242                    20.493188  \n",
              "547455                       22.479162                    21.825144  \n",
              "268110                       21.805344                    60.570399  \n",
              "386412                       21.617143                    19.414522  \n",
              "625157                       20.898257                    13.923161  \n",
              "306641                       19.284905                    28.512262  \n",
              "154695                       18.439668                    32.844037  \n",
              "360895                       17.805608                    53.409066  \n",
              "346250                       15.883093                    29.754467  \n",
              "81229                        13.666306                    10.484259  \n",
              "139196                       13.221296                    30.759532  \n",
              "410846                       12.819657                    22.070370  \n",
              "151757                       12.765454                    24.437281  \n",
              "652157                       12.552765                    23.109722  \n",
              "137514                       12.504853                    23.109722  \n",
              "644292                       12.456942                    23.109722  \n",
              "379842                       12.222790                     7.120859  \n",
              "644288                       11.513089                    23.307241  \n",
              "182792                       10.414391                    13.433657  \n",
              "619876                       10.385195                     9.180076  \n",
              "473998                        9.854040                    10.904406  \n",
              "373238                        9.830223                    20.383179  \n",
              "626819                        9.534177                    34.216127  \n",
              "380666                        9.198945                     8.936060  \n",
              "143097                        9.007356                     6.284588  \n",
              "631137                        8.581395                     8.477574  \n",
              "642907                        8.301274                    29.775990  \n",
              "190204                        8.084483                     6.355544  \n",
              "604101                        7.951313                    28.895434  \n",
              "154652                        7.233333                    23.471682  \n",
              "441143                        7.082266                    36.961938  \n",
              "379467                        7.068597                     4.690690  \n",
              "625472                        7.014352                     9.621824  \n",
              "144974                        6.807965                    10.809331  \n",
              "54120                         6.453947                     8.607540  \n",
              "389618                        6.292101                    30.293987  \n",
              "348760                        6.012834                     5.770454  \n",
              "472483                        5.838901                     5.675851  \n",
              "69268                         5.804155                     8.722809  \n",
              "227064                        5.789365                     4.827466  \n",
              "351057                        5.748945                     3.399905  \n",
              "275978                        5.660896                     7.779963  \n",
              "614669                        5.564162                     9.467361  \n",
              "218316                        5.534718                     5.702452  \n",
              "175326                        5.464871                     8.287335  \n",
              "345033                        5.460941                    14.787601  \n",
              "547438                        5.058638                     5.124174  \n",
              "526691                        5.029551                     4.457339  \n",
              "560774                        4.960247                     7.191224  \n",
              "127706                        4.718147                    42.021415  \n",
              "437228                        4.628185                     6.488655  \n",
              "268468                        4.620795                     8.425667  \n",
              "388144                        4.588248                     3.953576  \n",
              "72312                         4.531580                     5.709949  \n",
              "177874                        4.449362                     2.829735  \n",
              "346467                        4.425390                     5.971555  \n",
              "380444                        4.336835                     7.125872  \n",
              "430311                        4.332183                     6.010985  \n",
              "501265                        4.322391                    16.816872  \n",
              "587499                        4.263436                    13.190909  \n",
              "317335                        4.263357                     6.039755  \n",
              "580706                        4.115588                     4.889294  \n",
              "107188                        4.113496                     4.972834  \n",
              "192002                        4.110828                     5.417453  \n",
              "419231                        4.056882                     4.696727  \n",
              "546688                        4.019502                     9.955035  \n",
              "543664                        4.019027                     2.914344  \n",
              "181486                        3.996980                     4.555651  \n",
              "380195                        3.933442                     4.852784  \n",
              "88691                         3.859766                     8.696623  \n",
              "663208                        3.750256                     9.565360  \n",
              "653071                        3.731096                    10.015573  \n",
              "328212                        3.717569                     3.384585  \n",
              "4123                          3.679915                     3.371441  \n",
              "255925                        3.648933                     4.476482  \n",
              "79744                         3.646267                     6.692593  \n",
              "132642                        3.638272                     2.725586  \n",
              "347820                        3.618724                    85.429349  \n",
              "456400                        3.593349                     4.364149  \n",
              "276560                        3.470414                     4.417913  \n",
              "244886                        3.450279                     6.423473  \n",
              "618573                        3.443585                     4.130352  \n",
              "570765                        3.428443                     8.191450  \n",
              "316939                        3.425229                     5.254300  \n",
              "475975                        3.369250                     2.788455  \n",
              "7600                          3.365324                    12.195516  \n",
              "247712                        3.358969                    30.347938  \n",
              "260597                        3.329298                     2.858322  \n",
              "362254                        3.324969                     3.632792  \n",
              "655054                        3.316965                    10.279149  \n",
              "627739                        3.313364                    11.042256  \n",
              "537024                        3.295582                     2.875663  \n",
              "384249                        3.245529                     6.094520  \n",
              "472203                        3.221583                     2.746315  \n",
              "472235                        3.219492                     3.481156  \n",
              "667036                        3.200243                     2.474850  \n",
              "72922                         3.185853                     4.058249  \n",
              "428436                        3.184428                     4.363846  \n",
              "149406                        3.177778                     8.132594  \n",
              "80379                         3.156867                     5.911546  \n",
              "76908                         3.149744                     3.889035  \n",
              "56646                         3.132744                     5.206899  \n",
              "651010                        3.131309                     4.615285  \n",
              "472985                        3.127999                     4.365400  \n",
              "279432                        3.125323                     4.923214  \n",
              "598                           3.080222                     3.278927  \n",
              "455347                        3.079567                     2.995682  \n",
              "140396                        3.066198                     4.617361  \n",
              "122516                        3.062648                     5.942151  \n",
              "408057                        3.051341                     2.644706  \n",
              "364641                        3.040925                     3.824420  \n",
              "290249                        3.034656                     4.881506  \n",
              "480101                        3.031534                     2.474850  \n",
              "127878                        3.025999                     3.398759  \n",
              "11606                         3.018736                     5.198790  \n",
              "165022                        3.012523                     2.574706  \n",
              "648223                        2.997887                    10.564824  \n",
              "171437                        2.986425                     4.111260  \n",
              "179530                        2.972477                     6.742394  \n",
              "188896                        2.952921                     3.418312  \n",
              "357692                        2.878985                     9.051374  \n",
              "69167                         2.874584                     3.470934  \n",
              "247509                        2.845164                     3.411086  \n",
              "346410                        2.838145                     3.041985  \n",
              "562728                        2.822018                     9.004811  \n",
              "537376                        2.734922                     2.269267  \n",
              "470795                        2.720154                     4.880457  \n",
              "550408                        2.712591                     4.235674  \n",
              "78721                         2.705191                     6.405584  \n",
              "145284                        2.693610                     2.800714  \n",
              "148560                        2.669730                     6.132973  \n",
              "217143                        2.666246                     3.369275  \n",
              "607275                        2.656678                     3.511598  \n",
              "63797                         2.646596                     3.694547  \n",
              "91070                         2.639013                     3.014043  \n",
              "438035                        2.622099                     2.010774  \n",
              "180988                        2.603889                     2.532137  \n",
              "505039                        2.603295                     2.099295  \n",
              "481071                        2.593618                     3.169060  \n",
              "316132                        2.579555                     3.663451  \n",
              "40439                         2.579242                     3.167337  \n",
              "58102                         2.570355                     5.332711  \n",
              "176483                        2.570355                     5.332711  \n",
              "525634                        2.541581                     3.093400  \n",
              "58101                         2.541479                     5.417327  \n",
              "319821                        2.541341                     2.446603  \n",
              "322601                        2.536675                     2.451735  \n",
              "576646                        2.510895                     2.644145  \n",
              "133773                        2.509438                     3.260007  \n",
              "298431                        2.509291                     2.509995  \n",
              "294150                        2.493190                     4.703406  \n",
              "92372                         2.488663                     2.849930  \n",
              "408987                        2.476710                     3.559058  \n",
              "116991                        2.459094                     4.796376  \n",
              "464332                        2.448537                     5.498938  \n",
              "370482                        2.438770                     2.801286  \n",
              "61926                         2.421942                     2.308771  \n",
              "361494                        2.406284                     2.147058  \n",
              "532667                        2.404560                     2.161864  \n",
              "165951                        2.401854                     7.669465  \n",
              "356451                        2.399004                     2.029509  \n",
              "377146                        2.398395                     1.926760  \n",
              "248111                        2.391552                     4.791456  \n",
              "607207                        2.381067                     2.691981  \n",
              "515231                        2.362205                     1.807065  \n",
              "609566                        2.358186                     2.931640  \n",
              "391142                        2.354368                     3.324039  \n",
              "42449                         2.353814                     4.831196  \n",
              "67903                         2.348344                     4.718559  \n",
              "376283                        2.346918                     3.098536  \n",
              "58080                         2.344442                     4.007365  \n",
              "426662                        2.338028                     2.207999  \n",
              "87577                         2.325539                     3.137241  \n",
              "505961                        2.314672                     1.966784  \n",
              "443734                        2.310547                     3.296182  \n",
              "466359                        2.299921                     4.376548  \n",
              "632178                        2.299562                     2.117483  "
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keywords.loc[:,'max/min_semesterly_abstract_df'] = keywords.apply(lambda row: row['max_semesterly_abstract_df']/row['min_semesterly_abstract_df'], axis=1)\n",
        "keywords.loc[:,'max/min_semesterly_title_df'] = keywords.apply(lambda row: row['max_semesterly_title_df']/row['min_semesterly_title_df'], axis=1)\n",
        "\n",
        "keywords.sort_values('max/min_semesterly_abstract_df', ascending=False).head(200)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27835877-4bb5-4eeb-b105-822aecf20a6e",
      "metadata": {
        "id": "27835877-4bb5-4eeb-b105-822aecf20a6e"
      },
      "source": [
        "Let's now look at the distribution of the ratio of the maximum to minimum semesterly abstract document frequencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1479cf9f-bdaa-43a6-8162-a6350cdfcb3b",
      "metadata": {
        "id": "1479cf9f-bdaa-43a6-8162-a6350cdfcb3b",
        "outputId": "048d3e12-8d3e-415f-e3b9-39bda69f486b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    500.000000\n",
              "mean       6.639829\n",
              "std       20.295297\n",
              "min        1.126150\n",
              "25%        1.480039\n",
              "50%        2.012247\n",
              "75%        3.131668\n",
              "max      263.748704\n",
              "Name: max/min_semesterly_abstract_df, dtype: float64"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keywords['max/min_semesterly_abstract_df'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e920a9a4-8846-4568-8d55-5fcac1471fd1",
      "metadata": {
        "id": "e920a9a4-8846-4568-8d55-5fcac1471fd1"
      },
      "source": [
        "We see that there is a very small minority of words that have seen a very large change in document frequencies over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ff92a7-0517-4fab-a980-e82dd3cd2320",
      "metadata": {
        "scrolled": true,
        "id": "28ff92a7-0517-4fab-a980-e82dd3cd2320",
        "outputId": "f7c8bd6c-4c5c-4947-9539-82faeec82d7c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'keywords' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mkeywords\u001b[49m[keywords[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax/min_semesterly_abstract_df\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax/min_semesterly_abstract_df\u001b[39m\u001b[38;5;124m'\u001b[39m],ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keywords' is not defined"
          ]
        }
      ],
      "source": [
        "keywords[keywords['max/min_semesterly_abstract_df'] > 10].sort_values(['max/min_semesterly_abstract_df'],ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12674303-1613-4205-8567-a48b32337631",
      "metadata": {
        "id": "12674303-1613-4205-8567-a48b32337631"
      },
      "outputs": [],
      "source": [
        "def get_time_series_data(term):\n",
        "    return pd.Series(monthly_titles_df[:,term]), pd.Series(monthly_abstracts_df[:,term])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c82daf97-3e37-434e-a009-adfe18391c31",
      "metadata": {
        "id": "c82daf97-3e37-434e-a009-adfe18391c31"
      },
      "outputs": [],
      "source": [
        "keywords = keywords.sort_values('max/min_semesterly_abstract_df', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b67affb1-0c01-476a-a49a-7e31f1b37f9b",
      "metadata": {
        "scrolled": true,
        "id": "b67affb1-0c01-476a-a49a-7e31f1b37f9b",
        "outputId": "ca555642-dcff-4913-e240-9c7e45a16794"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
            "/tmp/ipykernel_56873/2474562057.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(title)'] = title\n",
            "/tmp/ipykernel_56873/2474562057.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>machine learning_(title)</th>\n",
              "      <th>machine learning_(abstracts)</th>\n",
              "      <th>topological insulator_(title)</th>\n",
              "      <th>topological insulator_(abstracts)</th>\n",
              "      <th>graphene_(title)</th>\n",
              "      <th>graphene_(abstracts)</th>\n",
              "      <th>majorana_(title)</th>\n",
              "      <th>majorana_(abstracts)</th>\n",
              "      <th>topological phase_(title)</th>\n",
              "      <th>topological phase_(abstracts)</th>\n",
              "      <th>...</th>\n",
              "      <th>calculations_(title)</th>\n",
              "      <th>calculations_(abstracts)</th>\n",
              "      <th>behavior_(title)</th>\n",
              "      <th>behavior_(abstracts)</th>\n",
              "      <th>one_(title)</th>\n",
              "      <th>one_(abstracts)</th>\n",
              "      <th>dependence_(title)</th>\n",
              "      <th>dependence_(abstracts)</th>\n",
              "      <th>function_(title)</th>\n",
              "      <th>function_(abstracts)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001976</td>\n",
              "      <td>0.001976</td>\n",
              "      <td>0.001976</td>\n",
              "      <td>0.001976</td>\n",
              "      <td>0.001976</td>\n",
              "      <td>0.003953</td>\n",
              "      <td>0.001976</td>\n",
              "      <td>0.003953</td>\n",
              "      <td>0.001976</td>\n",
              "      <td>0.003953</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013834</td>\n",
              "      <td>0.247036</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.116601</td>\n",
              "      <td>0.033597</td>\n",
              "      <td>0.199605</td>\n",
              "      <td>0.031621</td>\n",
              "      <td>0.227273</td>\n",
              "      <td>0.023715</td>\n",
              "      <td>0.258893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.003831</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.005747</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009579</td>\n",
              "      <td>0.199234</td>\n",
              "      <td>0.011494</td>\n",
              "      <td>0.130268</td>\n",
              "      <td>0.038314</td>\n",
              "      <td>0.216475</td>\n",
              "      <td>0.015326</td>\n",
              "      <td>0.212644</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.237548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.003515</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010545</td>\n",
              "      <td>0.203866</td>\n",
              "      <td>0.017575</td>\n",
              "      <td>0.117750</td>\n",
              "      <td>0.022847</td>\n",
              "      <td>0.240773</td>\n",
              "      <td>0.031634</td>\n",
              "      <td>0.231986</td>\n",
              "      <td>0.026362</td>\n",
              "      <td>0.219684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009242</td>\n",
              "      <td>0.184843</td>\n",
              "      <td>0.018484</td>\n",
              "      <td>0.131238</td>\n",
              "      <td>0.033272</td>\n",
              "      <td>0.192237</td>\n",
              "      <td>0.033272</td>\n",
              "      <td>0.210721</td>\n",
              "      <td>0.025878</td>\n",
              "      <td>0.232902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.001698</td>\n",
              "      <td>0.001698</td>\n",
              "      <td>0.001698</td>\n",
              "      <td>0.001698</td>\n",
              "      <td>0.001698</td>\n",
              "      <td>0.001698</td>\n",
              "      <td>0.001698</td>\n",
              "      <td>0.003396</td>\n",
              "      <td>0.001698</td>\n",
              "      <td>0.001698</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006791</td>\n",
              "      <td>0.220713</td>\n",
              "      <td>0.018676</td>\n",
              "      <td>0.137521</td>\n",
              "      <td>0.033956</td>\n",
              "      <td>0.258065</td>\n",
              "      <td>0.022071</td>\n",
              "      <td>0.229202</td>\n",
              "      <td>0.028862</td>\n",
              "      <td>0.254669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.003683</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007366</td>\n",
              "      <td>0.187845</td>\n",
              "      <td>0.016575</td>\n",
              "      <td>0.134438</td>\n",
              "      <td>0.027624</td>\n",
              "      <td>0.224678</td>\n",
              "      <td>0.023941</td>\n",
              "      <td>0.220994</td>\n",
              "      <td>0.023941</td>\n",
              "      <td>0.237569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.003521</td>\n",
              "      <td>0.003521</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.003521</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012324</td>\n",
              "      <td>0.195423</td>\n",
              "      <td>0.024648</td>\n",
              "      <td>0.135563</td>\n",
              "      <td>0.028169</td>\n",
              "      <td>0.234155</td>\n",
              "      <td>0.012324</td>\n",
              "      <td>0.214789</td>\n",
              "      <td>0.031690</td>\n",
              "      <td>0.258803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.003810</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.003810</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019048</td>\n",
              "      <td>0.186667</td>\n",
              "      <td>0.017143</td>\n",
              "      <td>0.125714</td>\n",
              "      <td>0.032381</td>\n",
              "      <td>0.194286</td>\n",
              "      <td>0.011429</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.022857</td>\n",
              "      <td>0.245714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.003738</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016822</td>\n",
              "      <td>0.164486</td>\n",
              "      <td>0.018692</td>\n",
              "      <td>0.138318</td>\n",
              "      <td>0.031776</td>\n",
              "      <td>0.228037</td>\n",
              "      <td>0.020561</td>\n",
              "      <td>0.209346</td>\n",
              "      <td>0.018692</td>\n",
              "      <td>0.218692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.007260</td>\n",
              "      <td>0.003630</td>\n",
              "      <td>0.003630</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010889</td>\n",
              "      <td>0.201452</td>\n",
              "      <td>0.021779</td>\n",
              "      <td>0.116152</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.221416</td>\n",
              "      <td>0.021779</td>\n",
              "      <td>0.206897</td>\n",
              "      <td>0.018149</td>\n",
              "      <td>0.257713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.001739</td>\n",
              "      <td>0.001739</td>\n",
              "      <td>0.001739</td>\n",
              "      <td>0.001739</td>\n",
              "      <td>0.001739</td>\n",
              "      <td>0.005217</td>\n",
              "      <td>0.001739</td>\n",
              "      <td>0.001739</td>\n",
              "      <td>0.003478</td>\n",
              "      <td>0.003478</td>\n",
              "      <td>...</td>\n",
              "      <td>0.029565</td>\n",
              "      <td>0.215652</td>\n",
              "      <td>0.005217</td>\n",
              "      <td>0.130435</td>\n",
              "      <td>0.029565</td>\n",
              "      <td>0.253913</td>\n",
              "      <td>0.022609</td>\n",
              "      <td>0.226087</td>\n",
              "      <td>0.026087</td>\n",
              "      <td>0.241739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.003521</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.003521</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010563</td>\n",
              "      <td>0.163732</td>\n",
              "      <td>0.022887</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.026408</td>\n",
              "      <td>0.183099</td>\n",
              "      <td>0.021127</td>\n",
              "      <td>0.234155</td>\n",
              "      <td>0.019366</td>\n",
              "      <td>0.216549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.003817</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007634</td>\n",
              "      <td>0.230916</td>\n",
              "      <td>0.020992</td>\n",
              "      <td>0.122137</td>\n",
              "      <td>0.034351</td>\n",
              "      <td>0.202290</td>\n",
              "      <td>0.013359</td>\n",
              "      <td>0.198473</td>\n",
              "      <td>0.026718</td>\n",
              "      <td>0.234733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.001754</td>\n",
              "      <td>0.001754</td>\n",
              "      <td>0.001754</td>\n",
              "      <td>0.001754</td>\n",
              "      <td>0.001754</td>\n",
              "      <td>0.001754</td>\n",
              "      <td>0.001754</td>\n",
              "      <td>0.001754</td>\n",
              "      <td>0.001754</td>\n",
              "      <td>0.001754</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012281</td>\n",
              "      <td>0.217544</td>\n",
              "      <td>0.021053</td>\n",
              "      <td>0.126316</td>\n",
              "      <td>0.036842</td>\n",
              "      <td>0.212281</td>\n",
              "      <td>0.015789</td>\n",
              "      <td>0.194737</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>0.217544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.002849</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.002849</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017094</td>\n",
              "      <td>0.179487</td>\n",
              "      <td>0.019943</td>\n",
              "      <td>0.113960</td>\n",
              "      <td>0.024217</td>\n",
              "      <td>0.220798</td>\n",
              "      <td>0.022792</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.025641</td>\n",
              "      <td>0.216524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.003221</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009662</td>\n",
              "      <td>0.183575</td>\n",
              "      <td>0.016103</td>\n",
              "      <td>0.123994</td>\n",
              "      <td>0.022544</td>\n",
              "      <td>0.202899</td>\n",
              "      <td>0.030596</td>\n",
              "      <td>0.244767</td>\n",
              "      <td>0.022544</td>\n",
              "      <td>0.238325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.004360</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008721</td>\n",
              "      <td>0.210756</td>\n",
              "      <td>0.020349</td>\n",
              "      <td>0.136628</td>\n",
              "      <td>0.020349</td>\n",
              "      <td>0.207849</td>\n",
              "      <td>0.021802</td>\n",
              "      <td>0.218023</td>\n",
              "      <td>0.024709</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.002797</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.002797</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006993</td>\n",
              "      <td>0.197203</td>\n",
              "      <td>0.012587</td>\n",
              "      <td>0.117483</td>\n",
              "      <td>0.018182</td>\n",
              "      <td>0.187413</td>\n",
              "      <td>0.027972</td>\n",
              "      <td>0.205594</td>\n",
              "      <td>0.023776</td>\n",
              "      <td>0.234965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.001506</td>\n",
              "      <td>0.001506</td>\n",
              "      <td>0.001506</td>\n",
              "      <td>0.001506</td>\n",
              "      <td>0.003012</td>\n",
              "      <td>0.003012</td>\n",
              "      <td>0.001506</td>\n",
              "      <td>0.001506</td>\n",
              "      <td>0.001506</td>\n",
              "      <td>0.004518</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012048</td>\n",
              "      <td>0.177711</td>\n",
              "      <td>0.015060</td>\n",
              "      <td>0.138554</td>\n",
              "      <td>0.024096</td>\n",
              "      <td>0.206325</td>\n",
              "      <td>0.027108</td>\n",
              "      <td>0.225904</td>\n",
              "      <td>0.022590</td>\n",
              "      <td>0.213855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.001653</td>\n",
              "      <td>0.001653</td>\n",
              "      <td>0.001653</td>\n",
              "      <td>0.001653</td>\n",
              "      <td>0.001653</td>\n",
              "      <td>0.004959</td>\n",
              "      <td>0.003306</td>\n",
              "      <td>0.004959</td>\n",
              "      <td>0.001653</td>\n",
              "      <td>0.001653</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011570</td>\n",
              "      <td>0.166942</td>\n",
              "      <td>0.009917</td>\n",
              "      <td>0.122314</td>\n",
              "      <td>0.014876</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.028099</td>\n",
              "      <td>0.221488</td>\n",
              "      <td>0.033058</td>\n",
              "      <td>0.213223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.001642</td>\n",
              "      <td>0.001642</td>\n",
              "      <td>0.001642</td>\n",
              "      <td>0.001642</td>\n",
              "      <td>0.001642</td>\n",
              "      <td>0.001642</td>\n",
              "      <td>0.001642</td>\n",
              "      <td>0.001642</td>\n",
              "      <td>0.001642</td>\n",
              "      <td>0.001642</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019704</td>\n",
              "      <td>0.213465</td>\n",
              "      <td>0.026273</td>\n",
              "      <td>0.100164</td>\n",
              "      <td>0.021346</td>\n",
              "      <td>0.226601</td>\n",
              "      <td>0.026273</td>\n",
              "      <td>0.246305</td>\n",
              "      <td>0.029557</td>\n",
              "      <td>0.269294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.002833</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009915</td>\n",
              "      <td>0.171388</td>\n",
              "      <td>0.012748</td>\n",
              "      <td>0.117564</td>\n",
              "      <td>0.025496</td>\n",
              "      <td>0.208215</td>\n",
              "      <td>0.026912</td>\n",
              "      <td>0.223796</td>\n",
              "      <td>0.025496</td>\n",
              "      <td>0.215297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.001520</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>0.003040</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>0.003040</td>\n",
              "      <td>0.003040</td>\n",
              "      <td>...</td>\n",
              "      <td>0.024316</td>\n",
              "      <td>0.205167</td>\n",
              "      <td>0.018237</td>\n",
              "      <td>0.104863</td>\n",
              "      <td>0.025836</td>\n",
              "      <td>0.189970</td>\n",
              "      <td>0.031915</td>\n",
              "      <td>0.221884</td>\n",
              "      <td>0.022796</td>\n",
              "      <td>0.238602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.001776</td>\n",
              "      <td>0.001776</td>\n",
              "      <td>0.001776</td>\n",
              "      <td>0.001776</td>\n",
              "      <td>0.001776</td>\n",
              "      <td>0.001776</td>\n",
              "      <td>0.001776</td>\n",
              "      <td>0.001776</td>\n",
              "      <td>0.001776</td>\n",
              "      <td>0.003552</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012433</td>\n",
              "      <td>0.209591</td>\n",
              "      <td>0.014210</td>\n",
              "      <td>0.115453</td>\n",
              "      <td>0.019538</td>\n",
              "      <td>0.191829</td>\n",
              "      <td>0.019538</td>\n",
              "      <td>0.220249</td>\n",
              "      <td>0.042629</td>\n",
              "      <td>0.220249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.001536</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010753</td>\n",
              "      <td>0.205837</td>\n",
              "      <td>0.012289</td>\n",
              "      <td>0.118280</td>\n",
              "      <td>0.021505</td>\n",
              "      <td>0.193548</td>\n",
              "      <td>0.023041</td>\n",
              "      <td>0.231951</td>\n",
              "      <td>0.023041</td>\n",
              "      <td>0.202765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.001686</td>\n",
              "      <td>0.001686</td>\n",
              "      <td>0.001686</td>\n",
              "      <td>0.001686</td>\n",
              "      <td>0.001686</td>\n",
              "      <td>0.001686</td>\n",
              "      <td>0.001686</td>\n",
              "      <td>0.003373</td>\n",
              "      <td>0.001686</td>\n",
              "      <td>0.001686</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013491</td>\n",
              "      <td>0.220911</td>\n",
              "      <td>0.018550</td>\n",
              "      <td>0.139966</td>\n",
              "      <td>0.020236</td>\n",
              "      <td>0.217538</td>\n",
              "      <td>0.032040</td>\n",
              "      <td>0.251265</td>\n",
              "      <td>0.020236</td>\n",
              "      <td>0.231029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.002833</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004249</td>\n",
              "      <td>0.195467</td>\n",
              "      <td>0.024079</td>\n",
              "      <td>0.130312</td>\n",
              "      <td>0.026912</td>\n",
              "      <td>0.212465</td>\n",
              "      <td>0.031161</td>\n",
              "      <td>0.245042</td>\n",
              "      <td>0.038244</td>\n",
              "      <td>0.209632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.001486</td>\n",
              "      <td>0.001486</td>\n",
              "      <td>0.001486</td>\n",
              "      <td>0.001486</td>\n",
              "      <td>0.001486</td>\n",
              "      <td>0.001486</td>\n",
              "      <td>0.001486</td>\n",
              "      <td>0.001486</td>\n",
              "      <td>0.001486</td>\n",
              "      <td>0.005944</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007429</td>\n",
              "      <td>0.196137</td>\n",
              "      <td>0.016345</td>\n",
              "      <td>0.123328</td>\n",
              "      <td>0.026746</td>\n",
              "      <td>0.216939</td>\n",
              "      <td>0.019316</td>\n",
              "      <td>0.205052</td>\n",
              "      <td>0.035661</td>\n",
              "      <td>0.225854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.002797</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009790</td>\n",
              "      <td>0.176224</td>\n",
              "      <td>0.019580</td>\n",
              "      <td>0.146853</td>\n",
              "      <td>0.016783</td>\n",
              "      <td>0.198601</td>\n",
              "      <td>0.018182</td>\n",
              "      <td>0.209790</td>\n",
              "      <td>0.046154</td>\n",
              "      <td>0.227972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.001546</td>\n",
              "      <td>0.001546</td>\n",
              "      <td>0.001546</td>\n",
              "      <td>0.001546</td>\n",
              "      <td>0.001546</td>\n",
              "      <td>0.001546</td>\n",
              "      <td>0.001546</td>\n",
              "      <td>0.003091</td>\n",
              "      <td>0.001546</td>\n",
              "      <td>0.004637</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009274</td>\n",
              "      <td>0.166924</td>\n",
              "      <td>0.021638</td>\n",
              "      <td>0.120556</td>\n",
              "      <td>0.018547</td>\n",
              "      <td>0.173107</td>\n",
              "      <td>0.035549</td>\n",
              "      <td>0.231839</td>\n",
              "      <td>0.027821</td>\n",
              "      <td>0.204019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.001267</td>\n",
              "      <td>0.001267</td>\n",
              "      <td>0.001267</td>\n",
              "      <td>0.001267</td>\n",
              "      <td>0.001267</td>\n",
              "      <td>0.001267</td>\n",
              "      <td>0.001267</td>\n",
              "      <td>0.001267</td>\n",
              "      <td>0.001267</td>\n",
              "      <td>0.002535</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015209</td>\n",
              "      <td>0.200253</td>\n",
              "      <td>0.020279</td>\n",
              "      <td>0.141952</td>\n",
              "      <td>0.020279</td>\n",
              "      <td>0.205323</td>\n",
              "      <td>0.021546</td>\n",
              "      <td>0.231939</td>\n",
              "      <td>0.017744</td>\n",
              "      <td>0.202788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.004552</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010622</td>\n",
              "      <td>0.198786</td>\n",
              "      <td>0.018209</td>\n",
              "      <td>0.121396</td>\n",
              "      <td>0.031866</td>\n",
              "      <td>0.207891</td>\n",
              "      <td>0.019727</td>\n",
              "      <td>0.268589</td>\n",
              "      <td>0.028832</td>\n",
              "      <td>0.242792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.001346</td>\n",
              "      <td>0.001346</td>\n",
              "      <td>0.001346</td>\n",
              "      <td>0.001346</td>\n",
              "      <td>0.001346</td>\n",
              "      <td>0.001346</td>\n",
              "      <td>0.001346</td>\n",
              "      <td>0.001346</td>\n",
              "      <td>0.001346</td>\n",
              "      <td>0.001346</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017497</td>\n",
              "      <td>0.173620</td>\n",
              "      <td>0.025572</td>\n",
              "      <td>0.122476</td>\n",
              "      <td>0.022880</td>\n",
              "      <td>0.207268</td>\n",
              "      <td>0.025572</td>\n",
              "      <td>0.231494</td>\n",
              "      <td>0.024226</td>\n",
              "      <td>0.193809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.002614</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016993</td>\n",
              "      <td>0.184314</td>\n",
              "      <td>0.018301</td>\n",
              "      <td>0.129412</td>\n",
              "      <td>0.026144</td>\n",
              "      <td>0.206536</td>\n",
              "      <td>0.020915</td>\n",
              "      <td>0.227451</td>\n",
              "      <td>0.028758</td>\n",
              "      <td>0.220915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.001330</td>\n",
              "      <td>0.001330</td>\n",
              "      <td>0.001330</td>\n",
              "      <td>0.001330</td>\n",
              "      <td>0.001330</td>\n",
              "      <td>0.003989</td>\n",
              "      <td>0.001330</td>\n",
              "      <td>0.001330</td>\n",
              "      <td>0.001330</td>\n",
              "      <td>0.002660</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010638</td>\n",
              "      <td>0.168883</td>\n",
              "      <td>0.011968</td>\n",
              "      <td>0.114362</td>\n",
              "      <td>0.013298</td>\n",
              "      <td>0.204787</td>\n",
              "      <td>0.021277</td>\n",
              "      <td>0.208777</td>\n",
              "      <td>0.022606</td>\n",
              "      <td>0.234043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.001422</td>\n",
              "      <td>0.001422</td>\n",
              "      <td>0.001422</td>\n",
              "      <td>0.001422</td>\n",
              "      <td>0.001422</td>\n",
              "      <td>0.002845</td>\n",
              "      <td>0.001422</td>\n",
              "      <td>0.001422</td>\n",
              "      <td>0.001422</td>\n",
              "      <td>0.002845</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015647</td>\n",
              "      <td>0.199147</td>\n",
              "      <td>0.024182</td>\n",
              "      <td>0.142248</td>\n",
              "      <td>0.024182</td>\n",
              "      <td>0.209104</td>\n",
              "      <td>0.022760</td>\n",
              "      <td>0.238976</td>\n",
              "      <td>0.039829</td>\n",
              "      <td>0.227596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.002920</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011679</td>\n",
              "      <td>0.186861</td>\n",
              "      <td>0.018978</td>\n",
              "      <td>0.138686</td>\n",
              "      <td>0.033577</td>\n",
              "      <td>0.210219</td>\n",
              "      <td>0.026277</td>\n",
              "      <td>0.216058</td>\n",
              "      <td>0.039416</td>\n",
              "      <td>0.237956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.001488</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008929</td>\n",
              "      <td>0.188988</td>\n",
              "      <td>0.016369</td>\n",
              "      <td>0.138393</td>\n",
              "      <td>0.016369</td>\n",
              "      <td>0.197917</td>\n",
              "      <td>0.014881</td>\n",
              "      <td>0.230655</td>\n",
              "      <td>0.022321</td>\n",
              "      <td>0.247024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011050</td>\n",
              "      <td>0.182320</td>\n",
              "      <td>0.016575</td>\n",
              "      <td>0.106354</td>\n",
              "      <td>0.022099</td>\n",
              "      <td>0.194751</td>\n",
              "      <td>0.029006</td>\n",
              "      <td>0.223757</td>\n",
              "      <td>0.023481</td>\n",
              "      <td>0.220994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.001333</td>\n",
              "      <td>0.001333</td>\n",
              "      <td>0.001333</td>\n",
              "      <td>0.001333</td>\n",
              "      <td>0.001333</td>\n",
              "      <td>0.001333</td>\n",
              "      <td>0.001333</td>\n",
              "      <td>0.001333</td>\n",
              "      <td>0.001333</td>\n",
              "      <td>0.002667</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012000</td>\n",
              "      <td>0.197333</td>\n",
              "      <td>0.018667</td>\n",
              "      <td>0.089333</td>\n",
              "      <td>0.025333</td>\n",
              "      <td>0.216000</td>\n",
              "      <td>0.025333</td>\n",
              "      <td>0.225333</td>\n",
              "      <td>0.018667</td>\n",
              "      <td>0.206667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.002581</td>\n",
              "      <td>0.003871</td>\n",
              "      <td>0.003871</td>\n",
              "      <td>0.003871</td>\n",
              "      <td>0.003871</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009032</td>\n",
              "      <td>0.198710</td>\n",
              "      <td>0.023226</td>\n",
              "      <td>0.125161</td>\n",
              "      <td>0.018065</td>\n",
              "      <td>0.209032</td>\n",
              "      <td>0.018065</td>\n",
              "      <td>0.233548</td>\n",
              "      <td>0.023226</td>\n",
              "      <td>0.211613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.002500</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.002500</td>\n",
              "      <td>0.002500</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.017500</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.028750</td>\n",
              "      <td>0.218750</td>\n",
              "      <td>0.018750</td>\n",
              "      <td>0.232500</td>\n",
              "      <td>0.026250</td>\n",
              "      <td>0.210000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.001205</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>0.002410</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013253</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.021687</td>\n",
              "      <td>0.136145</td>\n",
              "      <td>0.030120</td>\n",
              "      <td>0.207229</td>\n",
              "      <td>0.022892</td>\n",
              "      <td>0.244578</td>\n",
              "      <td>0.024096</td>\n",
              "      <td>0.216867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.001429</td>\n",
              "      <td>0.001429</td>\n",
              "      <td>0.001429</td>\n",
              "      <td>0.001429</td>\n",
              "      <td>0.004286</td>\n",
              "      <td>0.005714</td>\n",
              "      <td>0.001429</td>\n",
              "      <td>0.002857</td>\n",
              "      <td>0.001429</td>\n",
              "      <td>0.001429</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014286</td>\n",
              "      <td>0.218571</td>\n",
              "      <td>0.022857</td>\n",
              "      <td>0.131429</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>0.201429</td>\n",
              "      <td>0.027143</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.024286</td>\n",
              "      <td>0.227143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.001255</td>\n",
              "      <td>0.001255</td>\n",
              "      <td>0.001255</td>\n",
              "      <td>0.001255</td>\n",
              "      <td>0.002509</td>\n",
              "      <td>0.003764</td>\n",
              "      <td>0.001255</td>\n",
              "      <td>0.002509</td>\n",
              "      <td>0.003764</td>\n",
              "      <td>0.006274</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012547</td>\n",
              "      <td>0.179423</td>\n",
              "      <td>0.012547</td>\n",
              "      <td>0.116688</td>\n",
              "      <td>0.025094</td>\n",
              "      <td>0.208281</td>\n",
              "      <td>0.023839</td>\n",
              "      <td>0.217064</td>\n",
              "      <td>0.028858</td>\n",
              "      <td>0.229611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.002361</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.002361</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012987</td>\n",
              "      <td>0.171192</td>\n",
              "      <td>0.016529</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.029516</td>\n",
              "      <td>0.219599</td>\n",
              "      <td>0.017710</td>\n",
              "      <td>0.221960</td>\n",
              "      <td>0.027155</td>\n",
              "      <td>0.233766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.001395</td>\n",
              "      <td>0.001395</td>\n",
              "      <td>0.001395</td>\n",
              "      <td>0.001395</td>\n",
              "      <td>0.002789</td>\n",
              "      <td>0.004184</td>\n",
              "      <td>0.001395</td>\n",
              "      <td>0.001395</td>\n",
              "      <td>0.001395</td>\n",
              "      <td>0.002789</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011158</td>\n",
              "      <td>0.175732</td>\n",
              "      <td>0.016736</td>\n",
              "      <td>0.132497</td>\n",
              "      <td>0.023710</td>\n",
              "      <td>0.213389</td>\n",
              "      <td>0.025105</td>\n",
              "      <td>0.227336</td>\n",
              "      <td>0.022315</td>\n",
              "      <td>0.205021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.002484</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.002484</td>\n",
              "      <td>0.002484</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006211</td>\n",
              "      <td>0.162733</td>\n",
              "      <td>0.016149</td>\n",
              "      <td>0.125466</td>\n",
              "      <td>0.016149</td>\n",
              "      <td>0.206211</td>\n",
              "      <td>0.026087</td>\n",
              "      <td>0.219876</td>\n",
              "      <td>0.012422</td>\n",
              "      <td>0.228571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.001374</td>\n",
              "      <td>0.001374</td>\n",
              "      <td>0.001374</td>\n",
              "      <td>0.001374</td>\n",
              "      <td>0.001374</td>\n",
              "      <td>0.001374</td>\n",
              "      <td>0.001374</td>\n",
              "      <td>0.001374</td>\n",
              "      <td>0.002747</td>\n",
              "      <td>0.005495</td>\n",
              "      <td>...</td>\n",
              "      <td>0.024725</td>\n",
              "      <td>0.195055</td>\n",
              "      <td>0.012363</td>\n",
              "      <td>0.122253</td>\n",
              "      <td>0.030220</td>\n",
              "      <td>0.207418</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.023352</td>\n",
              "      <td>0.233516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.002646</td>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.002646</td>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.001323</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005291</td>\n",
              "      <td>0.187831</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.125661</td>\n",
              "      <td>0.026455</td>\n",
              "      <td>0.239418</td>\n",
              "      <td>0.021164</td>\n",
              "      <td>0.227513</td>\n",
              "      <td>0.026455</td>\n",
              "      <td>0.210317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.001206</td>\n",
              "      <td>0.001206</td>\n",
              "      <td>0.001206</td>\n",
              "      <td>0.001206</td>\n",
              "      <td>0.002413</td>\n",
              "      <td>0.002413</td>\n",
              "      <td>0.001206</td>\n",
              "      <td>0.001206</td>\n",
              "      <td>0.001206</td>\n",
              "      <td>0.002413</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009650</td>\n",
              "      <td>0.153197</td>\n",
              "      <td>0.015682</td>\n",
              "      <td>0.121834</td>\n",
              "      <td>0.019300</td>\n",
              "      <td>0.217129</td>\n",
              "      <td>0.018094</td>\n",
              "      <td>0.213510</td>\n",
              "      <td>0.018094</td>\n",
              "      <td>0.215923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.004884</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010989</td>\n",
              "      <td>0.183150</td>\n",
              "      <td>0.010989</td>\n",
              "      <td>0.130647</td>\n",
              "      <td>0.023199</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>0.026862</td>\n",
              "      <td>0.263736</td>\n",
              "      <td>0.029304</td>\n",
              "      <td>0.234432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0.001263</td>\n",
              "      <td>0.001263</td>\n",
              "      <td>0.001263</td>\n",
              "      <td>0.001263</td>\n",
              "      <td>0.001263</td>\n",
              "      <td>0.001263</td>\n",
              "      <td>0.001263</td>\n",
              "      <td>0.001263</td>\n",
              "      <td>0.001263</td>\n",
              "      <td>0.002525</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013889</td>\n",
              "      <td>0.193182</td>\n",
              "      <td>0.021465</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.025253</td>\n",
              "      <td>0.227273</td>\n",
              "      <td>0.017677</td>\n",
              "      <td>0.227273</td>\n",
              "      <td>0.029040</td>\n",
              "      <td>0.243687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0.001167</td>\n",
              "      <td>0.001167</td>\n",
              "      <td>0.001167</td>\n",
              "      <td>0.001167</td>\n",
              "      <td>0.001167</td>\n",
              "      <td>0.001167</td>\n",
              "      <td>0.001167</td>\n",
              "      <td>0.003501</td>\n",
              "      <td>0.002334</td>\n",
              "      <td>0.001167</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012835</td>\n",
              "      <td>0.185531</td>\n",
              "      <td>0.011669</td>\n",
              "      <td>0.120187</td>\n",
              "      <td>0.016336</td>\n",
              "      <td>0.190198</td>\n",
              "      <td>0.023337</td>\n",
              "      <td>0.266044</td>\n",
              "      <td>0.028005</td>\n",
              "      <td>0.225204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009978</td>\n",
              "      <td>0.207317</td>\n",
              "      <td>0.018847</td>\n",
              "      <td>0.128603</td>\n",
              "      <td>0.023282</td>\n",
              "      <td>0.220621</td>\n",
              "      <td>0.029933</td>\n",
              "      <td>0.250554</td>\n",
              "      <td>0.021064</td>\n",
              "      <td>0.220621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.001328</td>\n",
              "      <td>0.001328</td>\n",
              "      <td>0.001328</td>\n",
              "      <td>0.001328</td>\n",
              "      <td>0.001328</td>\n",
              "      <td>0.002656</td>\n",
              "      <td>0.001328</td>\n",
              "      <td>0.001328</td>\n",
              "      <td>0.002656</td>\n",
              "      <td>0.002656</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010624</td>\n",
              "      <td>0.193891</td>\n",
              "      <td>0.022576</td>\n",
              "      <td>0.140770</td>\n",
              "      <td>0.029216</td>\n",
              "      <td>0.239044</td>\n",
              "      <td>0.023904</td>\n",
              "      <td>0.235060</td>\n",
              "      <td>0.030544</td>\n",
              "      <td>0.245684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.002381</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.002381</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>...</td>\n",
              "      <td>0.020238</td>\n",
              "      <td>0.176190</td>\n",
              "      <td>0.022619</td>\n",
              "      <td>0.123810</td>\n",
              "      <td>0.017857</td>\n",
              "      <td>0.191667</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.223810</td>\n",
              "      <td>0.032143</td>\n",
              "      <td>0.219048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.004651</td>\n",
              "      <td>0.005814</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.002326</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009302</td>\n",
              "      <td>0.176744</td>\n",
              "      <td>0.018605</td>\n",
              "      <td>0.131395</td>\n",
              "      <td>0.025581</td>\n",
              "      <td>0.211628</td>\n",
              "      <td>0.025581</td>\n",
              "      <td>0.230233</td>\n",
              "      <td>0.033721</td>\n",
              "      <td>0.211628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>0.001198</td>\n",
              "      <td>0.001198</td>\n",
              "      <td>0.001198</td>\n",
              "      <td>0.001198</td>\n",
              "      <td>0.002395</td>\n",
              "      <td>0.005988</td>\n",
              "      <td>0.001198</td>\n",
              "      <td>0.003593</td>\n",
              "      <td>0.002395</td>\n",
              "      <td>0.001198</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002395</td>\n",
              "      <td>0.201198</td>\n",
              "      <td>0.004790</td>\n",
              "      <td>0.124551</td>\n",
              "      <td>0.021557</td>\n",
              "      <td>0.217964</td>\n",
              "      <td>0.026347</td>\n",
              "      <td>0.225150</td>\n",
              "      <td>0.032335</td>\n",
              "      <td>0.255090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.002320</td>\n",
              "      <td>0.002320</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.002320</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010441</td>\n",
              "      <td>0.198376</td>\n",
              "      <td>0.015081</td>\n",
              "      <td>0.129930</td>\n",
              "      <td>0.025522</td>\n",
              "      <td>0.238979</td>\n",
              "      <td>0.032483</td>\n",
              "      <td>0.232019</td>\n",
              "      <td>0.013921</td>\n",
              "      <td>0.229698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>0.001214</td>\n",
              "      <td>0.001214</td>\n",
              "      <td>0.001214</td>\n",
              "      <td>0.001214</td>\n",
              "      <td>0.001214</td>\n",
              "      <td>0.001214</td>\n",
              "      <td>0.001214</td>\n",
              "      <td>0.001214</td>\n",
              "      <td>0.001214</td>\n",
              "      <td>0.001214</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013350</td>\n",
              "      <td>0.182039</td>\n",
              "      <td>0.015777</td>\n",
              "      <td>0.126214</td>\n",
              "      <td>0.023058</td>\n",
              "      <td>0.202670</td>\n",
              "      <td>0.027913</td>\n",
              "      <td>0.218447</td>\n",
              "      <td>0.024272</td>\n",
              "      <td>0.208738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>0.001361</td>\n",
              "      <td>0.001361</td>\n",
              "      <td>0.001361</td>\n",
              "      <td>0.001361</td>\n",
              "      <td>0.001361</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>0.001361</td>\n",
              "      <td>0.001361</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>0.006803</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013605</td>\n",
              "      <td>0.198639</td>\n",
              "      <td>0.017687</td>\n",
              "      <td>0.108844</td>\n",
              "      <td>0.019048</td>\n",
              "      <td>0.225850</td>\n",
              "      <td>0.031293</td>\n",
              "      <td>0.214966</td>\n",
              "      <td>0.028571</td>\n",
              "      <td>0.224490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.002304</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.002304</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.002304</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009217</td>\n",
              "      <td>0.171659</td>\n",
              "      <td>0.010369</td>\n",
              "      <td>0.107143</td>\n",
              "      <td>0.017281</td>\n",
              "      <td>0.211982</td>\n",
              "      <td>0.027650</td>\n",
              "      <td>0.252304</td>\n",
              "      <td>0.023041</td>\n",
              "      <td>0.231567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>0.001104</td>\n",
              "      <td>0.002208</td>\n",
              "      <td>0.001104</td>\n",
              "      <td>0.001104</td>\n",
              "      <td>0.001104</td>\n",
              "      <td>0.002208</td>\n",
              "      <td>0.001104</td>\n",
              "      <td>0.001104</td>\n",
              "      <td>0.001104</td>\n",
              "      <td>0.003311</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009934</td>\n",
              "      <td>0.177704</td>\n",
              "      <td>0.019868</td>\n",
              "      <td>0.112583</td>\n",
              "      <td>0.020971</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>0.016556</td>\n",
              "      <td>0.220751</td>\n",
              "      <td>0.041943</td>\n",
              "      <td>0.246137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>0.001178</td>\n",
              "      <td>0.001178</td>\n",
              "      <td>0.001178</td>\n",
              "      <td>0.002356</td>\n",
              "      <td>0.002356</td>\n",
              "      <td>0.004711</td>\n",
              "      <td>0.001178</td>\n",
              "      <td>0.001178</td>\n",
              "      <td>0.003534</td>\n",
              "      <td>0.003534</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014134</td>\n",
              "      <td>0.190813</td>\n",
              "      <td>0.018846</td>\n",
              "      <td>0.117786</td>\n",
              "      <td>0.035336</td>\n",
              "      <td>0.221437</td>\n",
              "      <td>0.020024</td>\n",
              "      <td>0.203769</td>\n",
              "      <td>0.022379</td>\n",
              "      <td>0.244994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.002220</td>\n",
              "      <td>0.004440</td>\n",
              "      <td>0.002220</td>\n",
              "      <td>0.002220</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.002220</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007769</td>\n",
              "      <td>0.194229</td>\n",
              "      <td>0.014428</td>\n",
              "      <td>0.130966</td>\n",
              "      <td>0.028857</td>\n",
              "      <td>0.216426</td>\n",
              "      <td>0.022198</td>\n",
              "      <td>0.236404</td>\n",
              "      <td>0.031077</td>\n",
              "      <td>0.240844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.002361</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.002361</td>\n",
              "      <td>0.003542</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014168</td>\n",
              "      <td>0.193625</td>\n",
              "      <td>0.022432</td>\n",
              "      <td>0.128689</td>\n",
              "      <td>0.041322</td>\n",
              "      <td>0.224321</td>\n",
              "      <td>0.024793</td>\n",
              "      <td>0.232586</td>\n",
              "      <td>0.025974</td>\n",
              "      <td>0.204250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.003584</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.003584</td>\n",
              "      <td>0.002389</td>\n",
              "      <td>0.002389</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013142</td>\n",
              "      <td>0.197133</td>\n",
              "      <td>0.014337</td>\n",
              "      <td>0.112306</td>\n",
              "      <td>0.020311</td>\n",
              "      <td>0.199522</td>\n",
              "      <td>0.021505</td>\n",
              "      <td>0.207885</td>\n",
              "      <td>0.026284</td>\n",
              "      <td>0.235364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>0.001122</td>\n",
              "      <td>0.001122</td>\n",
              "      <td>0.001122</td>\n",
              "      <td>0.001122</td>\n",
              "      <td>0.004489</td>\n",
              "      <td>0.005612</td>\n",
              "      <td>0.001122</td>\n",
              "      <td>0.002245</td>\n",
              "      <td>0.001122</td>\n",
              "      <td>0.002245</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008979</td>\n",
              "      <td>0.168350</td>\n",
              "      <td>0.022447</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>0.023569</td>\n",
              "      <td>0.213244</td>\n",
              "      <td>0.020202</td>\n",
              "      <td>0.252525</td>\n",
              "      <td>0.023569</td>\n",
              "      <td>0.213244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>0.001078</td>\n",
              "      <td>0.001078</td>\n",
              "      <td>0.001078</td>\n",
              "      <td>0.001078</td>\n",
              "      <td>0.001078</td>\n",
              "      <td>0.002155</td>\n",
              "      <td>0.001078</td>\n",
              "      <td>0.003233</td>\n",
              "      <td>0.001078</td>\n",
              "      <td>0.002155</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011853</td>\n",
              "      <td>0.213362</td>\n",
              "      <td>0.012931</td>\n",
              "      <td>0.110991</td>\n",
              "      <td>0.025862</td>\n",
              "      <td>0.209052</td>\n",
              "      <td>0.025862</td>\n",
              "      <td>0.211207</td>\n",
              "      <td>0.033405</td>\n",
              "      <td>0.234914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.001183</td>\n",
              "      <td>0.001183</td>\n",
              "      <td>0.001183</td>\n",
              "      <td>0.001183</td>\n",
              "      <td>0.001183</td>\n",
              "      <td>0.002367</td>\n",
              "      <td>0.001183</td>\n",
              "      <td>0.003550</td>\n",
              "      <td>0.002367</td>\n",
              "      <td>0.003550</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014201</td>\n",
              "      <td>0.207101</td>\n",
              "      <td>0.015385</td>\n",
              "      <td>0.123077</td>\n",
              "      <td>0.026036</td>\n",
              "      <td>0.221302</td>\n",
              "      <td>0.023669</td>\n",
              "      <td>0.228402</td>\n",
              "      <td>0.035503</td>\n",
              "      <td>0.249704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0.001203</td>\n",
              "      <td>0.001203</td>\n",
              "      <td>0.001203</td>\n",
              "      <td>0.001203</td>\n",
              "      <td>0.006017</td>\n",
              "      <td>0.009627</td>\n",
              "      <td>0.001203</td>\n",
              "      <td>0.001203</td>\n",
              "      <td>0.001203</td>\n",
              "      <td>0.001203</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010830</td>\n",
              "      <td>0.219013</td>\n",
              "      <td>0.018051</td>\n",
              "      <td>0.132371</td>\n",
              "      <td>0.032491</td>\n",
              "      <td>0.245487</td>\n",
              "      <td>0.019254</td>\n",
              "      <td>0.234657</td>\n",
              "      <td>0.022864</td>\n",
              "      <td>0.226233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.001236</td>\n",
              "      <td>0.001236</td>\n",
              "      <td>0.001236</td>\n",
              "      <td>0.001236</td>\n",
              "      <td>0.001236</td>\n",
              "      <td>0.003708</td>\n",
              "      <td>0.002472</td>\n",
              "      <td>0.002472</td>\n",
              "      <td>0.002472</td>\n",
              "      <td>0.001236</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019778</td>\n",
              "      <td>0.203956</td>\n",
              "      <td>0.019778</td>\n",
              "      <td>0.126082</td>\n",
              "      <td>0.028430</td>\n",
              "      <td>0.233622</td>\n",
              "      <td>0.025958</td>\n",
              "      <td>0.244747</td>\n",
              "      <td>0.023486</td>\n",
              "      <td>0.208900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.010390</td>\n",
              "      <td>0.002597</td>\n",
              "      <td>0.002597</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.002597</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010390</td>\n",
              "      <td>0.172727</td>\n",
              "      <td>0.018182</td>\n",
              "      <td>0.107792</td>\n",
              "      <td>0.023377</td>\n",
              "      <td>0.220779</td>\n",
              "      <td>0.025974</td>\n",
              "      <td>0.223377</td>\n",
              "      <td>0.035065</td>\n",
              "      <td>0.249351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.012685</td>\n",
              "      <td>0.014799</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.002114</td>\n",
              "      <td>0.003171</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014799</td>\n",
              "      <td>0.195560</td>\n",
              "      <td>0.021142</td>\n",
              "      <td>0.133192</td>\n",
              "      <td>0.019027</td>\n",
              "      <td>0.228330</td>\n",
              "      <td>0.023256</td>\n",
              "      <td>0.281184</td>\n",
              "      <td>0.012685</td>\n",
              "      <td>0.213531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>0.001325</td>\n",
              "      <td>0.001325</td>\n",
              "      <td>0.001325</td>\n",
              "      <td>0.001325</td>\n",
              "      <td>0.019868</td>\n",
              "      <td>0.021192</td>\n",
              "      <td>0.001325</td>\n",
              "      <td>0.001325</td>\n",
              "      <td>0.001325</td>\n",
              "      <td>0.001325</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005298</td>\n",
              "      <td>0.185430</td>\n",
              "      <td>0.026490</td>\n",
              "      <td>0.125828</td>\n",
              "      <td>0.022517</td>\n",
              "      <td>0.214570</td>\n",
              "      <td>0.026490</td>\n",
              "      <td>0.235762</td>\n",
              "      <td>0.029139</td>\n",
              "      <td>0.233113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.007168</td>\n",
              "      <td>0.008363</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.003584</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.003584</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013142</td>\n",
              "      <td>0.182796</td>\n",
              "      <td>0.007168</td>\n",
              "      <td>0.121864</td>\n",
              "      <td>0.034648</td>\n",
              "      <td>0.216249</td>\n",
              "      <td>0.021505</td>\n",
              "      <td>0.256870</td>\n",
              "      <td>0.035842</td>\n",
              "      <td>0.243728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>0.001125</td>\n",
              "      <td>0.001125</td>\n",
              "      <td>0.001125</td>\n",
              "      <td>0.001125</td>\n",
              "      <td>0.022497</td>\n",
              "      <td>0.023622</td>\n",
              "      <td>0.002250</td>\n",
              "      <td>0.002250</td>\n",
              "      <td>0.001125</td>\n",
              "      <td>0.001125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012373</td>\n",
              "      <td>0.191226</td>\n",
              "      <td>0.015748</td>\n",
              "      <td>0.105737</td>\n",
              "      <td>0.033746</td>\n",
              "      <td>0.201350</td>\n",
              "      <td>0.041620</td>\n",
              "      <td>0.236220</td>\n",
              "      <td>0.026997</td>\n",
              "      <td>0.218223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.001096</td>\n",
              "      <td>0.001096</td>\n",
              "      <td>0.002193</td>\n",
              "      <td>0.004386</td>\n",
              "      <td>0.015351</td>\n",
              "      <td>0.016447</td>\n",
              "      <td>0.002193</td>\n",
              "      <td>0.002193</td>\n",
              "      <td>0.001096</td>\n",
              "      <td>0.002193</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008772</td>\n",
              "      <td>0.192982</td>\n",
              "      <td>0.008772</td>\n",
              "      <td>0.116228</td>\n",
              "      <td>0.020833</td>\n",
              "      <td>0.218202</td>\n",
              "      <td>0.032895</td>\n",
              "      <td>0.237939</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>0.223684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.001227</td>\n",
              "      <td>0.001227</td>\n",
              "      <td>0.001227</td>\n",
              "      <td>0.001227</td>\n",
              "      <td>0.013497</td>\n",
              "      <td>0.017178</td>\n",
              "      <td>0.002454</td>\n",
              "      <td>0.003681</td>\n",
              "      <td>0.001227</td>\n",
              "      <td>0.001227</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012270</td>\n",
              "      <td>0.179141</td>\n",
              "      <td>0.019632</td>\n",
              "      <td>0.120245</td>\n",
              "      <td>0.018405</td>\n",
              "      <td>0.212270</td>\n",
              "      <td>0.026994</td>\n",
              "      <td>0.226994</td>\n",
              "      <td>0.024540</td>\n",
              "      <td>0.239264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.014428</td>\n",
              "      <td>0.017758</td>\n",
              "      <td>0.002220</td>\n",
              "      <td>0.003330</td>\n",
              "      <td>0.002220</td>\n",
              "      <td>0.004440</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017758</td>\n",
              "      <td>0.189789</td>\n",
              "      <td>0.017758</td>\n",
              "      <td>0.106548</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.200888</td>\n",
              "      <td>0.017758</td>\n",
              "      <td>0.219756</td>\n",
              "      <td>0.026637</td>\n",
              "      <td>0.233074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.018888</td>\n",
              "      <td>0.020986</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.003148</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.003148</td>\n",
              "      <td>...</td>\n",
              "      <td>0.020986</td>\n",
              "      <td>0.183631</td>\n",
              "      <td>0.012592</td>\n",
              "      <td>0.125918</td>\n",
              "      <td>0.025184</td>\n",
              "      <td>0.208814</td>\n",
              "      <td>0.030430</td>\n",
              "      <td>0.251836</td>\n",
              "      <td>0.028332</td>\n",
              "      <td>0.236097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.001131</td>\n",
              "      <td>0.001131</td>\n",
              "      <td>0.002262</td>\n",
              "      <td>0.002262</td>\n",
              "      <td>0.028281</td>\n",
              "      <td>0.031674</td>\n",
              "      <td>0.001131</td>\n",
              "      <td>0.002262</td>\n",
              "      <td>0.002262</td>\n",
              "      <td>0.002262</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.184389</td>\n",
              "      <td>0.012443</td>\n",
              "      <td>0.111991</td>\n",
              "      <td>0.023756</td>\n",
              "      <td>0.210407</td>\n",
              "      <td>0.015837</td>\n",
              "      <td>0.223982</td>\n",
              "      <td>0.027149</td>\n",
              "      <td>0.259050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>0.001289</td>\n",
              "      <td>0.001289</td>\n",
              "      <td>0.001289</td>\n",
              "      <td>0.001289</td>\n",
              "      <td>0.029639</td>\n",
              "      <td>0.029639</td>\n",
              "      <td>0.002577</td>\n",
              "      <td>0.002577</td>\n",
              "      <td>0.001289</td>\n",
              "      <td>0.002577</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014175</td>\n",
              "      <td>0.185567</td>\n",
              "      <td>0.015464</td>\n",
              "      <td>0.135309</td>\n",
              "      <td>0.025773</td>\n",
              "      <td>0.217784</td>\n",
              "      <td>0.025773</td>\n",
              "      <td>0.253866</td>\n",
              "      <td>0.023196</td>\n",
              "      <td>0.213918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.001182</td>\n",
              "      <td>0.001182</td>\n",
              "      <td>0.001182</td>\n",
              "      <td>0.001182</td>\n",
              "      <td>0.026005</td>\n",
              "      <td>0.027187</td>\n",
              "      <td>0.002364</td>\n",
              "      <td>0.002364</td>\n",
              "      <td>0.001182</td>\n",
              "      <td>0.002364</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013002</td>\n",
              "      <td>0.202128</td>\n",
              "      <td>0.014184</td>\n",
              "      <td>0.127660</td>\n",
              "      <td>0.024823</td>\n",
              "      <td>0.198582</td>\n",
              "      <td>0.026005</td>\n",
              "      <td>0.244681</td>\n",
              "      <td>0.034279</td>\n",
              "      <td>0.230496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.002581</td>\n",
              "      <td>0.033548</td>\n",
              "      <td>0.036129</td>\n",
              "      <td>0.002581</td>\n",
              "      <td>0.002581</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014194</td>\n",
              "      <td>0.209032</td>\n",
              "      <td>0.012903</td>\n",
              "      <td>0.114839</td>\n",
              "      <td>0.023226</td>\n",
              "      <td>0.210323</td>\n",
              "      <td>0.024516</td>\n",
              "      <td>0.246452</td>\n",
              "      <td>0.027097</td>\n",
              "      <td>0.240000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>0.001072</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>0.028939</td>\n",
              "      <td>0.030011</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>0.003215</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013934</td>\n",
              "      <td>0.196141</td>\n",
              "      <td>0.013934</td>\n",
              "      <td>0.128617</td>\n",
              "      <td>0.012862</td>\n",
              "      <td>0.202572</td>\n",
              "      <td>0.024652</td>\n",
              "      <td>0.218650</td>\n",
              "      <td>0.034298</td>\n",
              "      <td>0.233655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.001259</td>\n",
              "      <td>0.001259</td>\n",
              "      <td>0.001259</td>\n",
              "      <td>0.001259</td>\n",
              "      <td>0.040302</td>\n",
              "      <td>0.042821</td>\n",
              "      <td>0.001259</td>\n",
              "      <td>0.002519</td>\n",
              "      <td>0.001259</td>\n",
              "      <td>0.003778</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016373</td>\n",
              "      <td>0.211587</td>\n",
              "      <td>0.017632</td>\n",
              "      <td>0.136020</td>\n",
              "      <td>0.016373</td>\n",
              "      <td>0.214106</td>\n",
              "      <td>0.030227</td>\n",
              "      <td>0.239295</td>\n",
              "      <td>0.030227</td>\n",
              "      <td>0.229219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.001120</td>\n",
              "      <td>0.001120</td>\n",
              "      <td>0.003359</td>\n",
              "      <td>0.003359</td>\n",
              "      <td>0.044793</td>\n",
              "      <td>0.050392</td>\n",
              "      <td>0.001120</td>\n",
              "      <td>0.001120</td>\n",
              "      <td>0.001120</td>\n",
              "      <td>0.001120</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010078</td>\n",
              "      <td>0.188130</td>\n",
              "      <td>0.007839</td>\n",
              "      <td>0.124300</td>\n",
              "      <td>0.024636</td>\n",
              "      <td>0.226204</td>\n",
              "      <td>0.029115</td>\n",
              "      <td>0.265398</td>\n",
              "      <td>0.019037</td>\n",
              "      <td>0.231803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.001147</td>\n",
              "      <td>0.001147</td>\n",
              "      <td>0.001147</td>\n",
              "      <td>0.001147</td>\n",
              "      <td>0.042431</td>\n",
              "      <td>0.043578</td>\n",
              "      <td>0.001147</td>\n",
              "      <td>0.002294</td>\n",
              "      <td>0.001147</td>\n",
              "      <td>0.002294</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010321</td>\n",
              "      <td>0.180046</td>\n",
              "      <td>0.021789</td>\n",
              "      <td>0.141055</td>\n",
              "      <td>0.020642</td>\n",
              "      <td>0.191514</td>\n",
              "      <td>0.022936</td>\n",
              "      <td>0.237385</td>\n",
              "      <td>0.017202</td>\n",
              "      <td>0.208716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.001087</td>\n",
              "      <td>0.001087</td>\n",
              "      <td>0.002174</td>\n",
              "      <td>0.002174</td>\n",
              "      <td>0.051087</td>\n",
              "      <td>0.051087</td>\n",
              "      <td>0.002174</td>\n",
              "      <td>0.004348</td>\n",
              "      <td>0.002174</td>\n",
              "      <td>0.003261</td>\n",
              "      <td>...</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.203261</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.113043</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.215217</td>\n",
              "      <td>0.022826</td>\n",
              "      <td>0.239130</td>\n",
              "      <td>0.031522</td>\n",
              "      <td>0.241304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.003472</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.033565</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.003472</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.003472</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012731</td>\n",
              "      <td>0.189815</td>\n",
              "      <td>0.015046</td>\n",
              "      <td>0.119213</td>\n",
              "      <td>0.032407</td>\n",
              "      <td>0.228009</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.223380</td>\n",
              "      <td>0.037037</td>\n",
              "      <td>0.231481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>0.001112</td>\n",
              "      <td>0.001112</td>\n",
              "      <td>0.001112</td>\n",
              "      <td>0.001112</td>\n",
              "      <td>0.040044</td>\n",
              "      <td>0.043382</td>\n",
              "      <td>0.001112</td>\n",
              "      <td>0.003337</td>\n",
              "      <td>0.002225</td>\n",
              "      <td>0.004449</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015573</td>\n",
              "      <td>0.171301</td>\n",
              "      <td>0.012236</td>\n",
              "      <td>0.104561</td>\n",
              "      <td>0.026696</td>\n",
              "      <td>0.209121</td>\n",
              "      <td>0.030033</td>\n",
              "      <td>0.228031</td>\n",
              "      <td>0.022247</td>\n",
              "      <td>0.233593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.002090</td>\n",
              "      <td>0.002090</td>\n",
              "      <td>0.037618</td>\n",
              "      <td>0.039707</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.003135</td>\n",
              "      <td>0.003135</td>\n",
              "      <td>...</td>\n",
              "      <td>0.021944</td>\n",
              "      <td>0.190178</td>\n",
              "      <td>0.016719</td>\n",
              "      <td>0.143156</td>\n",
              "      <td>0.019854</td>\n",
              "      <td>0.211076</td>\n",
              "      <td>0.032393</td>\n",
              "      <td>0.257053</td>\n",
              "      <td>0.025078</td>\n",
              "      <td>0.239289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.041475</td>\n",
              "      <td>0.048387</td>\n",
              "      <td>0.002304</td>\n",
              "      <td>0.003456</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.003456</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017281</td>\n",
              "      <td>0.193548</td>\n",
              "      <td>0.013825</td>\n",
              "      <td>0.116359</td>\n",
              "      <td>0.024194</td>\n",
              "      <td>0.210829</td>\n",
              "      <td>0.017281</td>\n",
              "      <td>0.228111</td>\n",
              "      <td>0.036866</td>\n",
              "      <td>0.256912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.001248</td>\n",
              "      <td>0.002497</td>\n",
              "      <td>0.001248</td>\n",
              "      <td>0.001248</td>\n",
              "      <td>0.046192</td>\n",
              "      <td>0.051186</td>\n",
              "      <td>0.001248</td>\n",
              "      <td>0.002497</td>\n",
              "      <td>0.002497</td>\n",
              "      <td>0.002497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011236</td>\n",
              "      <td>0.196005</td>\n",
              "      <td>0.024969</td>\n",
              "      <td>0.119850</td>\n",
              "      <td>0.026217</td>\n",
              "      <td>0.232210</td>\n",
              "      <td>0.026217</td>\n",
              "      <td>0.227216</td>\n",
              "      <td>0.033708</td>\n",
              "      <td>0.257179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.002315</td>\n",
              "      <td>0.040509</td>\n",
              "      <td>0.042824</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.002315</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016204</td>\n",
              "      <td>0.228009</td>\n",
              "      <td>0.015046</td>\n",
              "      <td>0.127315</td>\n",
              "      <td>0.021991</td>\n",
              "      <td>0.206019</td>\n",
              "      <td>0.027778</td>\n",
              "      <td>0.258102</td>\n",
              "      <td>0.038194</td>\n",
              "      <td>0.253472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.001253</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>0.002506</td>\n",
              "      <td>0.050125</td>\n",
              "      <td>0.051378</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>0.003759</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016291</td>\n",
              "      <td>0.189223</td>\n",
              "      <td>0.021303</td>\n",
              "      <td>0.111529</td>\n",
              "      <td>0.025063</td>\n",
              "      <td>0.228070</td>\n",
              "      <td>0.021303</td>\n",
              "      <td>0.200501</td>\n",
              "      <td>0.021303</td>\n",
              "      <td>0.216792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.001066</td>\n",
              "      <td>0.001066</td>\n",
              "      <td>0.002132</td>\n",
              "      <td>0.003198</td>\n",
              "      <td>0.045842</td>\n",
              "      <td>0.050107</td>\n",
              "      <td>0.001066</td>\n",
              "      <td>0.005330</td>\n",
              "      <td>0.002132</td>\n",
              "      <td>0.004264</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011727</td>\n",
              "      <td>0.195096</td>\n",
              "      <td>0.009595</td>\n",
              "      <td>0.122601</td>\n",
              "      <td>0.022388</td>\n",
              "      <td>0.202559</td>\n",
              "      <td>0.027719</td>\n",
              "      <td>0.205757</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.252665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.001044</td>\n",
              "      <td>0.001044</td>\n",
              "      <td>0.002088</td>\n",
              "      <td>0.003132</td>\n",
              "      <td>0.038622</td>\n",
              "      <td>0.040710</td>\n",
              "      <td>0.001044</td>\n",
              "      <td>0.003132</td>\n",
              "      <td>0.001044</td>\n",
              "      <td>0.001044</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008351</td>\n",
              "      <td>0.182672</td>\n",
              "      <td>0.013570</td>\n",
              "      <td>0.106472</td>\n",
              "      <td>0.025052</td>\n",
              "      <td>0.196242</td>\n",
              "      <td>0.022965</td>\n",
              "      <td>0.213987</td>\n",
              "      <td>0.029228</td>\n",
              "      <td>0.224426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.001116</td>\n",
              "      <td>0.001116</td>\n",
              "      <td>0.001116</td>\n",
              "      <td>0.002232</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>0.050223</td>\n",
              "      <td>0.002232</td>\n",
              "      <td>0.003348</td>\n",
              "      <td>0.002232</td>\n",
              "      <td>0.004464</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010045</td>\n",
              "      <td>0.196429</td>\n",
              "      <td>0.014509</td>\n",
              "      <td>0.130580</td>\n",
              "      <td>0.030134</td>\n",
              "      <td>0.218750</td>\n",
              "      <td>0.026786</td>\n",
              "      <td>0.228795</td>\n",
              "      <td>0.022321</td>\n",
              "      <td>0.204241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.001972</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.055227</td>\n",
              "      <td>0.064103</td>\n",
              "      <td>0.001972</td>\n",
              "      <td>0.004931</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.001972</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013807</td>\n",
              "      <td>0.205128</td>\n",
              "      <td>0.013807</td>\n",
              "      <td>0.113412</td>\n",
              "      <td>0.023669</td>\n",
              "      <td>0.218935</td>\n",
              "      <td>0.030572</td>\n",
              "      <td>0.258383</td>\n",
              "      <td>0.018738</td>\n",
              "      <td>0.210059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.001828</td>\n",
              "      <td>0.049360</td>\n",
              "      <td>0.053016</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.002742</td>\n",
              "      <td>0.002742</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015539</td>\n",
              "      <td>0.183729</td>\n",
              "      <td>0.007313</td>\n",
              "      <td>0.118830</td>\n",
              "      <td>0.021024</td>\n",
              "      <td>0.209324</td>\n",
              "      <td>0.027422</td>\n",
              "      <td>0.215722</td>\n",
              "      <td>0.022852</td>\n",
              "      <td>0.211152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>0.001081</td>\n",
              "      <td>0.001081</td>\n",
              "      <td>0.002162</td>\n",
              "      <td>0.002162</td>\n",
              "      <td>0.059459</td>\n",
              "      <td>0.064865</td>\n",
              "      <td>0.002162</td>\n",
              "      <td>0.002162</td>\n",
              "      <td>0.001081</td>\n",
              "      <td>0.003243</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017297</td>\n",
              "      <td>0.190270</td>\n",
              "      <td>0.010811</td>\n",
              "      <td>0.128649</td>\n",
              "      <td>0.018378</td>\n",
              "      <td>0.197838</td>\n",
              "      <td>0.023784</td>\n",
              "      <td>0.237838</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.244324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.002146</td>\n",
              "      <td>0.048283</td>\n",
              "      <td>0.050429</td>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.002146</td>\n",
              "      <td>0.002146</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008584</td>\n",
              "      <td>0.190987</td>\n",
              "      <td>0.017167</td>\n",
              "      <td>0.111588</td>\n",
              "      <td>0.019313</td>\n",
              "      <td>0.203863</td>\n",
              "      <td>0.020386</td>\n",
              "      <td>0.244635</td>\n",
              "      <td>0.034335</td>\n",
              "      <td>0.263948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.003820</td>\n",
              "      <td>0.051576</td>\n",
              "      <td>0.054441</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018147</td>\n",
              "      <td>0.197708</td>\n",
              "      <td>0.017192</td>\n",
              "      <td>0.133715</td>\n",
              "      <td>0.017192</td>\n",
              "      <td>0.185291</td>\n",
              "      <td>0.025788</td>\n",
              "      <td>0.246418</td>\n",
              "      <td>0.031519</td>\n",
              "      <td>0.246418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>0.001042</td>\n",
              "      <td>0.001042</td>\n",
              "      <td>0.002083</td>\n",
              "      <td>0.003125</td>\n",
              "      <td>0.039583</td>\n",
              "      <td>0.045833</td>\n",
              "      <td>0.003125</td>\n",
              "      <td>0.005208</td>\n",
              "      <td>0.003125</td>\n",
              "      <td>0.003125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.019792</td>\n",
              "      <td>0.129167</td>\n",
              "      <td>0.022917</td>\n",
              "      <td>0.214583</td>\n",
              "      <td>0.029167</td>\n",
              "      <td>0.253125</td>\n",
              "      <td>0.021875</td>\n",
              "      <td>0.257292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>0.001091</td>\n",
              "      <td>0.001091</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>0.004362</td>\n",
              "      <td>0.053435</td>\n",
              "      <td>0.058888</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>0.006543</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009815</td>\n",
              "      <td>0.188659</td>\n",
              "      <td>0.016358</td>\n",
              "      <td>0.119956</td>\n",
              "      <td>0.020720</td>\n",
              "      <td>0.224646</td>\n",
              "      <td>0.029444</td>\n",
              "      <td>0.257361</td>\n",
              "      <td>0.030534</td>\n",
              "      <td>0.232279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>0.001186</td>\n",
              "      <td>0.001186</td>\n",
              "      <td>0.004745</td>\n",
              "      <td>0.002372</td>\n",
              "      <td>0.060498</td>\n",
              "      <td>0.068802</td>\n",
              "      <td>0.001186</td>\n",
              "      <td>0.002372</td>\n",
              "      <td>0.001186</td>\n",
              "      <td>0.005931</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009490</td>\n",
              "      <td>0.204033</td>\n",
              "      <td>0.013049</td>\n",
              "      <td>0.125741</td>\n",
              "      <td>0.027284</td>\n",
              "      <td>0.206406</td>\n",
              "      <td>0.020166</td>\n",
              "      <td>0.263345</td>\n",
              "      <td>0.027284</td>\n",
              "      <td>0.220641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>0.001103</td>\n",
              "      <td>0.001103</td>\n",
              "      <td>0.004410</td>\n",
              "      <td>0.005513</td>\n",
              "      <td>0.040794</td>\n",
              "      <td>0.051819</td>\n",
              "      <td>0.001103</td>\n",
              "      <td>0.002205</td>\n",
              "      <td>0.001103</td>\n",
              "      <td>0.003308</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015436</td>\n",
              "      <td>0.212789</td>\n",
              "      <td>0.018743</td>\n",
              "      <td>0.127894</td>\n",
              "      <td>0.023153</td>\n",
              "      <td>0.205072</td>\n",
              "      <td>0.038589</td>\n",
              "      <td>0.259096</td>\n",
              "      <td>0.033076</td>\n",
              "      <td>0.231533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>0.000989</td>\n",
              "      <td>0.000989</td>\n",
              "      <td>0.002967</td>\n",
              "      <td>0.003956</td>\n",
              "      <td>0.053412</td>\n",
              "      <td>0.057369</td>\n",
              "      <td>0.003956</td>\n",
              "      <td>0.003956</td>\n",
              "      <td>0.000989</td>\n",
              "      <td>0.002967</td>\n",
              "      <td>...</td>\n",
              "      <td>0.020772</td>\n",
              "      <td>0.196835</td>\n",
              "      <td>0.020772</td>\n",
              "      <td>0.118694</td>\n",
              "      <td>0.012859</td>\n",
              "      <td>0.208704</td>\n",
              "      <td>0.019782</td>\n",
              "      <td>0.225519</td>\n",
              "      <td>0.030663</td>\n",
              "      <td>0.249258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>0.001094</td>\n",
              "      <td>0.001094</td>\n",
              "      <td>0.006565</td>\n",
              "      <td>0.005470</td>\n",
              "      <td>0.056893</td>\n",
              "      <td>0.065646</td>\n",
              "      <td>0.002188</td>\n",
              "      <td>0.003282</td>\n",
              "      <td>0.003282</td>\n",
              "      <td>0.004376</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008753</td>\n",
              "      <td>0.221007</td>\n",
              "      <td>0.012035</td>\n",
              "      <td>0.125821</td>\n",
              "      <td>0.022976</td>\n",
              "      <td>0.200219</td>\n",
              "      <td>0.035011</td>\n",
              "      <td>0.234136</td>\n",
              "      <td>0.029540</td>\n",
              "      <td>0.233042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>0.001095</td>\n",
              "      <td>0.001095</td>\n",
              "      <td>0.005476</td>\n",
              "      <td>0.004381</td>\n",
              "      <td>0.052574</td>\n",
              "      <td>0.061336</td>\n",
              "      <td>0.002191</td>\n",
              "      <td>0.003286</td>\n",
              "      <td>0.001095</td>\n",
              "      <td>0.001095</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019715</td>\n",
              "      <td>0.176342</td>\n",
              "      <td>0.017525</td>\n",
              "      <td>0.122673</td>\n",
              "      <td>0.014239</td>\n",
              "      <td>0.219058</td>\n",
              "      <td>0.025192</td>\n",
              "      <td>0.236583</td>\n",
              "      <td>0.016429</td>\n",
              "      <td>0.223439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.004292</td>\n",
              "      <td>0.004292</td>\n",
              "      <td>0.072961</td>\n",
              "      <td>0.080472</td>\n",
              "      <td>0.002146</td>\n",
              "      <td>0.004292</td>\n",
              "      <td>0.002146</td>\n",
              "      <td>0.003219</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015021</td>\n",
              "      <td>0.182403</td>\n",
              "      <td>0.013948</td>\n",
              "      <td>0.097639</td>\n",
              "      <td>0.020386</td>\n",
              "      <td>0.234979</td>\n",
              "      <td>0.024678</td>\n",
              "      <td>0.215665</td>\n",
              "      <td>0.028970</td>\n",
              "      <td>0.217811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>0.000971</td>\n",
              "      <td>0.001942</td>\n",
              "      <td>0.005825</td>\n",
              "      <td>0.008738</td>\n",
              "      <td>0.057282</td>\n",
              "      <td>0.067961</td>\n",
              "      <td>0.005825</td>\n",
              "      <td>0.007767</td>\n",
              "      <td>0.001942</td>\n",
              "      <td>0.003883</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011650</td>\n",
              "      <td>0.196117</td>\n",
              "      <td>0.014563</td>\n",
              "      <td>0.118447</td>\n",
              "      <td>0.026214</td>\n",
              "      <td>0.207767</td>\n",
              "      <td>0.026214</td>\n",
              "      <td>0.239806</td>\n",
              "      <td>0.034951</td>\n",
              "      <td>0.244660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.012791</td>\n",
              "      <td>0.018605</td>\n",
              "      <td>0.065116</td>\n",
              "      <td>0.075581</td>\n",
              "      <td>0.002326</td>\n",
              "      <td>0.002326</td>\n",
              "      <td>0.003488</td>\n",
              "      <td>0.005814</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013953</td>\n",
              "      <td>0.213953</td>\n",
              "      <td>0.013953</td>\n",
              "      <td>0.148837</td>\n",
              "      <td>0.017442</td>\n",
              "      <td>0.197674</td>\n",
              "      <td>0.029070</td>\n",
              "      <td>0.229070</td>\n",
              "      <td>0.017442</td>\n",
              "      <td>0.236047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.011134</td>\n",
              "      <td>0.015182</td>\n",
              "      <td>0.059717</td>\n",
              "      <td>0.062753</td>\n",
              "      <td>0.003036</td>\n",
              "      <td>0.005061</td>\n",
              "      <td>0.002024</td>\n",
              "      <td>0.005061</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010121</td>\n",
              "      <td>0.209514</td>\n",
              "      <td>0.021255</td>\n",
              "      <td>0.141700</td>\n",
              "      <td>0.025304</td>\n",
              "      <td>0.212551</td>\n",
              "      <td>0.025304</td>\n",
              "      <td>0.223684</td>\n",
              "      <td>0.025304</td>\n",
              "      <td>0.239879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.009862</td>\n",
              "      <td>0.010848</td>\n",
              "      <td>0.071006</td>\n",
              "      <td>0.078895</td>\n",
              "      <td>0.001972</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>0.001972</td>\n",
              "      <td>0.001972</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012821</td>\n",
              "      <td>0.194280</td>\n",
              "      <td>0.011834</td>\n",
              "      <td>0.127219</td>\n",
              "      <td>0.018738</td>\n",
              "      <td>0.216963</td>\n",
              "      <td>0.021696</td>\n",
              "      <td>0.263314</td>\n",
              "      <td>0.028600</td>\n",
              "      <td>0.258383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>0.001030</td>\n",
              "      <td>0.001030</td>\n",
              "      <td>0.008239</td>\n",
              "      <td>0.009269</td>\n",
              "      <td>0.047374</td>\n",
              "      <td>0.052523</td>\n",
              "      <td>0.004119</td>\n",
              "      <td>0.007209</td>\n",
              "      <td>0.001030</td>\n",
              "      <td>0.001030</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017508</td>\n",
              "      <td>0.215242</td>\n",
              "      <td>0.018538</td>\n",
              "      <td>0.138002</td>\n",
              "      <td>0.026777</td>\n",
              "      <td>0.246138</td>\n",
              "      <td>0.028836</td>\n",
              "      <td>0.228630</td>\n",
              "      <td>0.039135</td>\n",
              "      <td>0.236869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>0.000973</td>\n",
              "      <td>0.000973</td>\n",
              "      <td>0.011673</td>\n",
              "      <td>0.015564</td>\n",
              "      <td>0.063230</td>\n",
              "      <td>0.064202</td>\n",
              "      <td>0.003891</td>\n",
              "      <td>0.005837</td>\n",
              "      <td>0.002918</td>\n",
              "      <td>0.003891</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012646</td>\n",
              "      <td>0.189689</td>\n",
              "      <td>0.011673</td>\n",
              "      <td>0.134241</td>\n",
              "      <td>0.017510</td>\n",
              "      <td>0.214008</td>\n",
              "      <td>0.023346</td>\n",
              "      <td>0.246109</td>\n",
              "      <td>0.029183</td>\n",
              "      <td>0.230545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>0.001101</td>\n",
              "      <td>0.001101</td>\n",
              "      <td>0.011013</td>\n",
              "      <td>0.017621</td>\n",
              "      <td>0.052863</td>\n",
              "      <td>0.057269</td>\n",
              "      <td>0.003304</td>\n",
              "      <td>0.005507</td>\n",
              "      <td>0.001101</td>\n",
              "      <td>0.006608</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016520</td>\n",
              "      <td>0.211454</td>\n",
              "      <td>0.018722</td>\n",
              "      <td>0.140969</td>\n",
              "      <td>0.016520</td>\n",
              "      <td>0.204846</td>\n",
              "      <td>0.022026</td>\n",
              "      <td>0.251101</td>\n",
              "      <td>0.026432</td>\n",
              "      <td>0.227974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>0.001071</td>\n",
              "      <td>0.001071</td>\n",
              "      <td>0.013919</td>\n",
              "      <td>0.018201</td>\n",
              "      <td>0.063169</td>\n",
              "      <td>0.072805</td>\n",
              "      <td>0.003212</td>\n",
              "      <td>0.004283</td>\n",
              "      <td>0.003212</td>\n",
              "      <td>0.004283</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007495</td>\n",
              "      <td>0.224839</td>\n",
              "      <td>0.014989</td>\n",
              "      <td>0.123126</td>\n",
              "      <td>0.026767</td>\n",
              "      <td>0.198073</td>\n",
              "      <td>0.022484</td>\n",
              "      <td>0.241970</td>\n",
              "      <td>0.028908</td>\n",
              "      <td>0.231263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>0.000845</td>\n",
              "      <td>0.000845</td>\n",
              "      <td>0.014370</td>\n",
              "      <td>0.022823</td>\n",
              "      <td>0.071851</td>\n",
              "      <td>0.083686</td>\n",
              "      <td>0.002536</td>\n",
              "      <td>0.007608</td>\n",
              "      <td>0.002536</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015216</td>\n",
              "      <td>0.177515</td>\n",
              "      <td>0.016906</td>\n",
              "      <td>0.140321</td>\n",
              "      <td>0.027895</td>\n",
              "      <td>0.221471</td>\n",
              "      <td>0.022823</td>\n",
              "      <td>0.240068</td>\n",
              "      <td>0.029586</td>\n",
              "      <td>0.226543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>0.000928</td>\n",
              "      <td>0.000928</td>\n",
              "      <td>0.011132</td>\n",
              "      <td>0.017625</td>\n",
              "      <td>0.051948</td>\n",
              "      <td>0.060297</td>\n",
              "      <td>0.009276</td>\n",
              "      <td>0.012059</td>\n",
              "      <td>0.001855</td>\n",
              "      <td>0.004638</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008349</td>\n",
              "      <td>0.188312</td>\n",
              "      <td>0.012059</td>\n",
              "      <td>0.099258</td>\n",
              "      <td>0.015770</td>\n",
              "      <td>0.246753</td>\n",
              "      <td>0.032468</td>\n",
              "      <td>0.248609</td>\n",
              "      <td>0.027829</td>\n",
              "      <td>0.235622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>0.000932</td>\n",
              "      <td>0.000932</td>\n",
              "      <td>0.014911</td>\n",
              "      <td>0.016775</td>\n",
              "      <td>0.058714</td>\n",
              "      <td>0.068034</td>\n",
              "      <td>0.001864</td>\n",
              "      <td>0.004660</td>\n",
              "      <td>0.001864</td>\n",
              "      <td>0.005592</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013979</td>\n",
              "      <td>0.182665</td>\n",
              "      <td>0.019571</td>\n",
              "      <td>0.124884</td>\n",
              "      <td>0.019571</td>\n",
              "      <td>0.200373</td>\n",
              "      <td>0.021435</td>\n",
              "      <td>0.216216</td>\n",
              "      <td>0.018639</td>\n",
              "      <td>0.209692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>0.000895</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>0.012534</td>\n",
              "      <td>0.015219</td>\n",
              "      <td>0.060877</td>\n",
              "      <td>0.068935</td>\n",
              "      <td>0.003581</td>\n",
              "      <td>0.008953</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>0.008057</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011638</td>\n",
              "      <td>0.181737</td>\n",
              "      <td>0.017905</td>\n",
              "      <td>0.117278</td>\n",
              "      <td>0.011638</td>\n",
              "      <td>0.201432</td>\n",
              "      <td>0.019696</td>\n",
              "      <td>0.240824</td>\n",
              "      <td>0.020591</td>\n",
              "      <td>0.222023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>0.000898</td>\n",
              "      <td>0.000898</td>\n",
              "      <td>0.017969</td>\n",
              "      <td>0.020665</td>\n",
              "      <td>0.069182</td>\n",
              "      <td>0.077269</td>\n",
              "      <td>0.005391</td>\n",
              "      <td>0.007188</td>\n",
              "      <td>0.002695</td>\n",
              "      <td>0.006289</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012579</td>\n",
              "      <td>0.203953</td>\n",
              "      <td>0.010782</td>\n",
              "      <td>0.126685</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.203055</td>\n",
              "      <td>0.013477</td>\n",
              "      <td>0.228212</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.227314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>0.000904</td>\n",
              "      <td>0.000904</td>\n",
              "      <td>0.009946</td>\n",
              "      <td>0.011754</td>\n",
              "      <td>0.074141</td>\n",
              "      <td>0.081374</td>\n",
              "      <td>0.003617</td>\n",
              "      <td>0.007233</td>\n",
              "      <td>0.002712</td>\n",
              "      <td>0.006329</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018083</td>\n",
              "      <td>0.190778</td>\n",
              "      <td>0.009946</td>\n",
              "      <td>0.132911</td>\n",
              "      <td>0.027125</td>\n",
              "      <td>0.199819</td>\n",
              "      <td>0.033454</td>\n",
              "      <td>0.243219</td>\n",
              "      <td>0.036166</td>\n",
              "      <td>0.233273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>0.000874</td>\n",
              "      <td>0.000874</td>\n",
              "      <td>0.007867</td>\n",
              "      <td>0.012238</td>\n",
              "      <td>0.076049</td>\n",
              "      <td>0.084790</td>\n",
              "      <td>0.003497</td>\n",
              "      <td>0.004371</td>\n",
              "      <td>0.002622</td>\n",
              "      <td>0.002622</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018357</td>\n",
              "      <td>0.204545</td>\n",
              "      <td>0.015734</td>\n",
              "      <td>0.124126</td>\n",
              "      <td>0.020105</td>\n",
              "      <td>0.205420</td>\n",
              "      <td>0.025350</td>\n",
              "      <td>0.222902</td>\n",
              "      <td>0.027098</td>\n",
              "      <td>0.235140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>0.000842</td>\n",
              "      <td>0.000842</td>\n",
              "      <td>0.014322</td>\n",
              "      <td>0.019377</td>\n",
              "      <td>0.063184</td>\n",
              "      <td>0.075821</td>\n",
              "      <td>0.004212</td>\n",
              "      <td>0.006740</td>\n",
              "      <td>0.000842</td>\n",
              "      <td>0.005055</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010952</td>\n",
              "      <td>0.177759</td>\n",
              "      <td>0.014322</td>\n",
              "      <td>0.128896</td>\n",
              "      <td>0.022746</td>\n",
              "      <td>0.216512</td>\n",
              "      <td>0.022746</td>\n",
              "      <td>0.260320</td>\n",
              "      <td>0.028644</td>\n",
              "      <td>0.221567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>0.000894</td>\n",
              "      <td>0.001787</td>\n",
              "      <td>0.013405</td>\n",
              "      <td>0.019660</td>\n",
              "      <td>0.063450</td>\n",
              "      <td>0.075067</td>\n",
              "      <td>0.006256</td>\n",
              "      <td>0.011618</td>\n",
              "      <td>0.002681</td>\n",
              "      <td>0.006256</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016979</td>\n",
              "      <td>0.194817</td>\n",
              "      <td>0.011618</td>\n",
              "      <td>0.123324</td>\n",
              "      <td>0.020554</td>\n",
              "      <td>0.202860</td>\n",
              "      <td>0.028597</td>\n",
              "      <td>0.228776</td>\n",
              "      <td>0.021448</td>\n",
              "      <td>0.242181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>0.000894</td>\n",
              "      <td>0.000894</td>\n",
              "      <td>0.018767</td>\n",
              "      <td>0.023235</td>\n",
              "      <td>0.063450</td>\n",
              "      <td>0.070599</td>\n",
              "      <td>0.009830</td>\n",
              "      <td>0.012511</td>\n",
              "      <td>0.004468</td>\n",
              "      <td>0.009830</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008937</td>\n",
              "      <td>0.191242</td>\n",
              "      <td>0.012511</td>\n",
              "      <td>0.133155</td>\n",
              "      <td>0.025916</td>\n",
              "      <td>0.219839</td>\n",
              "      <td>0.025022</td>\n",
              "      <td>0.237712</td>\n",
              "      <td>0.033065</td>\n",
              "      <td>0.235031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.018287</td>\n",
              "      <td>0.025987</td>\n",
              "      <td>0.069297</td>\n",
              "      <td>0.087584</td>\n",
              "      <td>0.003850</td>\n",
              "      <td>0.006737</td>\n",
              "      <td>0.003850</td>\n",
              "      <td>0.003850</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013474</td>\n",
              "      <td>0.203080</td>\n",
              "      <td>0.013474</td>\n",
              "      <td>0.121270</td>\n",
              "      <td>0.024062</td>\n",
              "      <td>0.243503</td>\n",
              "      <td>0.018287</td>\n",
              "      <td>0.238691</td>\n",
              "      <td>0.040423</td>\n",
              "      <td>0.235804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>0.000935</td>\n",
              "      <td>0.000935</td>\n",
              "      <td>0.008419</td>\n",
              "      <td>0.014032</td>\n",
              "      <td>0.074836</td>\n",
              "      <td>0.083255</td>\n",
              "      <td>0.007484</td>\n",
              "      <td>0.011225</td>\n",
              "      <td>0.003742</td>\n",
              "      <td>0.004677</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017774</td>\n",
              "      <td>0.193639</td>\n",
              "      <td>0.012161</td>\n",
              "      <td>0.100094</td>\n",
              "      <td>0.024322</td>\n",
              "      <td>0.222638</td>\n",
              "      <td>0.013096</td>\n",
              "      <td>0.217025</td>\n",
              "      <td>0.032741</td>\n",
              "      <td>0.231057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>0.000823</td>\n",
              "      <td>0.000823</td>\n",
              "      <td>0.017284</td>\n",
              "      <td>0.023868</td>\n",
              "      <td>0.065844</td>\n",
              "      <td>0.071605</td>\n",
              "      <td>0.004938</td>\n",
              "      <td>0.008230</td>\n",
              "      <td>0.002469</td>\n",
              "      <td>0.004938</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013992</td>\n",
              "      <td>0.190123</td>\n",
              "      <td>0.009877</td>\n",
              "      <td>0.114403</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.200823</td>\n",
              "      <td>0.024691</td>\n",
              "      <td>0.240329</td>\n",
              "      <td>0.032099</td>\n",
              "      <td>0.222222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>0.000918</td>\n",
              "      <td>0.000918</td>\n",
              "      <td>0.019284</td>\n",
              "      <td>0.024793</td>\n",
              "      <td>0.083563</td>\n",
              "      <td>0.092746</td>\n",
              "      <td>0.004591</td>\n",
              "      <td>0.006428</td>\n",
              "      <td>0.003673</td>\n",
              "      <td>0.008264</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011938</td>\n",
              "      <td>0.203857</td>\n",
              "      <td>0.015611</td>\n",
              "      <td>0.117539</td>\n",
              "      <td>0.017447</td>\n",
              "      <td>0.185491</td>\n",
              "      <td>0.028466</td>\n",
              "      <td>0.232323</td>\n",
              "      <td>0.028466</td>\n",
              "      <td>0.226814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>0.000829</td>\n",
              "      <td>0.001657</td>\n",
              "      <td>0.022370</td>\n",
              "      <td>0.033140</td>\n",
              "      <td>0.085336</td>\n",
              "      <td>0.096935</td>\n",
              "      <td>0.005800</td>\n",
              "      <td>0.013256</td>\n",
              "      <td>0.002486</td>\n",
              "      <td>0.006628</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014085</td>\n",
              "      <td>0.196355</td>\n",
              "      <td>0.012428</td>\n",
              "      <td>0.107705</td>\n",
              "      <td>0.034797</td>\n",
              "      <td>0.217067</td>\n",
              "      <td>0.029826</td>\n",
              "      <td>0.229495</td>\n",
              "      <td>0.026512</td>\n",
              "      <td>0.231152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>0.000818</td>\n",
              "      <td>0.000818</td>\n",
              "      <td>0.009820</td>\n",
              "      <td>0.016367</td>\n",
              "      <td>0.050736</td>\n",
              "      <td>0.063830</td>\n",
              "      <td>0.009002</td>\n",
              "      <td>0.012275</td>\n",
              "      <td>0.003273</td>\n",
              "      <td>0.008183</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009002</td>\n",
              "      <td>0.192308</td>\n",
              "      <td>0.016367</td>\n",
              "      <td>0.116203</td>\n",
              "      <td>0.026187</td>\n",
              "      <td>0.220131</td>\n",
              "      <td>0.031097</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.025368</td>\n",
              "      <td>0.230769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>0.000883</td>\n",
              "      <td>0.000883</td>\n",
              "      <td>0.015018</td>\n",
              "      <td>0.022085</td>\n",
              "      <td>0.072438</td>\n",
              "      <td>0.080389</td>\n",
              "      <td>0.005300</td>\n",
              "      <td>0.007951</td>\n",
              "      <td>0.003534</td>\n",
              "      <td>0.004417</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014134</td>\n",
              "      <td>0.193463</td>\n",
              "      <td>0.016784</td>\n",
              "      <td>0.115724</td>\n",
              "      <td>0.025618</td>\n",
              "      <td>0.207597</td>\n",
              "      <td>0.027385</td>\n",
              "      <td>0.246466</td>\n",
              "      <td>0.025618</td>\n",
              "      <td>0.227915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>0.000860</td>\n",
              "      <td>0.000860</td>\n",
              "      <td>0.024076</td>\n",
              "      <td>0.037833</td>\n",
              "      <td>0.072227</td>\n",
              "      <td>0.088564</td>\n",
              "      <td>0.007739</td>\n",
              "      <td>0.014617</td>\n",
              "      <td>0.004299</td>\n",
              "      <td>0.007739</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011178</td>\n",
              "      <td>0.182287</td>\n",
              "      <td>0.010318</td>\n",
              "      <td>0.124678</td>\n",
              "      <td>0.018917</td>\n",
              "      <td>0.176268</td>\n",
              "      <td>0.021496</td>\n",
              "      <td>0.249355</td>\n",
              "      <td>0.023216</td>\n",
              "      <td>0.227859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>0.001692</td>\n",
              "      <td>0.001692</td>\n",
              "      <td>0.019459</td>\n",
              "      <td>0.026227</td>\n",
              "      <td>0.060914</td>\n",
              "      <td>0.072758</td>\n",
              "      <td>0.007614</td>\n",
              "      <td>0.010152</td>\n",
              "      <td>0.002538</td>\n",
              "      <td>0.005076</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011844</td>\n",
              "      <td>0.187817</td>\n",
              "      <td>0.006768</td>\n",
              "      <td>0.116751</td>\n",
              "      <td>0.016920</td>\n",
              "      <td>0.204738</td>\n",
              "      <td>0.027073</td>\n",
              "      <td>0.253807</td>\n",
              "      <td>0.024535</td>\n",
              "      <td>0.202200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>0.000856</td>\n",
              "      <td>0.000856</td>\n",
              "      <td>0.017123</td>\n",
              "      <td>0.027397</td>\n",
              "      <td>0.057363</td>\n",
              "      <td>0.068493</td>\n",
              "      <td>0.006849</td>\n",
              "      <td>0.011986</td>\n",
              "      <td>0.003425</td>\n",
              "      <td>0.006849</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010274</td>\n",
              "      <td>0.166952</td>\n",
              "      <td>0.011986</td>\n",
              "      <td>0.133562</td>\n",
              "      <td>0.026541</td>\n",
              "      <td>0.227740</td>\n",
              "      <td>0.019692</td>\n",
              "      <td>0.233733</td>\n",
              "      <td>0.019692</td>\n",
              "      <td>0.221747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>0.000773</td>\n",
              "      <td>0.000773</td>\n",
              "      <td>0.017015</td>\n",
              "      <td>0.023975</td>\n",
              "      <td>0.064192</td>\n",
              "      <td>0.078886</td>\n",
              "      <td>0.009281</td>\n",
              "      <td>0.016241</td>\n",
              "      <td>0.003094</td>\n",
              "      <td>0.009281</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015468</td>\n",
              "      <td>0.217324</td>\n",
              "      <td>0.017788</td>\n",
              "      <td>0.124517</td>\n",
              "      <td>0.019335</td>\n",
              "      <td>0.242846</td>\n",
              "      <td>0.020882</td>\n",
              "      <td>0.241299</td>\n",
              "      <td>0.024749</td>\n",
              "      <td>0.238979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>0.000845</td>\n",
              "      <td>0.000845</td>\n",
              "      <td>0.021133</td>\n",
              "      <td>0.025359</td>\n",
              "      <td>0.065089</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.009298</td>\n",
              "      <td>0.015216</td>\n",
              "      <td>0.001691</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010144</td>\n",
              "      <td>0.180051</td>\n",
              "      <td>0.009298</td>\n",
              "      <td>0.126796</td>\n",
              "      <td>0.022823</td>\n",
              "      <td>0.224007</td>\n",
              "      <td>0.025359</td>\n",
              "      <td>0.221471</td>\n",
              "      <td>0.029586</td>\n",
              "      <td>0.222316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.016904</td>\n",
              "      <td>0.028470</td>\n",
              "      <td>0.068505</td>\n",
              "      <td>0.075623</td>\n",
              "      <td>0.007117</td>\n",
              "      <td>0.009786</td>\n",
              "      <td>0.001779</td>\n",
              "      <td>0.008007</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011566</td>\n",
              "      <td>0.191281</td>\n",
              "      <td>0.008897</td>\n",
              "      <td>0.120996</td>\n",
              "      <td>0.022242</td>\n",
              "      <td>0.208185</td>\n",
              "      <td>0.024021</td>\n",
              "      <td>0.233986</td>\n",
              "      <td>0.025801</td>\n",
              "      <td>0.248221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>0.000848</td>\n",
              "      <td>0.000848</td>\n",
              "      <td>0.019508</td>\n",
              "      <td>0.027142</td>\n",
              "      <td>0.076336</td>\n",
              "      <td>0.087362</td>\n",
              "      <td>0.009330</td>\n",
              "      <td>0.012723</td>\n",
              "      <td>0.004241</td>\n",
              "      <td>0.007634</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011026</td>\n",
              "      <td>0.181510</td>\n",
              "      <td>0.013571</td>\n",
              "      <td>0.127226</td>\n",
              "      <td>0.023749</td>\n",
              "      <td>0.220526</td>\n",
              "      <td>0.018660</td>\n",
              "      <td>0.233249</td>\n",
              "      <td>0.017812</td>\n",
              "      <td>0.234945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.015472</td>\n",
              "      <td>0.028502</td>\n",
              "      <td>0.073290</td>\n",
              "      <td>0.078176</td>\n",
              "      <td>0.009772</td>\n",
              "      <td>0.014658</td>\n",
              "      <td>0.004886</td>\n",
              "      <td>0.004072</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011401</td>\n",
              "      <td>0.194625</td>\n",
              "      <td>0.009772</td>\n",
              "      <td>0.134365</td>\n",
              "      <td>0.021987</td>\n",
              "      <td>0.221498</td>\n",
              "      <td>0.026059</td>\n",
              "      <td>0.232085</td>\n",
              "      <td>0.026873</td>\n",
              "      <td>0.241042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>0.000812</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>0.015422</td>\n",
              "      <td>0.022727</td>\n",
              "      <td>0.073864</td>\n",
              "      <td>0.088474</td>\n",
              "      <td>0.012175</td>\n",
              "      <td>0.019481</td>\n",
              "      <td>0.008117</td>\n",
              "      <td>0.012987</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009740</td>\n",
              "      <td>0.176136</td>\n",
              "      <td>0.012987</td>\n",
              "      <td>0.127435</td>\n",
              "      <td>0.023539</td>\n",
              "      <td>0.202922</td>\n",
              "      <td>0.025974</td>\n",
              "      <td>0.255682</td>\n",
              "      <td>0.024351</td>\n",
              "      <td>0.213474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>0.000826</td>\n",
              "      <td>0.000826</td>\n",
              "      <td>0.016515</td>\n",
              "      <td>0.025599</td>\n",
              "      <td>0.072667</td>\n",
              "      <td>0.081751</td>\n",
              "      <td>0.007432</td>\n",
              "      <td>0.010735</td>\n",
              "      <td>0.004129</td>\n",
              "      <td>0.009909</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009083</td>\n",
              "      <td>0.201486</td>\n",
              "      <td>0.013212</td>\n",
              "      <td>0.127993</td>\n",
              "      <td>0.020644</td>\n",
              "      <td>0.190751</td>\n",
              "      <td>0.026424</td>\n",
              "      <td>0.230388</td>\n",
              "      <td>0.031379</td>\n",
              "      <td>0.251858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>0.000773</td>\n",
              "      <td>0.000773</td>\n",
              "      <td>0.023957</td>\n",
              "      <td>0.036321</td>\n",
              "      <td>0.062597</td>\n",
              "      <td>0.073416</td>\n",
              "      <td>0.014683</td>\n",
              "      <td>0.022411</td>\n",
              "      <td>0.004637</td>\n",
              "      <td>0.009274</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008501</td>\n",
              "      <td>0.195518</td>\n",
              "      <td>0.015456</td>\n",
              "      <td>0.136012</td>\n",
              "      <td>0.021638</td>\n",
              "      <td>0.202473</td>\n",
              "      <td>0.037867</td>\n",
              "      <td>0.257342</td>\n",
              "      <td>0.031685</td>\n",
              "      <td>0.227975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.009070</td>\n",
              "      <td>0.022676</td>\n",
              "      <td>0.054422</td>\n",
              "      <td>0.067271</td>\n",
              "      <td>0.007559</td>\n",
              "      <td>0.013605</td>\n",
              "      <td>0.005291</td>\n",
              "      <td>0.011338</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010582</td>\n",
              "      <td>0.174603</td>\n",
              "      <td>0.018896</td>\n",
              "      <td>0.128496</td>\n",
              "      <td>0.024943</td>\n",
              "      <td>0.235072</td>\n",
              "      <td>0.022676</td>\n",
              "      <td>0.238095</td>\n",
              "      <td>0.033258</td>\n",
              "      <td>0.229025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>0.001685</td>\n",
              "      <td>0.000842</td>\n",
              "      <td>0.026116</td>\n",
              "      <td>0.033698</td>\n",
              "      <td>0.058130</td>\n",
              "      <td>0.074979</td>\n",
              "      <td>0.008425</td>\n",
              "      <td>0.019377</td>\n",
              "      <td>0.003370</td>\n",
              "      <td>0.009267</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006740</td>\n",
              "      <td>0.186184</td>\n",
              "      <td>0.014322</td>\n",
              "      <td>0.116259</td>\n",
              "      <td>0.011794</td>\n",
              "      <td>0.196293</td>\n",
              "      <td>0.022746</td>\n",
              "      <td>0.235889</td>\n",
              "      <td>0.027801</td>\n",
              "      <td>0.215670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>0.000873</td>\n",
              "      <td>0.000873</td>\n",
              "      <td>0.016579</td>\n",
              "      <td>0.027923</td>\n",
              "      <td>0.045375</td>\n",
              "      <td>0.056719</td>\n",
              "      <td>0.004363</td>\n",
              "      <td>0.007853</td>\n",
              "      <td>0.006981</td>\n",
              "      <td>0.012216</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012216</td>\n",
              "      <td>0.195462</td>\n",
              "      <td>0.013089</td>\n",
              "      <td>0.127400</td>\n",
              "      <td>0.024433</td>\n",
              "      <td>0.220768</td>\n",
              "      <td>0.022688</td>\n",
              "      <td>0.242583</td>\n",
              "      <td>0.034031</td>\n",
              "      <td>0.252182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>0.000740</td>\n",
              "      <td>0.001479</td>\n",
              "      <td>0.017012</td>\n",
              "      <td>0.020710</td>\n",
              "      <td>0.062870</td>\n",
              "      <td>0.075444</td>\n",
              "      <td>0.008876</td>\n",
              "      <td>0.018491</td>\n",
              "      <td>0.004438</td>\n",
              "      <td>0.008876</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008136</td>\n",
              "      <td>0.231509</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>0.127219</td>\n",
              "      <td>0.022189</td>\n",
              "      <td>0.198964</td>\n",
              "      <td>0.032544</td>\n",
              "      <td>0.246302</td>\n",
              "      <td>0.029586</td>\n",
              "      <td>0.234467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>0.000783</td>\n",
              "      <td>0.000783</td>\n",
              "      <td>0.018794</td>\n",
              "      <td>0.025842</td>\n",
              "      <td>0.065779</td>\n",
              "      <td>0.079092</td>\n",
              "      <td>0.010180</td>\n",
              "      <td>0.017228</td>\n",
              "      <td>0.007048</td>\n",
              "      <td>0.012529</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011746</td>\n",
              "      <td>0.194205</td>\n",
              "      <td>0.010963</td>\n",
              "      <td>0.110415</td>\n",
              "      <td>0.016445</td>\n",
              "      <td>0.191073</td>\n",
              "      <td>0.027408</td>\n",
              "      <td>0.254503</td>\n",
              "      <td>0.029757</td>\n",
              "      <td>0.245889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>0.000838</td>\n",
              "      <td>0.001676</td>\n",
              "      <td>0.017603</td>\n",
              "      <td>0.033529</td>\n",
              "      <td>0.053646</td>\n",
              "      <td>0.062867</td>\n",
              "      <td>0.008382</td>\n",
              "      <td>0.013412</td>\n",
              "      <td>0.010059</td>\n",
              "      <td>0.012573</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010897</td>\n",
              "      <td>0.182733</td>\n",
              "      <td>0.015926</td>\n",
              "      <td>0.124895</td>\n",
              "      <td>0.021794</td>\n",
              "      <td>0.203688</td>\n",
              "      <td>0.020117</td>\n",
              "      <td>0.238894</td>\n",
              "      <td>0.025147</td>\n",
              "      <td>0.230511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.013546</td>\n",
              "      <td>0.026295</td>\n",
              "      <td>0.068526</td>\n",
              "      <td>0.082072</td>\n",
              "      <td>0.007171</td>\n",
              "      <td>0.015139</td>\n",
              "      <td>0.006375</td>\n",
              "      <td>0.012749</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011155</td>\n",
              "      <td>0.181673</td>\n",
              "      <td>0.007171</td>\n",
              "      <td>0.120319</td>\n",
              "      <td>0.026295</td>\n",
              "      <td>0.232669</td>\n",
              "      <td>0.024701</td>\n",
              "      <td>0.225498</td>\n",
              "      <td>0.027092</td>\n",
              "      <td>0.244622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.018927</td>\n",
              "      <td>0.028391</td>\n",
              "      <td>0.063880</td>\n",
              "      <td>0.072555</td>\n",
              "      <td>0.012618</td>\n",
              "      <td>0.025237</td>\n",
              "      <td>0.007098</td>\n",
              "      <td>0.011830</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013407</td>\n",
              "      <td>0.194006</td>\n",
              "      <td>0.007098</td>\n",
              "      <td>0.122240</td>\n",
              "      <td>0.015773</td>\n",
              "      <td>0.197161</td>\n",
              "      <td>0.033912</td>\n",
              "      <td>0.242902</td>\n",
              "      <td>0.025237</td>\n",
              "      <td>0.236593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>0.000758</td>\n",
              "      <td>0.001515</td>\n",
              "      <td>0.020455</td>\n",
              "      <td>0.029545</td>\n",
              "      <td>0.056061</td>\n",
              "      <td>0.073485</td>\n",
              "      <td>0.012121</td>\n",
              "      <td>0.021212</td>\n",
              "      <td>0.006818</td>\n",
              "      <td>0.012879</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016667</td>\n",
              "      <td>0.213636</td>\n",
              "      <td>0.009091</td>\n",
              "      <td>0.131061</td>\n",
              "      <td>0.021970</td>\n",
              "      <td>0.225758</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.253788</td>\n",
              "      <td>0.028788</td>\n",
              "      <td>0.229545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>0.000774</td>\n",
              "      <td>0.000774</td>\n",
              "      <td>0.015480</td>\n",
              "      <td>0.020898</td>\n",
              "      <td>0.071981</td>\n",
              "      <td>0.088235</td>\n",
              "      <td>0.008514</td>\n",
              "      <td>0.010836</td>\n",
              "      <td>0.005418</td>\n",
              "      <td>0.017802</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010836</td>\n",
              "      <td>0.195046</td>\n",
              "      <td>0.016254</td>\n",
              "      <td>0.111455</td>\n",
              "      <td>0.021672</td>\n",
              "      <td>0.208204</td>\n",
              "      <td>0.017028</td>\n",
              "      <td>0.232198</td>\n",
              "      <td>0.027864</td>\n",
              "      <td>0.230650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>0.000789</td>\n",
              "      <td>0.000789</td>\n",
              "      <td>0.015773</td>\n",
              "      <td>0.030757</td>\n",
              "      <td>0.070189</td>\n",
              "      <td>0.078076</td>\n",
              "      <td>0.008675</td>\n",
              "      <td>0.015773</td>\n",
              "      <td>0.003155</td>\n",
              "      <td>0.011830</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011830</td>\n",
              "      <td>0.205836</td>\n",
              "      <td>0.011041</td>\n",
              "      <td>0.119085</td>\n",
              "      <td>0.014984</td>\n",
              "      <td>0.218454</td>\n",
              "      <td>0.026814</td>\n",
              "      <td>0.216877</td>\n",
              "      <td>0.029180</td>\n",
              "      <td>0.241325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>0.001631</td>\n",
              "      <td>0.002447</td>\n",
              "      <td>0.017945</td>\n",
              "      <td>0.030995</td>\n",
              "      <td>0.044861</td>\n",
              "      <td>0.056281</td>\n",
              "      <td>0.008972</td>\n",
              "      <td>0.020392</td>\n",
              "      <td>0.005710</td>\n",
              "      <td>0.011419</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014682</td>\n",
              "      <td>0.203100</td>\n",
              "      <td>0.009788</td>\n",
              "      <td>0.137031</td>\n",
              "      <td>0.023654</td>\n",
              "      <td>0.239804</td>\n",
              "      <td>0.019576</td>\n",
              "      <td>0.234095</td>\n",
              "      <td>0.019576</td>\n",
              "      <td>0.227569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>0.001404</td>\n",
              "      <td>0.002107</td>\n",
              "      <td>0.014747</td>\n",
              "      <td>0.028090</td>\n",
              "      <td>0.054775</td>\n",
              "      <td>0.070225</td>\n",
              "      <td>0.012640</td>\n",
              "      <td>0.018961</td>\n",
              "      <td>0.004916</td>\n",
              "      <td>0.012640</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007725</td>\n",
              "      <td>0.192416</td>\n",
              "      <td>0.011236</td>\n",
              "      <td>0.127809</td>\n",
              "      <td>0.016152</td>\n",
              "      <td>0.212781</td>\n",
              "      <td>0.020365</td>\n",
              "      <td>0.230337</td>\n",
              "      <td>0.025983</td>\n",
              "      <td>0.226826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>0.000841</td>\n",
              "      <td>0.000841</td>\n",
              "      <td>0.017662</td>\n",
              "      <td>0.033642</td>\n",
              "      <td>0.066442</td>\n",
              "      <td>0.074853</td>\n",
              "      <td>0.010934</td>\n",
              "      <td>0.022708</td>\n",
              "      <td>0.005887</td>\n",
              "      <td>0.013457</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010093</td>\n",
              "      <td>0.191758</td>\n",
              "      <td>0.010934</td>\n",
              "      <td>0.124474</td>\n",
              "      <td>0.020185</td>\n",
              "      <td>0.193440</td>\n",
              "      <td>0.025231</td>\n",
              "      <td>0.232128</td>\n",
              "      <td>0.023549</td>\n",
              "      <td>0.223717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>0.000741</td>\n",
              "      <td>0.000741</td>\n",
              "      <td>0.016308</td>\n",
              "      <td>0.022980</td>\n",
              "      <td>0.054855</td>\n",
              "      <td>0.070423</td>\n",
              "      <td>0.008154</td>\n",
              "      <td>0.015567</td>\n",
              "      <td>0.003706</td>\n",
              "      <td>0.013343</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011861</td>\n",
              "      <td>0.198666</td>\n",
              "      <td>0.011861</td>\n",
              "      <td>0.126019</td>\n",
              "      <td>0.020015</td>\n",
              "      <td>0.217198</td>\n",
              "      <td>0.022239</td>\n",
              "      <td>0.223870</td>\n",
              "      <td>0.034841</td>\n",
              "      <td>0.247591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>0.001467</td>\n",
              "      <td>0.002201</td>\n",
              "      <td>0.016141</td>\n",
              "      <td>0.026412</td>\n",
              "      <td>0.053558</td>\n",
              "      <td>0.069699</td>\n",
              "      <td>0.008070</td>\n",
              "      <td>0.015407</td>\n",
              "      <td>0.006603</td>\n",
              "      <td>0.008804</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018342</td>\n",
              "      <td>0.202494</td>\n",
              "      <td>0.007337</td>\n",
              "      <td>0.118855</td>\n",
              "      <td>0.014674</td>\n",
              "      <td>0.224505</td>\n",
              "      <td>0.016141</td>\n",
              "      <td>0.227439</td>\n",
              "      <td>0.023478</td>\n",
              "      <td>0.241379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>0.000820</td>\n",
              "      <td>0.000820</td>\n",
              "      <td>0.014766</td>\n",
              "      <td>0.022970</td>\n",
              "      <td>0.060705</td>\n",
              "      <td>0.072190</td>\n",
              "      <td>0.004922</td>\n",
              "      <td>0.011485</td>\n",
              "      <td>0.004102</td>\n",
              "      <td>0.013946</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009024</td>\n",
              "      <td>0.205086</td>\n",
              "      <td>0.010664</td>\n",
              "      <td>0.127153</td>\n",
              "      <td>0.021329</td>\n",
              "      <td>0.219032</td>\n",
              "      <td>0.022970</td>\n",
              "      <td>0.214930</td>\n",
              "      <td>0.027892</td>\n",
              "      <td>0.237900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.012893</td>\n",
              "      <td>0.025786</td>\n",
              "      <td>0.047542</td>\n",
              "      <td>0.064464</td>\n",
              "      <td>0.008058</td>\n",
              "      <td>0.012893</td>\n",
              "      <td>0.004029</td>\n",
              "      <td>0.010475</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008864</td>\n",
              "      <td>0.188558</td>\n",
              "      <td>0.008864</td>\n",
              "      <td>0.130540</td>\n",
              "      <td>0.025786</td>\n",
              "      <td>0.229654</td>\n",
              "      <td>0.020951</td>\n",
              "      <td>0.230459</td>\n",
              "      <td>0.030620</td>\n",
              "      <td>0.241741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>0.001674</td>\n",
              "      <td>0.002510</td>\n",
              "      <td>0.018410</td>\n",
              "      <td>0.028452</td>\n",
              "      <td>0.056904</td>\n",
              "      <td>0.070293</td>\n",
              "      <td>0.010879</td>\n",
              "      <td>0.013389</td>\n",
              "      <td>0.006695</td>\n",
              "      <td>0.018410</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015900</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.012552</td>\n",
              "      <td>0.097908</td>\n",
              "      <td>0.021757</td>\n",
              "      <td>0.225105</td>\n",
              "      <td>0.022594</td>\n",
              "      <td>0.243515</td>\n",
              "      <td>0.035146</td>\n",
              "      <td>0.246862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.001627</td>\n",
              "      <td>0.016273</td>\n",
              "      <td>0.026037</td>\n",
              "      <td>0.048820</td>\n",
              "      <td>0.055330</td>\n",
              "      <td>0.005696</td>\n",
              "      <td>0.016273</td>\n",
              "      <td>0.004882</td>\n",
              "      <td>0.011391</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013832</td>\n",
              "      <td>0.187144</td>\n",
              "      <td>0.007323</td>\n",
              "      <td>0.113914</td>\n",
              "      <td>0.017901</td>\n",
              "      <td>0.210740</td>\n",
              "      <td>0.023596</td>\n",
              "      <td>0.248983</td>\n",
              "      <td>0.024410</td>\n",
              "      <td>0.227828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>0.000713</td>\n",
              "      <td>0.000713</td>\n",
              "      <td>0.014979</td>\n",
              "      <td>0.032097</td>\n",
              "      <td>0.043509</td>\n",
              "      <td>0.066334</td>\n",
              "      <td>0.014265</td>\n",
              "      <td>0.024964</td>\n",
              "      <td>0.008559</td>\n",
              "      <td>0.019258</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007846</td>\n",
              "      <td>0.196862</td>\n",
              "      <td>0.007846</td>\n",
              "      <td>0.120542</td>\n",
              "      <td>0.022825</td>\n",
              "      <td>0.228245</td>\n",
              "      <td>0.020685</td>\n",
              "      <td>0.232525</td>\n",
              "      <td>0.026391</td>\n",
              "      <td>0.248217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>0.000838</td>\n",
              "      <td>0.002515</td>\n",
              "      <td>0.014250</td>\n",
              "      <td>0.027661</td>\n",
              "      <td>0.065381</td>\n",
              "      <td>0.075440</td>\n",
              "      <td>0.008382</td>\n",
              "      <td>0.016764</td>\n",
              "      <td>0.004191</td>\n",
              "      <td>0.008382</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014250</td>\n",
              "      <td>0.195306</td>\n",
              "      <td>0.011735</td>\n",
              "      <td>0.121542</td>\n",
              "      <td>0.026823</td>\n",
              "      <td>0.220453</td>\n",
              "      <td>0.025147</td>\n",
              "      <td>0.232188</td>\n",
              "      <td>0.019279</td>\n",
              "      <td>0.237217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>0.001627</td>\n",
              "      <td>0.001627</td>\n",
              "      <td>0.013019</td>\n",
              "      <td>0.030106</td>\n",
              "      <td>0.063466</td>\n",
              "      <td>0.073230</td>\n",
              "      <td>0.009764</td>\n",
              "      <td>0.020342</td>\n",
              "      <td>0.005696</td>\n",
              "      <td>0.014646</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008950</td>\n",
              "      <td>0.211554</td>\n",
              "      <td>0.013019</td>\n",
              "      <td>0.113100</td>\n",
              "      <td>0.026037</td>\n",
              "      <td>0.222945</td>\n",
              "      <td>0.016273</td>\n",
              "      <td>0.233523</td>\n",
              "      <td>0.025224</td>\n",
              "      <td>0.222132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>0.001488</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>0.012649</td>\n",
              "      <td>0.029018</td>\n",
              "      <td>0.063244</td>\n",
              "      <td>0.074405</td>\n",
              "      <td>0.005952</td>\n",
              "      <td>0.013393</td>\n",
              "      <td>0.010417</td>\n",
              "      <td>0.016369</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011161</td>\n",
              "      <td>0.188988</td>\n",
              "      <td>0.010417</td>\n",
              "      <td>0.118304</td>\n",
              "      <td>0.023810</td>\n",
              "      <td>0.228423</td>\n",
              "      <td>0.023065</td>\n",
              "      <td>0.253720</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.232887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>0.000673</td>\n",
              "      <td>0.000673</td>\n",
              "      <td>0.016162</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.051852</td>\n",
              "      <td>0.070034</td>\n",
              "      <td>0.006734</td>\n",
              "      <td>0.016835</td>\n",
              "      <td>0.005387</td>\n",
              "      <td>0.018182</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014141</td>\n",
              "      <td>0.183165</td>\n",
              "      <td>0.016162</td>\n",
              "      <td>0.131987</td>\n",
              "      <td>0.014815</td>\n",
              "      <td>0.212795</td>\n",
              "      <td>0.023569</td>\n",
              "      <td>0.235017</td>\n",
              "      <td>0.029630</td>\n",
              "      <td>0.226936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>0.001621</td>\n",
              "      <td>0.002431</td>\n",
              "      <td>0.015397</td>\n",
              "      <td>0.034036</td>\n",
              "      <td>0.048622</td>\n",
              "      <td>0.069692</td>\n",
              "      <td>0.007293</td>\n",
              "      <td>0.019449</td>\n",
              "      <td>0.002431</td>\n",
              "      <td>0.015397</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008104</td>\n",
              "      <td>0.176661</td>\n",
              "      <td>0.018639</td>\n",
              "      <td>0.120746</td>\n",
              "      <td>0.021880</td>\n",
              "      <td>0.230956</td>\n",
              "      <td>0.021880</td>\n",
              "      <td>0.270665</td>\n",
              "      <td>0.014587</td>\n",
              "      <td>0.217990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>0.002185</td>\n",
              "      <td>0.002913</td>\n",
              "      <td>0.021122</td>\n",
              "      <td>0.029862</td>\n",
              "      <td>0.045157</td>\n",
              "      <td>0.068463</td>\n",
              "      <td>0.011653</td>\n",
              "      <td>0.019665</td>\n",
              "      <td>0.007283</td>\n",
              "      <td>0.019665</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011653</td>\n",
              "      <td>0.195193</td>\n",
              "      <td>0.016752</td>\n",
              "      <td>0.116533</td>\n",
              "      <td>0.018208</td>\n",
              "      <td>0.213401</td>\n",
              "      <td>0.016023</td>\n",
              "      <td>0.209760</td>\n",
              "      <td>0.019665</td>\n",
              "      <td>0.217771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>0.002094</td>\n",
              "      <td>0.003489</td>\n",
              "      <td>0.015352</td>\n",
              "      <td>0.030007</td>\n",
              "      <td>0.048151</td>\n",
              "      <td>0.069784</td>\n",
              "      <td>0.006281</td>\n",
              "      <td>0.014655</td>\n",
              "      <td>0.006281</td>\n",
              "      <td>0.016050</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012561</td>\n",
              "      <td>0.195394</td>\n",
              "      <td>0.013957</td>\n",
              "      <td>0.120726</td>\n",
              "      <td>0.023029</td>\n",
              "      <td>0.229588</td>\n",
              "      <td>0.024424</td>\n",
              "      <td>0.220516</td>\n",
              "      <td>0.018144</td>\n",
              "      <td>0.217027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>0.000801</td>\n",
              "      <td>0.001603</td>\n",
              "      <td>0.011218</td>\n",
              "      <td>0.016827</td>\n",
              "      <td>0.060897</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.009615</td>\n",
              "      <td>0.014423</td>\n",
              "      <td>0.005609</td>\n",
              "      <td>0.008814</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014423</td>\n",
              "      <td>0.206731</td>\n",
              "      <td>0.010417</td>\n",
              "      <td>0.129808</td>\n",
              "      <td>0.022436</td>\n",
              "      <td>0.215545</td>\n",
              "      <td>0.024840</td>\n",
              "      <td>0.233974</td>\n",
              "      <td>0.027244</td>\n",
              "      <td>0.228365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>0.001423</td>\n",
              "      <td>0.000712</td>\n",
              "      <td>0.019217</td>\n",
              "      <td>0.034875</td>\n",
              "      <td>0.056228</td>\n",
              "      <td>0.078292</td>\n",
              "      <td>0.009964</td>\n",
              "      <td>0.017794</td>\n",
              "      <td>0.004270</td>\n",
              "      <td>0.012811</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009964</td>\n",
              "      <td>0.193594</td>\n",
              "      <td>0.019217</td>\n",
              "      <td>0.115302</td>\n",
              "      <td>0.024199</td>\n",
              "      <td>0.234164</td>\n",
              "      <td>0.025623</td>\n",
              "      <td>0.244840</td>\n",
              "      <td>0.024911</td>\n",
              "      <td>0.248399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>0.000808</td>\n",
              "      <td>0.002423</td>\n",
              "      <td>0.011309</td>\n",
              "      <td>0.024233</td>\n",
              "      <td>0.045234</td>\n",
              "      <td>0.063005</td>\n",
              "      <td>0.008885</td>\n",
              "      <td>0.020194</td>\n",
              "      <td>0.004847</td>\n",
              "      <td>0.013732</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012116</td>\n",
              "      <td>0.229402</td>\n",
              "      <td>0.014540</td>\n",
              "      <td>0.115509</td>\n",
              "      <td>0.027464</td>\n",
              "      <td>0.226171</td>\n",
              "      <td>0.018578</td>\n",
              "      <td>0.230210</td>\n",
              "      <td>0.034733</td>\n",
              "      <td>0.246365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>0.000763</td>\n",
              "      <td>0.002290</td>\n",
              "      <td>0.012214</td>\n",
              "      <td>0.034351</td>\n",
              "      <td>0.050382</td>\n",
              "      <td>0.067939</td>\n",
              "      <td>0.003053</td>\n",
              "      <td>0.012977</td>\n",
              "      <td>0.005344</td>\n",
              "      <td>0.019847</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007634</td>\n",
              "      <td>0.172519</td>\n",
              "      <td>0.006107</td>\n",
              "      <td>0.119084</td>\n",
              "      <td>0.027481</td>\n",
              "      <td>0.215267</td>\n",
              "      <td>0.019084</td>\n",
              "      <td>0.259542</td>\n",
              "      <td>0.015267</td>\n",
              "      <td>0.220611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>0.001342</td>\n",
              "      <td>0.002013</td>\n",
              "      <td>0.016107</td>\n",
              "      <td>0.040268</td>\n",
              "      <td>0.056376</td>\n",
              "      <td>0.077852</td>\n",
              "      <td>0.006711</td>\n",
              "      <td>0.016107</td>\n",
              "      <td>0.007383</td>\n",
              "      <td>0.018792</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018121</td>\n",
              "      <td>0.182550</td>\n",
              "      <td>0.005369</td>\n",
              "      <td>0.114765</td>\n",
              "      <td>0.025503</td>\n",
              "      <td>0.237584</td>\n",
              "      <td>0.017450</td>\n",
              "      <td>0.236242</td>\n",
              "      <td>0.027517</td>\n",
              "      <td>0.232886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>0.000758</td>\n",
              "      <td>0.000758</td>\n",
              "      <td>0.015909</td>\n",
              "      <td>0.035606</td>\n",
              "      <td>0.052273</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.011364</td>\n",
              "      <td>0.017424</td>\n",
              "      <td>0.006818</td>\n",
              "      <td>0.012879</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010606</td>\n",
              "      <td>0.198485</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.118939</td>\n",
              "      <td>0.019697</td>\n",
              "      <td>0.224242</td>\n",
              "      <td>0.031818</td>\n",
              "      <td>0.247727</td>\n",
              "      <td>0.024242</td>\n",
              "      <td>0.234848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>0.000722</td>\n",
              "      <td>0.002166</td>\n",
              "      <td>0.020939</td>\n",
              "      <td>0.038989</td>\n",
              "      <td>0.047653</td>\n",
              "      <td>0.067148</td>\n",
              "      <td>0.007220</td>\n",
              "      <td>0.011552</td>\n",
              "      <td>0.005054</td>\n",
              "      <td>0.014440</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013718</td>\n",
              "      <td>0.191336</td>\n",
              "      <td>0.006498</td>\n",
              "      <td>0.109747</td>\n",
              "      <td>0.027437</td>\n",
              "      <td>0.231769</td>\n",
              "      <td>0.026715</td>\n",
              "      <td>0.236823</td>\n",
              "      <td>0.023827</td>\n",
              "      <td>0.241155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>0.001357</td>\n",
              "      <td>0.002714</td>\n",
              "      <td>0.018996</td>\n",
              "      <td>0.036635</td>\n",
              "      <td>0.053596</td>\n",
              "      <td>0.071913</td>\n",
              "      <td>0.011533</td>\n",
              "      <td>0.025780</td>\n",
              "      <td>0.005427</td>\n",
              "      <td>0.021031</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014925</td>\n",
              "      <td>0.185889</td>\n",
              "      <td>0.014247</td>\n",
              "      <td>0.103121</td>\n",
              "      <td>0.014247</td>\n",
              "      <td>0.202171</td>\n",
              "      <td>0.025102</td>\n",
              "      <td>0.227951</td>\n",
              "      <td>0.020353</td>\n",
              "      <td>0.230665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.010760</td>\n",
              "      <td>0.026227</td>\n",
              "      <td>0.052455</td>\n",
              "      <td>0.065905</td>\n",
              "      <td>0.008742</td>\n",
              "      <td>0.016812</td>\n",
              "      <td>0.006052</td>\n",
              "      <td>0.011432</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008070</td>\n",
              "      <td>0.182919</td>\n",
              "      <td>0.008070</td>\n",
              "      <td>0.128447</td>\n",
              "      <td>0.020847</td>\n",
              "      <td>0.223941</td>\n",
              "      <td>0.018830</td>\n",
              "      <td>0.238736</td>\n",
              "      <td>0.026900</td>\n",
              "      <td>0.223941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.014254</td>\n",
              "      <td>0.021005</td>\n",
              "      <td>0.060015</td>\n",
              "      <td>0.072768</td>\n",
              "      <td>0.006002</td>\n",
              "      <td>0.015004</td>\n",
              "      <td>0.012003</td>\n",
              "      <td>0.014254</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004501</td>\n",
              "      <td>0.204051</td>\n",
              "      <td>0.013503</td>\n",
              "      <td>0.123031</td>\n",
              "      <td>0.025506</td>\n",
              "      <td>0.234059</td>\n",
              "      <td>0.021755</td>\n",
              "      <td>0.234809</td>\n",
              "      <td>0.027007</td>\n",
              "      <td>0.242311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>0.001395</td>\n",
              "      <td>0.002789</td>\n",
              "      <td>0.017434</td>\n",
              "      <td>0.028591</td>\n",
              "      <td>0.050907</td>\n",
              "      <td>0.071130</td>\n",
              "      <td>0.010460</td>\n",
              "      <td>0.016736</td>\n",
              "      <td>0.006276</td>\n",
              "      <td>0.018131</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013250</td>\n",
              "      <td>0.186890</td>\n",
              "      <td>0.014644</td>\n",
              "      <td>0.107392</td>\n",
              "      <td>0.025802</td>\n",
              "      <td>0.215481</td>\n",
              "      <td>0.021618</td>\n",
              "      <td>0.231520</td>\n",
              "      <td>0.027197</td>\n",
              "      <td>0.225941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>0.000693</td>\n",
              "      <td>0.001386</td>\n",
              "      <td>0.016632</td>\n",
              "      <td>0.030492</td>\n",
              "      <td>0.048510</td>\n",
              "      <td>0.062370</td>\n",
              "      <td>0.006930</td>\n",
              "      <td>0.012474</td>\n",
              "      <td>0.005544</td>\n",
              "      <td>0.011088</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013167</td>\n",
              "      <td>0.198198</td>\n",
              "      <td>0.007623</td>\n",
              "      <td>0.118503</td>\n",
              "      <td>0.022869</td>\n",
              "      <td>0.224532</td>\n",
              "      <td>0.023562</td>\n",
              "      <td>0.218295</td>\n",
              "      <td>0.029106</td>\n",
              "      <td>0.250173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.001428</td>\n",
              "      <td>0.021413</td>\n",
              "      <td>0.034261</td>\n",
              "      <td>0.052106</td>\n",
              "      <td>0.071378</td>\n",
              "      <td>0.008565</td>\n",
              "      <td>0.017131</td>\n",
              "      <td>0.004996</td>\n",
              "      <td>0.022841</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009279</td>\n",
              "      <td>0.178444</td>\n",
              "      <td>0.013562</td>\n",
              "      <td>0.131335</td>\n",
              "      <td>0.014989</td>\n",
              "      <td>0.229122</td>\n",
              "      <td>0.022127</td>\n",
              "      <td>0.229836</td>\n",
              "      <td>0.020700</td>\n",
              "      <td>0.220557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>0.002764</td>\n",
              "      <td>0.003455</td>\n",
              "      <td>0.015204</td>\n",
              "      <td>0.031099</td>\n",
              "      <td>0.046994</td>\n",
              "      <td>0.071182</td>\n",
              "      <td>0.005529</td>\n",
              "      <td>0.018659</td>\n",
              "      <td>0.006911</td>\n",
              "      <td>0.017968</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012440</td>\n",
              "      <td>0.201106</td>\n",
              "      <td>0.006220</td>\n",
              "      <td>0.117484</td>\n",
              "      <td>0.015895</td>\n",
              "      <td>0.223912</td>\n",
              "      <td>0.023497</td>\n",
              "      <td>0.241189</td>\n",
              "      <td>0.030408</td>\n",
              "      <td>0.243953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>0.000717</td>\n",
              "      <td>0.002151</td>\n",
              "      <td>0.020789</td>\n",
              "      <td>0.040143</td>\n",
              "      <td>0.060932</td>\n",
              "      <td>0.078853</td>\n",
              "      <td>0.011470</td>\n",
              "      <td>0.025806</td>\n",
              "      <td>0.010753</td>\n",
              "      <td>0.020789</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016487</td>\n",
              "      <td>0.200717</td>\n",
              "      <td>0.012186</td>\n",
              "      <td>0.116846</td>\n",
              "      <td>0.015771</td>\n",
              "      <td>0.222939</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.223656</td>\n",
              "      <td>0.021505</td>\n",
              "      <td>0.231541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>0.001432</td>\n",
              "      <td>0.001432</td>\n",
              "      <td>0.015032</td>\n",
              "      <td>0.030064</td>\n",
              "      <td>0.049392</td>\n",
              "      <td>0.067287</td>\n",
              "      <td>0.012169</td>\n",
              "      <td>0.018611</td>\n",
              "      <td>0.007158</td>\n",
              "      <td>0.013601</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010737</td>\n",
              "      <td>0.202577</td>\n",
              "      <td>0.015748</td>\n",
              "      <td>0.132427</td>\n",
              "      <td>0.023622</td>\n",
              "      <td>0.235505</td>\n",
              "      <td>0.023622</td>\n",
              "      <td>0.256263</td>\n",
              "      <td>0.029349</td>\n",
              "      <td>0.236220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.001948</td>\n",
              "      <td>0.016234</td>\n",
              "      <td>0.037662</td>\n",
              "      <td>0.042857</td>\n",
              "      <td>0.059091</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.022727</td>\n",
              "      <td>0.009091</td>\n",
              "      <td>0.018182</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014935</td>\n",
              "      <td>0.220779</td>\n",
              "      <td>0.009091</td>\n",
              "      <td>0.105195</td>\n",
              "      <td>0.019481</td>\n",
              "      <td>0.216234</td>\n",
              "      <td>0.017532</td>\n",
              "      <td>0.241558</td>\n",
              "      <td>0.029870</td>\n",
              "      <td>0.242208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>0.001441</td>\n",
              "      <td>0.001441</td>\n",
              "      <td>0.015130</td>\n",
              "      <td>0.036744</td>\n",
              "      <td>0.038184</td>\n",
              "      <td>0.053314</td>\n",
              "      <td>0.006484</td>\n",
              "      <td>0.018732</td>\n",
              "      <td>0.007925</td>\n",
              "      <td>0.024496</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012248</td>\n",
              "      <td>0.201729</td>\n",
              "      <td>0.009366</td>\n",
              "      <td>0.124640</td>\n",
              "      <td>0.016571</td>\n",
              "      <td>0.223343</td>\n",
              "      <td>0.021614</td>\n",
              "      <td>0.238473</td>\n",
              "      <td>0.019452</td>\n",
              "      <td>0.229107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>0.001331</td>\n",
              "      <td>0.001331</td>\n",
              "      <td>0.015968</td>\n",
              "      <td>0.029275</td>\n",
              "      <td>0.035928</td>\n",
              "      <td>0.046574</td>\n",
              "      <td>0.009315</td>\n",
              "      <td>0.019960</td>\n",
              "      <td>0.007319</td>\n",
              "      <td>0.022621</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009315</td>\n",
              "      <td>0.198270</td>\n",
              "      <td>0.008649</td>\n",
              "      <td>0.119760</td>\n",
              "      <td>0.022621</td>\n",
              "      <td>0.228876</td>\n",
              "      <td>0.028609</td>\n",
              "      <td>0.214238</td>\n",
              "      <td>0.019960</td>\n",
              "      <td>0.245509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>0.001368</td>\n",
              "      <td>0.005472</td>\n",
              "      <td>0.012996</td>\n",
              "      <td>0.028728</td>\n",
              "      <td>0.038304</td>\n",
              "      <td>0.057456</td>\n",
              "      <td>0.008208</td>\n",
              "      <td>0.011628</td>\n",
              "      <td>0.006840</td>\n",
              "      <td>0.014364</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010944</td>\n",
              "      <td>0.207934</td>\n",
              "      <td>0.011628</td>\n",
              "      <td>0.114911</td>\n",
              "      <td>0.021888</td>\n",
              "      <td>0.226402</td>\n",
              "      <td>0.023940</td>\n",
              "      <td>0.223666</td>\n",
              "      <td>0.017784</td>\n",
              "      <td>0.244870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>0.001388</td>\n",
              "      <td>0.002082</td>\n",
              "      <td>0.019431</td>\n",
              "      <td>0.034004</td>\n",
              "      <td>0.040944</td>\n",
              "      <td>0.059681</td>\n",
              "      <td>0.007634</td>\n",
              "      <td>0.020125</td>\n",
              "      <td>0.005552</td>\n",
              "      <td>0.022207</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007634</td>\n",
              "      <td>0.185982</td>\n",
              "      <td>0.010409</td>\n",
              "      <td>0.120056</td>\n",
              "      <td>0.014573</td>\n",
              "      <td>0.222068</td>\n",
              "      <td>0.021513</td>\n",
              "      <td>0.243581</td>\n",
              "      <td>0.025677</td>\n",
              "      <td>0.238723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>0.002122</td>\n",
              "      <td>0.003536</td>\n",
              "      <td>0.013437</td>\n",
              "      <td>0.028996</td>\n",
              "      <td>0.044554</td>\n",
              "      <td>0.067893</td>\n",
              "      <td>0.007072</td>\n",
              "      <td>0.011315</td>\n",
              "      <td>0.006365</td>\n",
              "      <td>0.018388</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009901</td>\n",
              "      <td>0.198020</td>\n",
              "      <td>0.012730</td>\n",
              "      <td>0.117397</td>\n",
              "      <td>0.014144</td>\n",
              "      <td>0.220651</td>\n",
              "      <td>0.021216</td>\n",
              "      <td>0.244696</td>\n",
              "      <td>0.020509</td>\n",
              "      <td>0.228430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>0.004578</td>\n",
              "      <td>0.004578</td>\n",
              "      <td>0.016351</td>\n",
              "      <td>0.027469</td>\n",
              "      <td>0.045128</td>\n",
              "      <td>0.068672</td>\n",
              "      <td>0.011118</td>\n",
              "      <td>0.021583</td>\n",
              "      <td>0.007194</td>\n",
              "      <td>0.018967</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007194</td>\n",
              "      <td>0.181164</td>\n",
              "      <td>0.009156</td>\n",
              "      <td>0.117070</td>\n",
              "      <td>0.019621</td>\n",
              "      <td>0.235448</td>\n",
              "      <td>0.019621</td>\n",
              "      <td>0.247220</td>\n",
              "      <td>0.022891</td>\n",
              "      <td>0.232832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>0.001944</td>\n",
              "      <td>0.004537</td>\n",
              "      <td>0.007129</td>\n",
              "      <td>0.022683</td>\n",
              "      <td>0.048607</td>\n",
              "      <td>0.069345</td>\n",
              "      <td>0.003889</td>\n",
              "      <td>0.012314</td>\n",
              "      <td>0.012314</td>\n",
              "      <td>0.016850</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010369</td>\n",
              "      <td>0.191186</td>\n",
              "      <td>0.011666</td>\n",
              "      <td>0.137395</td>\n",
              "      <td>0.026572</td>\n",
              "      <td>0.215165</td>\n",
              "      <td>0.016850</td>\n",
              "      <td>0.220350</td>\n",
              "      <td>0.022683</td>\n",
              "      <td>0.228127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>0.003966</td>\n",
              "      <td>0.007270</td>\n",
              "      <td>0.011897</td>\n",
              "      <td>0.025116</td>\n",
              "      <td>0.044944</td>\n",
              "      <td>0.060806</td>\n",
              "      <td>0.011236</td>\n",
              "      <td>0.018506</td>\n",
              "      <td>0.005288</td>\n",
              "      <td>0.019167</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011897</td>\n",
              "      <td>0.201586</td>\n",
              "      <td>0.012558</td>\n",
              "      <td>0.120952</td>\n",
              "      <td>0.020489</td>\n",
              "      <td>0.226041</td>\n",
              "      <td>0.018506</td>\n",
              "      <td>0.234633</td>\n",
              "      <td>0.022472</td>\n",
              "      <td>0.245869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>0.000674</td>\n",
              "      <td>0.004046</td>\n",
              "      <td>0.017532</td>\n",
              "      <td>0.030344</td>\n",
              "      <td>0.043156</td>\n",
              "      <td>0.056642</td>\n",
              "      <td>0.008766</td>\n",
              "      <td>0.018206</td>\n",
              "      <td>0.005394</td>\n",
              "      <td>0.022252</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012812</td>\n",
              "      <td>0.198921</td>\n",
              "      <td>0.010115</td>\n",
              "      <td>0.118004</td>\n",
              "      <td>0.020229</td>\n",
              "      <td>0.231962</td>\n",
              "      <td>0.016858</td>\n",
              "      <td>0.236682</td>\n",
              "      <td>0.020904</td>\n",
              "      <td>0.219825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>0.001526</td>\n",
              "      <td>0.004577</td>\n",
              "      <td>0.013730</td>\n",
              "      <td>0.025172</td>\n",
              "      <td>0.039664</td>\n",
              "      <td>0.055683</td>\n",
              "      <td>0.006865</td>\n",
              "      <td>0.020595</td>\n",
              "      <td>0.006865</td>\n",
              "      <td>0.022121</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>0.186880</td>\n",
              "      <td>0.011442</td>\n",
              "      <td>0.134249</td>\n",
              "      <td>0.027460</td>\n",
              "      <td>0.225782</td>\n",
              "      <td>0.022121</td>\n",
              "      <td>0.259344</td>\n",
              "      <td>0.018307</td>\n",
              "      <td>0.220442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>0.000762</td>\n",
              "      <td>0.003808</td>\n",
              "      <td>0.012947</td>\n",
              "      <td>0.027418</td>\n",
              "      <td>0.061691</td>\n",
              "      <td>0.078446</td>\n",
              "      <td>0.008378</td>\n",
              "      <td>0.012186</td>\n",
              "      <td>0.006093</td>\n",
              "      <td>0.012186</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005331</td>\n",
              "      <td>0.188880</td>\n",
              "      <td>0.008378</td>\n",
              "      <td>0.131759</td>\n",
              "      <td>0.017517</td>\n",
              "      <td>0.229246</td>\n",
              "      <td>0.031988</td>\n",
              "      <td>0.242193</td>\n",
              "      <td>0.018279</td>\n",
              "      <td>0.233816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>0.002446</td>\n",
              "      <td>0.006116</td>\n",
              "      <td>0.015902</td>\n",
              "      <td>0.028746</td>\n",
              "      <td>0.051376</td>\n",
              "      <td>0.072171</td>\n",
              "      <td>0.008563</td>\n",
              "      <td>0.025076</td>\n",
              "      <td>0.006728</td>\n",
              "      <td>0.017125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007951</td>\n",
              "      <td>0.196942</td>\n",
              "      <td>0.007951</td>\n",
              "      <td>0.116208</td>\n",
              "      <td>0.014679</td>\n",
              "      <td>0.231193</td>\n",
              "      <td>0.020795</td>\n",
              "      <td>0.234251</td>\n",
              "      <td>0.020183</td>\n",
              "      <td>0.226911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>0.003019</td>\n",
              "      <td>0.005283</td>\n",
              "      <td>0.010566</td>\n",
              "      <td>0.021132</td>\n",
              "      <td>0.040000</td>\n",
              "      <td>0.055094</td>\n",
              "      <td>0.006038</td>\n",
              "      <td>0.015094</td>\n",
              "      <td>0.007547</td>\n",
              "      <td>0.012830</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012075</td>\n",
              "      <td>0.200755</td>\n",
              "      <td>0.009811</td>\n",
              "      <td>0.117736</td>\n",
              "      <td>0.016604</td>\n",
              "      <td>0.215094</td>\n",
              "      <td>0.019623</td>\n",
              "      <td>0.250566</td>\n",
              "      <td>0.022642</td>\n",
              "      <td>0.230189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>0.003819</td>\n",
              "      <td>0.007002</td>\n",
              "      <td>0.008275</td>\n",
              "      <td>0.021642</td>\n",
              "      <td>0.047104</td>\n",
              "      <td>0.061744</td>\n",
              "      <td>0.006365</td>\n",
              "      <td>0.019096</td>\n",
              "      <td>0.007002</td>\n",
              "      <td>0.026098</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008912</td>\n",
              "      <td>0.194780</td>\n",
              "      <td>0.010185</td>\n",
              "      <td>0.120306</td>\n",
              "      <td>0.021006</td>\n",
              "      <td>0.232973</td>\n",
              "      <td>0.017823</td>\n",
              "      <td>0.245703</td>\n",
              "      <td>0.022279</td>\n",
              "      <td>0.215786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>0.004394</td>\n",
              "      <td>0.007533</td>\n",
              "      <td>0.012555</td>\n",
              "      <td>0.027621</td>\n",
              "      <td>0.033898</td>\n",
              "      <td>0.045825</td>\n",
              "      <td>0.007533</td>\n",
              "      <td>0.019460</td>\n",
              "      <td>0.009416</td>\n",
              "      <td>0.020088</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012555</td>\n",
              "      <td>0.188324</td>\n",
              "      <td>0.008788</td>\n",
              "      <td>0.116133</td>\n",
              "      <td>0.016949</td>\n",
              "      <td>0.224733</td>\n",
              "      <td>0.020716</td>\n",
              "      <td>0.252982</td>\n",
              "      <td>0.018205</td>\n",
              "      <td>0.221594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>0.005098</td>\n",
              "      <td>0.003642</td>\n",
              "      <td>0.013110</td>\n",
              "      <td>0.026220</td>\n",
              "      <td>0.040787</td>\n",
              "      <td>0.064822</td>\n",
              "      <td>0.008012</td>\n",
              "      <td>0.022578</td>\n",
              "      <td>0.009468</td>\n",
              "      <td>0.024035</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012382</td>\n",
              "      <td>0.182083</td>\n",
              "      <td>0.009468</td>\n",
              "      <td>0.120175</td>\n",
              "      <td>0.020393</td>\n",
              "      <td>0.257101</td>\n",
              "      <td>0.017480</td>\n",
              "      <td>0.246176</td>\n",
              "      <td>0.026948</td>\n",
              "      <td>0.230881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>0.005682</td>\n",
              "      <td>0.008523</td>\n",
              "      <td>0.018466</td>\n",
              "      <td>0.028409</td>\n",
              "      <td>0.036222</td>\n",
              "      <td>0.052557</td>\n",
              "      <td>0.009233</td>\n",
              "      <td>0.024858</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.018466</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009943</td>\n",
              "      <td>0.198153</td>\n",
              "      <td>0.010653</td>\n",
              "      <td>0.113636</td>\n",
              "      <td>0.019886</td>\n",
              "      <td>0.220170</td>\n",
              "      <td>0.029119</td>\n",
              "      <td>0.249290</td>\n",
              "      <td>0.031960</td>\n",
              "      <td>0.242898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>0.008421</td>\n",
              "      <td>0.012632</td>\n",
              "      <td>0.011228</td>\n",
              "      <td>0.023158</td>\n",
              "      <td>0.042105</td>\n",
              "      <td>0.053333</td>\n",
              "      <td>0.007018</td>\n",
              "      <td>0.016140</td>\n",
              "      <td>0.010526</td>\n",
              "      <td>0.026667</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009123</td>\n",
              "      <td>0.206316</td>\n",
              "      <td>0.011930</td>\n",
              "      <td>0.124211</td>\n",
              "      <td>0.013333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.022456</td>\n",
              "      <td>0.236491</td>\n",
              "      <td>0.032281</td>\n",
              "      <td>0.254737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>0.007412</td>\n",
              "      <td>0.012353</td>\n",
              "      <td>0.008030</td>\n",
              "      <td>0.028413</td>\n",
              "      <td>0.043854</td>\n",
              "      <td>0.056825</td>\n",
              "      <td>0.014824</td>\n",
              "      <td>0.025324</td>\n",
              "      <td>0.009265</td>\n",
              "      <td>0.027177</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009883</td>\n",
              "      <td>0.204447</td>\n",
              "      <td>0.011118</td>\n",
              "      <td>0.117356</td>\n",
              "      <td>0.021001</td>\n",
              "      <td>0.232242</td>\n",
              "      <td>0.020383</td>\n",
              "      <td>0.211859</td>\n",
              "      <td>0.030883</td>\n",
              "      <td>0.230389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>0.001319</td>\n",
              "      <td>0.005937</td>\n",
              "      <td>0.012533</td>\n",
              "      <td>0.027704</td>\n",
              "      <td>0.041557</td>\n",
              "      <td>0.056728</td>\n",
              "      <td>0.011214</td>\n",
              "      <td>0.019129</td>\n",
              "      <td>0.010554</td>\n",
              "      <td>0.023087</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011873</td>\n",
              "      <td>0.201187</td>\n",
              "      <td>0.014512</td>\n",
              "      <td>0.140501</td>\n",
              "      <td>0.021108</td>\n",
              "      <td>0.218997</td>\n",
              "      <td>0.021768</td>\n",
              "      <td>0.223615</td>\n",
              "      <td>0.023087</td>\n",
              "      <td>0.241425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>0.002007</td>\n",
              "      <td>0.005351</td>\n",
              "      <td>0.012709</td>\n",
              "      <td>0.022074</td>\n",
              "      <td>0.044816</td>\n",
              "      <td>0.065552</td>\n",
              "      <td>0.011371</td>\n",
              "      <td>0.024080</td>\n",
              "      <td>0.008696</td>\n",
              "      <td>0.025418</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012040</td>\n",
              "      <td>0.195318</td>\n",
              "      <td>0.016722</td>\n",
              "      <td>0.121739</td>\n",
              "      <td>0.019398</td>\n",
              "      <td>0.215385</td>\n",
              "      <td>0.018060</td>\n",
              "      <td>0.239465</td>\n",
              "      <td>0.029431</td>\n",
              "      <td>0.233445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>0.003516</td>\n",
              "      <td>0.006329</td>\n",
              "      <td>0.014065</td>\n",
              "      <td>0.030239</td>\n",
              "      <td>0.048523</td>\n",
              "      <td>0.071730</td>\n",
              "      <td>0.012658</td>\n",
              "      <td>0.028833</td>\n",
              "      <td>0.009142</td>\n",
              "      <td>0.024613</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011252</td>\n",
              "      <td>0.202532</td>\n",
              "      <td>0.014065</td>\n",
              "      <td>0.123769</td>\n",
              "      <td>0.023207</td>\n",
              "      <td>0.248242</td>\n",
              "      <td>0.023910</td>\n",
              "      <td>0.233474</td>\n",
              "      <td>0.028129</td>\n",
              "      <td>0.237693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>0.005204</td>\n",
              "      <td>0.009665</td>\n",
              "      <td>0.008922</td>\n",
              "      <td>0.029740</td>\n",
              "      <td>0.040149</td>\n",
              "      <td>0.058736</td>\n",
              "      <td>0.007435</td>\n",
              "      <td>0.015613</td>\n",
              "      <td>0.013383</td>\n",
              "      <td>0.027509</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006691</td>\n",
              "      <td>0.183643</td>\n",
              "      <td>0.008922</td>\n",
              "      <td>0.130855</td>\n",
              "      <td>0.020818</td>\n",
              "      <td>0.228996</td>\n",
              "      <td>0.024535</td>\n",
              "      <td>0.241636</td>\n",
              "      <td>0.021561</td>\n",
              "      <td>0.249071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>0.009479</td>\n",
              "      <td>0.016588</td>\n",
              "      <td>0.007109</td>\n",
              "      <td>0.020142</td>\n",
              "      <td>0.034953</td>\n",
              "      <td>0.056872</td>\n",
              "      <td>0.008886</td>\n",
              "      <td>0.018365</td>\n",
              "      <td>0.008294</td>\n",
              "      <td>0.023697</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010071</td>\n",
              "      <td>0.200829</td>\n",
              "      <td>0.011848</td>\n",
              "      <td>0.117299</td>\n",
              "      <td>0.020735</td>\n",
              "      <td>0.224526</td>\n",
              "      <td>0.023104</td>\n",
              "      <td>0.241114</td>\n",
              "      <td>0.024289</td>\n",
              "      <td>0.248815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>0.006766</td>\n",
              "      <td>0.010149</td>\n",
              "      <td>0.016915</td>\n",
              "      <td>0.027740</td>\n",
              "      <td>0.053451</td>\n",
              "      <td>0.071042</td>\n",
              "      <td>0.007442</td>\n",
              "      <td>0.021651</td>\n",
              "      <td>0.006089</td>\n",
              "      <td>0.017591</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012179</td>\n",
              "      <td>0.186739</td>\n",
              "      <td>0.011502</td>\n",
              "      <td>0.132612</td>\n",
              "      <td>0.026387</td>\n",
              "      <td>0.228011</td>\n",
              "      <td>0.016238</td>\n",
              "      <td>0.231394</td>\n",
              "      <td>0.018945</td>\n",
              "      <td>0.226658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>0.005044</td>\n",
              "      <td>0.006936</td>\n",
              "      <td>0.015132</td>\n",
              "      <td>0.028373</td>\n",
              "      <td>0.063682</td>\n",
              "      <td>0.086381</td>\n",
              "      <td>0.009458</td>\n",
              "      <td>0.020177</td>\n",
              "      <td>0.004414</td>\n",
              "      <td>0.026482</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014502</td>\n",
              "      <td>0.191677</td>\n",
              "      <td>0.010719</td>\n",
              "      <td>0.117276</td>\n",
              "      <td>0.018916</td>\n",
              "      <td>0.218159</td>\n",
              "      <td>0.020177</td>\n",
              "      <td>0.234552</td>\n",
              "      <td>0.027743</td>\n",
              "      <td>0.239596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>0.004436</td>\n",
              "      <td>0.009506</td>\n",
              "      <td>0.012674</td>\n",
              "      <td>0.036122</td>\n",
              "      <td>0.044360</td>\n",
              "      <td>0.058935</td>\n",
              "      <td>0.010773</td>\n",
              "      <td>0.022814</td>\n",
              "      <td>0.008238</td>\n",
              "      <td>0.022814</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012674</td>\n",
              "      <td>0.200253</td>\n",
              "      <td>0.008238</td>\n",
              "      <td>0.117871</td>\n",
              "      <td>0.021546</td>\n",
              "      <td>0.226869</td>\n",
              "      <td>0.013308</td>\n",
              "      <td>0.232573</td>\n",
              "      <td>0.018378</td>\n",
              "      <td>0.238276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>0.006489</td>\n",
              "      <td>0.009085</td>\n",
              "      <td>0.011681</td>\n",
              "      <td>0.029202</td>\n",
              "      <td>0.033095</td>\n",
              "      <td>0.051265</td>\n",
              "      <td>0.010383</td>\n",
              "      <td>0.024010</td>\n",
              "      <td>0.009085</td>\n",
              "      <td>0.026606</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010383</td>\n",
              "      <td>0.200519</td>\n",
              "      <td>0.012330</td>\n",
              "      <td>0.122648</td>\n",
              "      <td>0.014925</td>\n",
              "      <td>0.217391</td>\n",
              "      <td>0.018170</td>\n",
              "      <td>0.236859</td>\n",
              "      <td>0.022064</td>\n",
              "      <td>0.231668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>0.006901</td>\n",
              "      <td>0.011042</td>\n",
              "      <td>0.019324</td>\n",
              "      <td>0.033126</td>\n",
              "      <td>0.044168</td>\n",
              "      <td>0.072464</td>\n",
              "      <td>0.010352</td>\n",
              "      <td>0.023464</td>\n",
              "      <td>0.007591</td>\n",
              "      <td>0.024155</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013112</td>\n",
              "      <td>0.197378</td>\n",
              "      <td>0.011042</td>\n",
              "      <td>0.132505</td>\n",
              "      <td>0.018634</td>\n",
              "      <td>0.240166</td>\n",
              "      <td>0.019324</td>\n",
              "      <td>0.229124</td>\n",
              "      <td>0.024155</td>\n",
              "      <td>0.250518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>0.006711</td>\n",
              "      <td>0.014094</td>\n",
              "      <td>0.018121</td>\n",
              "      <td>0.036913</td>\n",
              "      <td>0.047651</td>\n",
              "      <td>0.061074</td>\n",
              "      <td>0.012752</td>\n",
              "      <td>0.030201</td>\n",
              "      <td>0.006711</td>\n",
              "      <td>0.029530</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012081</td>\n",
              "      <td>0.203356</td>\n",
              "      <td>0.014765</td>\n",
              "      <td>0.120805</td>\n",
              "      <td>0.020134</td>\n",
              "      <td>0.230201</td>\n",
              "      <td>0.016107</td>\n",
              "      <td>0.228859</td>\n",
              "      <td>0.020805</td>\n",
              "      <td>0.238926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>0.008751</td>\n",
              "      <td>0.014586</td>\n",
              "      <td>0.012252</td>\n",
              "      <td>0.030338</td>\n",
              "      <td>0.039090</td>\n",
              "      <td>0.064177</td>\n",
              "      <td>0.005834</td>\n",
              "      <td>0.018086</td>\n",
              "      <td>0.008168</td>\n",
              "      <td>0.020420</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012252</td>\n",
              "      <td>0.177946</td>\n",
              "      <td>0.012835</td>\n",
              "      <td>0.127188</td>\n",
              "      <td>0.016336</td>\n",
              "      <td>0.235123</td>\n",
              "      <td>0.019837</td>\n",
              "      <td>0.201867</td>\n",
              "      <td>0.023921</td>\n",
              "      <td>0.239790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>0.007555</td>\n",
              "      <td>0.015797</td>\n",
              "      <td>0.014423</td>\n",
              "      <td>0.025412</td>\n",
              "      <td>0.037088</td>\n",
              "      <td>0.061126</td>\n",
              "      <td>0.013049</td>\n",
              "      <td>0.024725</td>\n",
              "      <td>0.006181</td>\n",
              "      <td>0.018544</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012363</td>\n",
              "      <td>0.195742</td>\n",
              "      <td>0.004121</td>\n",
              "      <td>0.117445</td>\n",
              "      <td>0.014423</td>\n",
              "      <td>0.219093</td>\n",
              "      <td>0.016484</td>\n",
              "      <td>0.245192</td>\n",
              "      <td>0.019918</td>\n",
              "      <td>0.245879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>0.003303</td>\n",
              "      <td>0.010568</td>\n",
              "      <td>0.013210</td>\n",
              "      <td>0.032365</td>\n",
              "      <td>0.042933</td>\n",
              "      <td>0.063408</td>\n",
              "      <td>0.011889</td>\n",
              "      <td>0.023778</td>\n",
              "      <td>0.007926</td>\n",
              "      <td>0.021797</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010568</td>\n",
              "      <td>0.175694</td>\n",
              "      <td>0.017173</td>\n",
              "      <td>0.126816</td>\n",
              "      <td>0.016513</td>\n",
              "      <td>0.229855</td>\n",
              "      <td>0.025099</td>\n",
              "      <td>0.235799</td>\n",
              "      <td>0.023118</td>\n",
              "      <td>0.216645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>0.013228</td>\n",
              "      <td>0.016534</td>\n",
              "      <td>0.010582</td>\n",
              "      <td>0.027116</td>\n",
              "      <td>0.050926</td>\n",
              "      <td>0.066799</td>\n",
              "      <td>0.009259</td>\n",
              "      <td>0.019180</td>\n",
              "      <td>0.005952</td>\n",
              "      <td>0.022487</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011243</td>\n",
              "      <td>0.187169</td>\n",
              "      <td>0.009259</td>\n",
              "      <td>0.103175</td>\n",
              "      <td>0.014550</td>\n",
              "      <td>0.220899</td>\n",
              "      <td>0.016534</td>\n",
              "      <td>0.227513</td>\n",
              "      <td>0.030423</td>\n",
              "      <td>0.228836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>0.006077</td>\n",
              "      <td>0.013504</td>\n",
              "      <td>0.008103</td>\n",
              "      <td>0.018231</td>\n",
              "      <td>0.029710</td>\n",
              "      <td>0.045240</td>\n",
              "      <td>0.010804</td>\n",
              "      <td>0.018231</td>\n",
              "      <td>0.004727</td>\n",
              "      <td>0.016880</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012154</td>\n",
              "      <td>0.201891</td>\n",
              "      <td>0.008778</td>\n",
              "      <td>0.118163</td>\n",
              "      <td>0.018231</td>\n",
              "      <td>0.226874</td>\n",
              "      <td>0.019581</td>\n",
              "      <td>0.243079</td>\n",
              "      <td>0.021607</td>\n",
              "      <td>0.232951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>0.011816</td>\n",
              "      <td>0.013682</td>\n",
              "      <td>0.009328</td>\n",
              "      <td>0.027363</td>\n",
              "      <td>0.052239</td>\n",
              "      <td>0.070274</td>\n",
              "      <td>0.006219</td>\n",
              "      <td>0.017413</td>\n",
              "      <td>0.008706</td>\n",
              "      <td>0.024254</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008706</td>\n",
              "      <td>0.187811</td>\n",
              "      <td>0.012438</td>\n",
              "      <td>0.124378</td>\n",
              "      <td>0.025498</td>\n",
              "      <td>0.227612</td>\n",
              "      <td>0.021766</td>\n",
              "      <td>0.235697</td>\n",
              "      <td>0.021766</td>\n",
              "      <td>0.228856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>0.006297</td>\n",
              "      <td>0.015743</td>\n",
              "      <td>0.014484</td>\n",
              "      <td>0.023300</td>\n",
              "      <td>0.030227</td>\n",
              "      <td>0.054156</td>\n",
              "      <td>0.011965</td>\n",
              "      <td>0.026448</td>\n",
              "      <td>0.008816</td>\n",
              "      <td>0.020151</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010076</td>\n",
              "      <td>0.184509</td>\n",
              "      <td>0.011335</td>\n",
              "      <td>0.132872</td>\n",
              "      <td>0.017003</td>\n",
              "      <td>0.214736</td>\n",
              "      <td>0.015743</td>\n",
              "      <td>0.206549</td>\n",
              "      <td>0.031486</td>\n",
              "      <td>0.251889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>0.006615</td>\n",
              "      <td>0.012628</td>\n",
              "      <td>0.013830</td>\n",
              "      <td>0.037282</td>\n",
              "      <td>0.049308</td>\n",
              "      <td>0.067949</td>\n",
              "      <td>0.010222</td>\n",
              "      <td>0.022850</td>\n",
              "      <td>0.009020</td>\n",
              "      <td>0.036681</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007817</td>\n",
              "      <td>0.201443</td>\n",
              "      <td>0.007216</td>\n",
              "      <td>0.123271</td>\n",
              "      <td>0.018641</td>\n",
              "      <td>0.223692</td>\n",
              "      <td>0.017438</td>\n",
              "      <td>0.233915</td>\n",
              "      <td>0.024053</td>\n",
              "      <td>0.227300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>0.008135</td>\n",
              "      <td>0.016270</td>\n",
              "      <td>0.012516</td>\n",
              "      <td>0.024406</td>\n",
              "      <td>0.035670</td>\n",
              "      <td>0.051940</td>\n",
              "      <td>0.005632</td>\n",
              "      <td>0.019399</td>\n",
              "      <td>0.010013</td>\n",
              "      <td>0.028160</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005006</td>\n",
              "      <td>0.178974</td>\n",
              "      <td>0.013141</td>\n",
              "      <td>0.132040</td>\n",
              "      <td>0.012516</td>\n",
              "      <td>0.222153</td>\n",
              "      <td>0.014393</td>\n",
              "      <td>0.238423</td>\n",
              "      <td>0.023154</td>\n",
              "      <td>0.228411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>0.007177</td>\n",
              "      <td>0.012560</td>\n",
              "      <td>0.014354</td>\n",
              "      <td>0.035885</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>0.066388</td>\n",
              "      <td>0.006579</td>\n",
              "      <td>0.019139</td>\n",
              "      <td>0.009569</td>\n",
              "      <td>0.024522</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010167</td>\n",
              "      <td>0.202153</td>\n",
              "      <td>0.011364</td>\n",
              "      <td>0.117225</td>\n",
              "      <td>0.014354</td>\n",
              "      <td>0.209330</td>\n",
              "      <td>0.017943</td>\n",
              "      <td>0.247608</td>\n",
              "      <td>0.018541</td>\n",
              "      <td>0.230263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>0.009759</td>\n",
              "      <td>0.013663</td>\n",
              "      <td>0.013012</td>\n",
              "      <td>0.022772</td>\n",
              "      <td>0.031880</td>\n",
              "      <td>0.053351</td>\n",
              "      <td>0.003904</td>\n",
              "      <td>0.018217</td>\n",
              "      <td>0.007807</td>\n",
              "      <td>0.021470</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011711</td>\n",
              "      <td>0.184776</td>\n",
              "      <td>0.012362</td>\n",
              "      <td>0.121666</td>\n",
              "      <td>0.020820</td>\n",
              "      <td>0.235524</td>\n",
              "      <td>0.022121</td>\n",
              "      <td>0.219909</td>\n",
              "      <td>0.029278</td>\n",
              "      <td>0.226415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>0.006567</td>\n",
              "      <td>0.013134</td>\n",
              "      <td>0.008955</td>\n",
              "      <td>0.022687</td>\n",
              "      <td>0.038806</td>\n",
              "      <td>0.051940</td>\n",
              "      <td>0.009552</td>\n",
              "      <td>0.022090</td>\n",
              "      <td>0.007164</td>\n",
              "      <td>0.030448</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009552</td>\n",
              "      <td>0.174328</td>\n",
              "      <td>0.008358</td>\n",
              "      <td>0.125373</td>\n",
              "      <td>0.019701</td>\n",
              "      <td>0.237015</td>\n",
              "      <td>0.017910</td>\n",
              "      <td>0.202985</td>\n",
              "      <td>0.014328</td>\n",
              "      <td>0.222090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>0.007058</td>\n",
              "      <td>0.016287</td>\n",
              "      <td>0.013572</td>\n",
              "      <td>0.028230</td>\n",
              "      <td>0.041802</td>\n",
              "      <td>0.060803</td>\n",
              "      <td>0.007058</td>\n",
              "      <td>0.019544</td>\n",
              "      <td>0.004886</td>\n",
              "      <td>0.029859</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007058</td>\n",
              "      <td>0.192182</td>\n",
              "      <td>0.010858</td>\n",
              "      <td>0.129207</td>\n",
              "      <td>0.021716</td>\n",
              "      <td>0.226384</td>\n",
              "      <td>0.016830</td>\n",
              "      <td>0.222041</td>\n",
              "      <td>0.026059</td>\n",
              "      <td>0.238871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238</th>\n",
              "      <td>0.006897</td>\n",
              "      <td>0.011285</td>\n",
              "      <td>0.021317</td>\n",
              "      <td>0.032602</td>\n",
              "      <td>0.042006</td>\n",
              "      <td>0.063950</td>\n",
              "      <td>0.010031</td>\n",
              "      <td>0.020690</td>\n",
              "      <td>0.005016</td>\n",
              "      <td>0.026332</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010658</td>\n",
              "      <td>0.191223</td>\n",
              "      <td>0.011912</td>\n",
              "      <td>0.124765</td>\n",
              "      <td>0.017555</td>\n",
              "      <td>0.221317</td>\n",
              "      <td>0.018182</td>\n",
              "      <td>0.223197</td>\n",
              "      <td>0.023197</td>\n",
              "      <td>0.221317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>0.006271</td>\n",
              "      <td>0.015964</td>\n",
              "      <td>0.010832</td>\n",
              "      <td>0.027366</td>\n",
              "      <td>0.042189</td>\n",
              "      <td>0.057013</td>\n",
              "      <td>0.006842</td>\n",
              "      <td>0.014823</td>\n",
              "      <td>0.006842</td>\n",
              "      <td>0.026226</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011403</td>\n",
              "      <td>0.192132</td>\n",
              "      <td>0.011973</td>\n",
              "      <td>0.118586</td>\n",
              "      <td>0.011403</td>\n",
              "      <td>0.221779</td>\n",
              "      <td>0.023375</td>\n",
              "      <td>0.220639</td>\n",
              "      <td>0.022805</td>\n",
              "      <td>0.216078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>0.011765</td>\n",
              "      <td>0.018954</td>\n",
              "      <td>0.016993</td>\n",
              "      <td>0.035948</td>\n",
              "      <td>0.028758</td>\n",
              "      <td>0.040523</td>\n",
              "      <td>0.006536</td>\n",
              "      <td>0.015686</td>\n",
              "      <td>0.009804</td>\n",
              "      <td>0.031373</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011765</td>\n",
              "      <td>0.182353</td>\n",
              "      <td>0.009804</td>\n",
              "      <td>0.108497</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.217647</td>\n",
              "      <td>0.020261</td>\n",
              "      <td>0.226144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>0.009862</td>\n",
              "      <td>0.017751</td>\n",
              "      <td>0.008547</td>\n",
              "      <td>0.019066</td>\n",
              "      <td>0.034188</td>\n",
              "      <td>0.054569</td>\n",
              "      <td>0.008547</td>\n",
              "      <td>0.021039</td>\n",
              "      <td>0.003287</td>\n",
              "      <td>0.015122</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012492</td>\n",
              "      <td>0.190007</td>\n",
              "      <td>0.011834</td>\n",
              "      <td>0.116371</td>\n",
              "      <td>0.019066</td>\n",
              "      <td>0.206443</td>\n",
              "      <td>0.021696</td>\n",
              "      <td>0.207101</td>\n",
              "      <td>0.020381</td>\n",
              "      <td>0.234714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>0.010613</td>\n",
              "      <td>0.021816</td>\n",
              "      <td>0.012972</td>\n",
              "      <td>0.025943</td>\n",
              "      <td>0.039505</td>\n",
              "      <td>0.060142</td>\n",
              "      <td>0.005307</td>\n",
              "      <td>0.020047</td>\n",
              "      <td>0.007665</td>\n",
              "      <td>0.025354</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013561</td>\n",
              "      <td>0.192217</td>\n",
              "      <td>0.015330</td>\n",
              "      <td>0.133844</td>\n",
              "      <td>0.022406</td>\n",
              "      <td>0.219929</td>\n",
              "      <td>0.023585</td>\n",
              "      <td>0.228184</td>\n",
              "      <td>0.022995</td>\n",
              "      <td>0.248821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>0.012493</td>\n",
              "      <td>0.025554</td>\n",
              "      <td>0.019875</td>\n",
              "      <td>0.034639</td>\n",
              "      <td>0.035207</td>\n",
              "      <td>0.057354</td>\n",
              "      <td>0.011357</td>\n",
              "      <td>0.024418</td>\n",
              "      <td>0.007382</td>\n",
              "      <td>0.031232</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003975</td>\n",
              "      <td>0.176036</td>\n",
              "      <td>0.013629</td>\n",
              "      <td>0.128336</td>\n",
              "      <td>0.015332</td>\n",
              "      <td>0.233958</td>\n",
              "      <td>0.019875</td>\n",
              "      <td>0.223737</td>\n",
              "      <td>0.024986</td>\n",
              "      <td>0.223737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>0.011871</td>\n",
              "      <td>0.025438</td>\n",
              "      <td>0.010741</td>\n",
              "      <td>0.026003</td>\n",
              "      <td>0.047484</td>\n",
              "      <td>0.065008</td>\n",
              "      <td>0.003957</td>\n",
              "      <td>0.014698</td>\n",
              "      <td>0.006218</td>\n",
              "      <td>0.023742</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011871</td>\n",
              "      <td>0.184850</td>\n",
              "      <td>0.010741</td>\n",
              "      <td>0.113058</td>\n",
              "      <td>0.014698</td>\n",
              "      <td>0.225551</td>\n",
              "      <td>0.019785</td>\n",
              "      <td>0.231769</td>\n",
              "      <td>0.025438</td>\n",
              "      <td>0.234031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>0.013468</td>\n",
              "      <td>0.024691</td>\n",
              "      <td>0.008418</td>\n",
              "      <td>0.025253</td>\n",
              "      <td>0.043210</td>\n",
              "      <td>0.065657</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.022447</td>\n",
              "      <td>0.005051</td>\n",
              "      <td>0.022447</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.177890</td>\n",
              "      <td>0.011785</td>\n",
              "      <td>0.109989</td>\n",
              "      <td>0.017396</td>\n",
              "      <td>0.218855</td>\n",
              "      <td>0.016835</td>\n",
              "      <td>0.231762</td>\n",
              "      <td>0.017957</td>\n",
              "      <td>0.220539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>0.009855</td>\n",
              "      <td>0.019191</td>\n",
              "      <td>0.012967</td>\n",
              "      <td>0.031120</td>\n",
              "      <td>0.043568</td>\n",
              "      <td>0.062759</td>\n",
              "      <td>0.006743</td>\n",
              "      <td>0.019710</td>\n",
              "      <td>0.006224</td>\n",
              "      <td>0.021784</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006743</td>\n",
              "      <td>0.193983</td>\n",
              "      <td>0.011929</td>\n",
              "      <td>0.123963</td>\n",
              "      <td>0.022303</td>\n",
              "      <td>0.228734</td>\n",
              "      <td>0.021784</td>\n",
              "      <td>0.241183</td>\n",
              "      <td>0.020228</td>\n",
              "      <td>0.226141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>0.011786</td>\n",
              "      <td>0.022392</td>\n",
              "      <td>0.015321</td>\n",
              "      <td>0.027696</td>\n",
              "      <td>0.038303</td>\n",
              "      <td>0.061874</td>\n",
              "      <td>0.005893</td>\n",
              "      <td>0.016500</td>\n",
              "      <td>0.008839</td>\n",
              "      <td>0.024750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010607</td>\n",
              "      <td>0.206246</td>\n",
              "      <td>0.008250</td>\n",
              "      <td>0.113141</td>\n",
              "      <td>0.012964</td>\n",
              "      <td>0.217443</td>\n",
              "      <td>0.015321</td>\n",
              "      <td>0.214496</td>\n",
              "      <td>0.018857</td>\n",
              "      <td>0.242192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>0.007986</td>\n",
              "      <td>0.014832</td>\n",
              "      <td>0.013120</td>\n",
              "      <td>0.022248</td>\n",
              "      <td>0.036509</td>\n",
              "      <td>0.055904</td>\n",
              "      <td>0.007986</td>\n",
              "      <td>0.018254</td>\n",
              "      <td>0.007416</td>\n",
              "      <td>0.025670</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011409</td>\n",
              "      <td>0.187108</td>\n",
              "      <td>0.007986</td>\n",
              "      <td>0.124929</td>\n",
              "      <td>0.017684</td>\n",
              "      <td>0.219053</td>\n",
              "      <td>0.016543</td>\n",
              "      <td>0.225898</td>\n",
              "      <td>0.020536</td>\n",
              "      <td>0.240730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>0.010863</td>\n",
              "      <td>0.027444</td>\n",
              "      <td>0.014866</td>\n",
              "      <td>0.028588</td>\n",
              "      <td>0.034877</td>\n",
              "      <td>0.052030</td>\n",
              "      <td>0.005718</td>\n",
              "      <td>0.015437</td>\n",
              "      <td>0.008576</td>\n",
              "      <td>0.024585</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011435</td>\n",
              "      <td>0.198971</td>\n",
              "      <td>0.005718</td>\n",
              "      <td>0.112064</td>\n",
              "      <td>0.012007</td>\n",
              "      <td>0.202401</td>\n",
              "      <td>0.022298</td>\n",
              "      <td>0.233848</td>\n",
              "      <td>0.027444</td>\n",
              "      <td>0.240137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>0.009050</td>\n",
              "      <td>0.022059</td>\n",
              "      <td>0.007919</td>\n",
              "      <td>0.016403</td>\n",
              "      <td>0.036199</td>\n",
              "      <td>0.060520</td>\n",
              "      <td>0.005090</td>\n",
              "      <td>0.017534</td>\n",
              "      <td>0.005656</td>\n",
              "      <td>0.026584</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006787</td>\n",
              "      <td>0.186652</td>\n",
              "      <td>0.012443</td>\n",
              "      <td>0.119344</td>\n",
              "      <td>0.014706</td>\n",
              "      <td>0.229072</td>\n",
              "      <td>0.020362</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.026018</td>\n",
              "      <td>0.244344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>0.007837</td>\n",
              "      <td>0.021944</td>\n",
              "      <td>0.013062</td>\n",
              "      <td>0.026123</td>\n",
              "      <td>0.036573</td>\n",
              "      <td>0.055381</td>\n",
              "      <td>0.008359</td>\n",
              "      <td>0.021944</td>\n",
              "      <td>0.003657</td>\n",
              "      <td>0.020899</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008359</td>\n",
              "      <td>0.188610</td>\n",
              "      <td>0.014629</td>\n",
              "      <td>0.127482</td>\n",
              "      <td>0.016719</td>\n",
              "      <td>0.223093</td>\n",
              "      <td>0.021944</td>\n",
              "      <td>0.225705</td>\n",
              "      <td>0.015674</td>\n",
              "      <td>0.236677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>0.013601</td>\n",
              "      <td>0.031088</td>\n",
              "      <td>0.013601</td>\n",
              "      <td>0.028497</td>\n",
              "      <td>0.032383</td>\n",
              "      <td>0.047280</td>\n",
              "      <td>0.006477</td>\n",
              "      <td>0.020078</td>\n",
              "      <td>0.004534</td>\n",
              "      <td>0.018135</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009715</td>\n",
              "      <td>0.185881</td>\n",
              "      <td>0.010363</td>\n",
              "      <td>0.106218</td>\n",
              "      <td>0.014896</td>\n",
              "      <td>0.218264</td>\n",
              "      <td>0.018782</td>\n",
              "      <td>0.229275</td>\n",
              "      <td>0.026554</td>\n",
              "      <td>0.242876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>0.011553</td>\n",
              "      <td>0.023107</td>\n",
              "      <td>0.007060</td>\n",
              "      <td>0.021181</td>\n",
              "      <td>0.034660</td>\n",
              "      <td>0.053915</td>\n",
              "      <td>0.005777</td>\n",
              "      <td>0.015404</td>\n",
              "      <td>0.005777</td>\n",
              "      <td>0.021823</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006418</td>\n",
              "      <td>0.175225</td>\n",
              "      <td>0.011553</td>\n",
              "      <td>0.127086</td>\n",
              "      <td>0.013479</td>\n",
              "      <td>0.211810</td>\n",
              "      <td>0.016688</td>\n",
              "      <td>0.224005</td>\n",
              "      <td>0.024390</td>\n",
              "      <td>0.238768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>0.007067</td>\n",
              "      <td>0.019687</td>\n",
              "      <td>0.008582</td>\n",
              "      <td>0.020697</td>\n",
              "      <td>0.038869</td>\n",
              "      <td>0.056537</td>\n",
              "      <td>0.007067</td>\n",
              "      <td>0.022716</td>\n",
              "      <td>0.007067</td>\n",
              "      <td>0.027259</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010096</td>\n",
              "      <td>0.172640</td>\n",
              "      <td>0.008077</td>\n",
              "      <td>0.115093</td>\n",
              "      <td>0.014639</td>\n",
              "      <td>0.220091</td>\n",
              "      <td>0.017163</td>\n",
              "      <td>0.226148</td>\n",
              "      <td>0.026249</td>\n",
              "      <td>0.241292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>0.008323</td>\n",
              "      <td>0.016052</td>\n",
              "      <td>0.013080</td>\n",
              "      <td>0.022592</td>\n",
              "      <td>0.038644</td>\n",
              "      <td>0.058859</td>\n",
              "      <td>0.004756</td>\n",
              "      <td>0.016647</td>\n",
              "      <td>0.006540</td>\n",
              "      <td>0.023781</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010702</td>\n",
              "      <td>0.188466</td>\n",
              "      <td>0.011296</td>\n",
              "      <td>0.117717</td>\n",
              "      <td>0.015458</td>\n",
              "      <td>0.243163</td>\n",
              "      <td>0.019025</td>\n",
              "      <td>0.219382</td>\n",
              "      <td>0.030321</td>\n",
              "      <td>0.245541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>0.009884</td>\n",
              "      <td>0.019767</td>\n",
              "      <td>0.011047</td>\n",
              "      <td>0.021512</td>\n",
              "      <td>0.027326</td>\n",
              "      <td>0.045349</td>\n",
              "      <td>0.007558</td>\n",
              "      <td>0.020349</td>\n",
              "      <td>0.006395</td>\n",
              "      <td>0.022093</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010465</td>\n",
              "      <td>0.211047</td>\n",
              "      <td>0.008721</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.019767</td>\n",
              "      <td>0.220930</td>\n",
              "      <td>0.016279</td>\n",
              "      <td>0.209884</td>\n",
              "      <td>0.022674</td>\n",
              "      <td>0.238372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>0.010435</td>\n",
              "      <td>0.028406</td>\n",
              "      <td>0.011014</td>\n",
              "      <td>0.026087</td>\n",
              "      <td>0.042319</td>\n",
              "      <td>0.064348</td>\n",
              "      <td>0.006377</td>\n",
              "      <td>0.016232</td>\n",
              "      <td>0.006957</td>\n",
              "      <td>0.029565</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008116</td>\n",
              "      <td>0.180870</td>\n",
              "      <td>0.004058</td>\n",
              "      <td>0.111884</td>\n",
              "      <td>0.014493</td>\n",
              "      <td>0.234203</td>\n",
              "      <td>0.020870</td>\n",
              "      <td>0.239420</td>\n",
              "      <td>0.025507</td>\n",
              "      <td>0.251014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>0.013978</td>\n",
              "      <td>0.026209</td>\n",
              "      <td>0.011066</td>\n",
              "      <td>0.029703</td>\n",
              "      <td>0.036109</td>\n",
              "      <td>0.053582</td>\n",
              "      <td>0.006989</td>\n",
              "      <td>0.015725</td>\n",
              "      <td>0.004077</td>\n",
              "      <td>0.029703</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007571</td>\n",
              "      <td>0.172394</td>\n",
              "      <td>0.009901</td>\n",
              "      <td>0.133372</td>\n",
              "      <td>0.019220</td>\n",
              "      <td>0.222481</td>\n",
              "      <td>0.018055</td>\n",
              "      <td>0.232382</td>\n",
              "      <td>0.022714</td>\n",
              "      <td>0.230635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>0.010821</td>\n",
              "      <td>0.027371</td>\n",
              "      <td>0.010821</td>\n",
              "      <td>0.028644</td>\n",
              "      <td>0.034373</td>\n",
              "      <td>0.055379</td>\n",
              "      <td>0.007002</td>\n",
              "      <td>0.017823</td>\n",
              "      <td>0.001273</td>\n",
              "      <td>0.025461</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008275</td>\n",
              "      <td>0.184596</td>\n",
              "      <td>0.008912</td>\n",
              "      <td>0.120306</td>\n",
              "      <td>0.013367</td>\n",
              "      <td>0.211967</td>\n",
              "      <td>0.017187</td>\n",
              "      <td>0.213240</td>\n",
              "      <td>0.024188</td>\n",
              "      <td>0.246976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>0.009858</td>\n",
              "      <td>0.019717</td>\n",
              "      <td>0.007394</td>\n",
              "      <td>0.027110</td>\n",
              "      <td>0.037585</td>\n",
              "      <td>0.057301</td>\n",
              "      <td>0.003697</td>\n",
              "      <td>0.014171</td>\n",
              "      <td>0.008626</td>\n",
              "      <td>0.030191</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007394</td>\n",
              "      <td>0.191620</td>\n",
              "      <td>0.009858</td>\n",
              "      <td>0.117683</td>\n",
              "      <td>0.015404</td>\n",
              "      <td>0.209489</td>\n",
              "      <td>0.021565</td>\n",
              "      <td>0.229205</td>\n",
              "      <td>0.024030</td>\n",
              "      <td>0.245841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>0.014140</td>\n",
              "      <td>0.023190</td>\n",
              "      <td>0.009615</td>\n",
              "      <td>0.028281</td>\n",
              "      <td>0.034502</td>\n",
              "      <td>0.055430</td>\n",
              "      <td>0.007919</td>\n",
              "      <td>0.017534</td>\n",
              "      <td>0.006787</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007919</td>\n",
              "      <td>0.188914</td>\n",
              "      <td>0.009615</td>\n",
              "      <td>0.114253</td>\n",
              "      <td>0.016968</td>\n",
              "      <td>0.221719</td>\n",
              "      <td>0.018665</td>\n",
              "      <td>0.238122</td>\n",
              "      <td>0.023190</td>\n",
              "      <td>0.230769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>0.016298</td>\n",
              "      <td>0.031432</td>\n",
              "      <td>0.008731</td>\n",
              "      <td>0.026193</td>\n",
              "      <td>0.037253</td>\n",
              "      <td>0.056461</td>\n",
              "      <td>0.009313</td>\n",
              "      <td>0.018626</td>\n",
              "      <td>0.008149</td>\n",
              "      <td>0.025611</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010477</td>\n",
              "      <td>0.189173</td>\n",
              "      <td>0.014552</td>\n",
              "      <td>0.128056</td>\n",
              "      <td>0.011059</td>\n",
              "      <td>0.213620</td>\n",
              "      <td>0.018626</td>\n",
              "      <td>0.213620</td>\n",
              "      <td>0.023283</td>\n",
              "      <td>0.242724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>0.012379</td>\n",
              "      <td>0.020990</td>\n",
              "      <td>0.011841</td>\n",
              "      <td>0.027449</td>\n",
              "      <td>0.037137</td>\n",
              "      <td>0.054360</td>\n",
              "      <td>0.008073</td>\n",
              "      <td>0.019376</td>\n",
              "      <td>0.008073</td>\n",
              "      <td>0.037675</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009150</td>\n",
              "      <td>0.184069</td>\n",
              "      <td>0.005920</td>\n",
              "      <td>0.107643</td>\n",
              "      <td>0.018837</td>\n",
              "      <td>0.235199</td>\n",
              "      <td>0.022605</td>\n",
              "      <td>0.242734</td>\n",
              "      <td>0.025834</td>\n",
              "      <td>0.241119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>0.016960</td>\n",
              "      <td>0.030779</td>\n",
              "      <td>0.006281</td>\n",
              "      <td>0.018216</td>\n",
              "      <td>0.030779</td>\n",
              "      <td>0.045854</td>\n",
              "      <td>0.005653</td>\n",
              "      <td>0.010050</td>\n",
              "      <td>0.008166</td>\n",
              "      <td>0.028894</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008166</td>\n",
              "      <td>0.189698</td>\n",
              "      <td>0.010678</td>\n",
              "      <td>0.122487</td>\n",
              "      <td>0.008794</td>\n",
              "      <td>0.211055</td>\n",
              "      <td>0.020101</td>\n",
              "      <td>0.231156</td>\n",
              "      <td>0.023869</td>\n",
              "      <td>0.233040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>0.017210</td>\n",
              "      <td>0.035034</td>\n",
              "      <td>0.010449</td>\n",
              "      <td>0.024585</td>\n",
              "      <td>0.030117</td>\n",
              "      <td>0.051629</td>\n",
              "      <td>0.005532</td>\n",
              "      <td>0.011063</td>\n",
              "      <td>0.009834</td>\n",
              "      <td>0.029502</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011678</td>\n",
              "      <td>0.215120</td>\n",
              "      <td>0.011678</td>\n",
              "      <td>0.111862</td>\n",
              "      <td>0.020283</td>\n",
              "      <td>0.233559</td>\n",
              "      <td>0.016595</td>\n",
              "      <td>0.234173</td>\n",
              "      <td>0.028273</td>\n",
              "      <td>0.256915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>0.013492</td>\n",
              "      <td>0.029142</td>\n",
              "      <td>0.008635</td>\n",
              "      <td>0.023206</td>\n",
              "      <td>0.031301</td>\n",
              "      <td>0.051268</td>\n",
              "      <td>0.009174</td>\n",
              "      <td>0.017809</td>\n",
              "      <td>0.005936</td>\n",
              "      <td>0.023206</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013492</td>\n",
              "      <td>0.177010</td>\n",
              "      <td>0.007016</td>\n",
              "      <td>0.121425</td>\n",
              "      <td>0.018888</td>\n",
              "      <td>0.212628</td>\n",
              "      <td>0.020507</td>\n",
              "      <td>0.219644</td>\n",
              "      <td>0.021047</td>\n",
              "      <td>0.238532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>0.007208</td>\n",
              "      <td>0.022936</td>\n",
              "      <td>0.007208</td>\n",
              "      <td>0.024246</td>\n",
              "      <td>0.033421</td>\n",
              "      <td>0.050459</td>\n",
              "      <td>0.004587</td>\n",
              "      <td>0.015072</td>\n",
              "      <td>0.007208</td>\n",
              "      <td>0.020970</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011140</td>\n",
              "      <td>0.186763</td>\n",
              "      <td>0.004587</td>\n",
              "      <td>0.111402</td>\n",
              "      <td>0.013761</td>\n",
              "      <td>0.233290</td>\n",
              "      <td>0.020315</td>\n",
              "      <td>0.222805</td>\n",
              "      <td>0.025557</td>\n",
              "      <td>0.249017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>0.012173</td>\n",
              "      <td>0.034693</td>\n",
              "      <td>0.013999</td>\n",
              "      <td>0.028606</td>\n",
              "      <td>0.027998</td>\n",
              "      <td>0.040170</td>\n",
              "      <td>0.007912</td>\n",
              "      <td>0.017651</td>\n",
              "      <td>0.007304</td>\n",
              "      <td>0.019477</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010347</td>\n",
              "      <td>0.189288</td>\n",
              "      <td>0.009738</td>\n",
              "      <td>0.118077</td>\n",
              "      <td>0.014607</td>\n",
              "      <td>0.218503</td>\n",
              "      <td>0.015825</td>\n",
              "      <td>0.231893</td>\n",
              "      <td>0.018259</td>\n",
              "      <td>0.220329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>0.013530</td>\n",
              "      <td>0.026445</td>\n",
              "      <td>0.011070</td>\n",
              "      <td>0.019680</td>\n",
              "      <td>0.044280</td>\n",
              "      <td>0.062116</td>\n",
              "      <td>0.005535</td>\n",
              "      <td>0.017220</td>\n",
              "      <td>0.006765</td>\n",
              "      <td>0.023370</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005535</td>\n",
              "      <td>0.191267</td>\n",
              "      <td>0.010455</td>\n",
              "      <td>0.121156</td>\n",
              "      <td>0.020295</td>\n",
              "      <td>0.231857</td>\n",
              "      <td>0.019065</td>\n",
              "      <td>0.222632</td>\n",
              "      <td>0.016605</td>\n",
              "      <td>0.218942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>0.012971</td>\n",
              "      <td>0.028413</td>\n",
              "      <td>0.011736</td>\n",
              "      <td>0.025324</td>\n",
              "      <td>0.032736</td>\n",
              "      <td>0.053737</td>\n",
              "      <td>0.007412</td>\n",
              "      <td>0.018530</td>\n",
              "      <td>0.002471</td>\n",
              "      <td>0.025324</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008030</td>\n",
              "      <td>0.172329</td>\n",
              "      <td>0.012971</td>\n",
              "      <td>0.112415</td>\n",
              "      <td>0.016677</td>\n",
              "      <td>0.222359</td>\n",
              "      <td>0.014824</td>\n",
              "      <td>0.205065</td>\n",
              "      <td>0.020383</td>\n",
              "      <td>0.225448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>0.018856</td>\n",
              "      <td>0.035827</td>\n",
              "      <td>0.008799</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.036455</td>\n",
              "      <td>0.057197</td>\n",
              "      <td>0.006914</td>\n",
              "      <td>0.017599</td>\n",
              "      <td>0.007542</td>\n",
              "      <td>0.026398</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009428</td>\n",
              "      <td>0.179133</td>\n",
              "      <td>0.007542</td>\n",
              "      <td>0.115651</td>\n",
              "      <td>0.011942</td>\n",
              "      <td>0.225644</td>\n",
              "      <td>0.020113</td>\n",
              "      <td>0.222502</td>\n",
              "      <td>0.022627</td>\n",
              "      <td>0.252043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>0.010284</td>\n",
              "      <td>0.029643</td>\n",
              "      <td>0.009679</td>\n",
              "      <td>0.018149</td>\n",
              "      <td>0.028433</td>\n",
              "      <td>0.045372</td>\n",
              "      <td>0.004840</td>\n",
              "      <td>0.016334</td>\n",
              "      <td>0.006655</td>\n",
              "      <td>0.029643</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008469</td>\n",
              "      <td>0.188143</td>\n",
              "      <td>0.011494</td>\n",
              "      <td>0.113733</td>\n",
              "      <td>0.012704</td>\n",
              "      <td>0.231095</td>\n",
              "      <td>0.019964</td>\n",
              "      <td>0.228675</td>\n",
              "      <td>0.022989</td>\n",
              "      <td>0.228675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>0.014043</td>\n",
              "      <td>0.033353</td>\n",
              "      <td>0.008192</td>\n",
              "      <td>0.019895</td>\n",
              "      <td>0.037449</td>\n",
              "      <td>0.060269</td>\n",
              "      <td>0.006437</td>\n",
              "      <td>0.018139</td>\n",
              "      <td>0.008777</td>\n",
              "      <td>0.028672</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014043</td>\n",
              "      <td>0.202458</td>\n",
              "      <td>0.010532</td>\n",
              "      <td>0.126390</td>\n",
              "      <td>0.018724</td>\n",
              "      <td>0.225278</td>\n",
              "      <td>0.017554</td>\n",
              "      <td>0.241662</td>\n",
              "      <td>0.026331</td>\n",
              "      <td>0.233470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>0.016696</td>\n",
              "      <td>0.031604</td>\n",
              "      <td>0.005963</td>\n",
              "      <td>0.020274</td>\n",
              "      <td>0.033393</td>\n",
              "      <td>0.050089</td>\n",
              "      <td>0.007156</td>\n",
              "      <td>0.023852</td>\n",
              "      <td>0.007156</td>\n",
              "      <td>0.023852</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009541</td>\n",
              "      <td>0.189028</td>\n",
              "      <td>0.011330</td>\n",
              "      <td>0.119857</td>\n",
              "      <td>0.017889</td>\n",
              "      <td>0.231366</td>\n",
              "      <td>0.018485</td>\n",
              "      <td>0.237925</td>\n",
              "      <td>0.022660</td>\n",
              "      <td>0.238521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>0.015528</td>\n",
              "      <td>0.033540</td>\n",
              "      <td>0.014907</td>\n",
              "      <td>0.026708</td>\n",
              "      <td>0.032919</td>\n",
              "      <td>0.048447</td>\n",
              "      <td>0.005590</td>\n",
              "      <td>0.022360</td>\n",
              "      <td>0.010559</td>\n",
              "      <td>0.028571</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007453</td>\n",
              "      <td>0.178882</td>\n",
              "      <td>0.010559</td>\n",
              "      <td>0.121739</td>\n",
              "      <td>0.009938</td>\n",
              "      <td>0.226087</td>\n",
              "      <td>0.015528</td>\n",
              "      <td>0.215528</td>\n",
              "      <td>0.023602</td>\n",
              "      <td>0.225466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>0.012953</td>\n",
              "      <td>0.029793</td>\n",
              "      <td>0.012306</td>\n",
              "      <td>0.023316</td>\n",
              "      <td>0.035622</td>\n",
              "      <td>0.058290</td>\n",
              "      <td>0.007772</td>\n",
              "      <td>0.020078</td>\n",
              "      <td>0.002591</td>\n",
              "      <td>0.023316</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012953</td>\n",
              "      <td>0.207254</td>\n",
              "      <td>0.009067</td>\n",
              "      <td>0.126295</td>\n",
              "      <td>0.021373</td>\n",
              "      <td>0.228627</td>\n",
              "      <td>0.022668</td>\n",
              "      <td>0.232513</td>\n",
              "      <td>0.018782</td>\n",
              "      <td>0.241580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>0.009536</td>\n",
              "      <td>0.024793</td>\n",
              "      <td>0.010172</td>\n",
              "      <td>0.019708</td>\n",
              "      <td>0.026065</td>\n",
              "      <td>0.038779</td>\n",
              "      <td>0.005722</td>\n",
              "      <td>0.017800</td>\n",
              "      <td>0.008264</td>\n",
              "      <td>0.023522</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012079</td>\n",
              "      <td>0.190718</td>\n",
              "      <td>0.010172</td>\n",
              "      <td>0.125874</td>\n",
              "      <td>0.024158</td>\n",
              "      <td>0.200890</td>\n",
              "      <td>0.026065</td>\n",
              "      <td>0.212969</td>\n",
              "      <td>0.023522</td>\n",
              "      <td>0.244755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>0.016824</td>\n",
              "      <td>0.025762</td>\n",
              "      <td>0.006835</td>\n",
              "      <td>0.022608</td>\n",
              "      <td>0.031546</td>\n",
              "      <td>0.052050</td>\n",
              "      <td>0.009989</td>\n",
              "      <td>0.020505</td>\n",
              "      <td>0.008938</td>\n",
              "      <td>0.024185</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011041</td>\n",
              "      <td>0.181914</td>\n",
              "      <td>0.007361</td>\n",
              "      <td>0.120925</td>\n",
              "      <td>0.019453</td>\n",
              "      <td>0.229758</td>\n",
              "      <td>0.021556</td>\n",
              "      <td>0.231335</td>\n",
              "      <td>0.028917</td>\n",
              "      <td>0.243428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>0.018205</td>\n",
              "      <td>0.033160</td>\n",
              "      <td>0.011053</td>\n",
              "      <td>0.026658</td>\n",
              "      <td>0.030559</td>\n",
              "      <td>0.049415</td>\n",
              "      <td>0.006502</td>\n",
              "      <td>0.018856</td>\n",
              "      <td>0.007802</td>\n",
              "      <td>0.026658</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007802</td>\n",
              "      <td>0.191808</td>\n",
              "      <td>0.009103</td>\n",
              "      <td>0.132640</td>\n",
              "      <td>0.014304</td>\n",
              "      <td>0.222367</td>\n",
              "      <td>0.018856</td>\n",
              "      <td>0.223667</td>\n",
              "      <td>0.018205</td>\n",
              "      <td>0.241873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>0.014239</td>\n",
              "      <td>0.032859</td>\n",
              "      <td>0.007667</td>\n",
              "      <td>0.020263</td>\n",
              "      <td>0.035049</td>\n",
              "      <td>0.053122</td>\n",
              "      <td>0.007119</td>\n",
              "      <td>0.013143</td>\n",
              "      <td>0.009858</td>\n",
              "      <td>0.030120</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007667</td>\n",
              "      <td>0.194962</td>\n",
              "      <td>0.007667</td>\n",
              "      <td>0.135268</td>\n",
              "      <td>0.015882</td>\n",
              "      <td>0.200986</td>\n",
              "      <td>0.012596</td>\n",
              "      <td>0.233844</td>\n",
              "      <td>0.019168</td>\n",
              "      <td>0.232202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281</th>\n",
              "      <td>0.018878</td>\n",
              "      <td>0.037757</td>\n",
              "      <td>0.005552</td>\n",
              "      <td>0.014992</td>\n",
              "      <td>0.029983</td>\n",
              "      <td>0.044975</td>\n",
              "      <td>0.007773</td>\n",
              "      <td>0.013881</td>\n",
              "      <td>0.004442</td>\n",
              "      <td>0.026097</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007773</td>\n",
              "      <td>0.162132</td>\n",
              "      <td>0.011105</td>\n",
              "      <td>0.126041</td>\n",
              "      <td>0.009994</td>\n",
              "      <td>0.210439</td>\n",
              "      <td>0.016657</td>\n",
              "      <td>0.213770</td>\n",
              "      <td>0.017768</td>\n",
              "      <td>0.207107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>0.014180</td>\n",
              "      <td>0.030630</td>\n",
              "      <td>0.009075</td>\n",
              "      <td>0.019853</td>\n",
              "      <td>0.034600</td>\n",
              "      <td>0.048213</td>\n",
              "      <td>0.006239</td>\n",
              "      <td>0.014748</td>\n",
              "      <td>0.004538</td>\n",
              "      <td>0.020987</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012479</td>\n",
              "      <td>0.175269</td>\n",
              "      <td>0.007941</td>\n",
              "      <td>0.132728</td>\n",
              "      <td>0.018151</td>\n",
              "      <td>0.233693</td>\n",
              "      <td>0.018151</td>\n",
              "      <td>0.239932</td>\n",
              "      <td>0.026092</td>\n",
              "      <td>0.234260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>0.015303</td>\n",
              "      <td>0.038846</td>\n",
              "      <td>0.012949</td>\n",
              "      <td>0.022366</td>\n",
              "      <td>0.024720</td>\n",
              "      <td>0.045321</td>\n",
              "      <td>0.007652</td>\n",
              "      <td>0.015303</td>\n",
              "      <td>0.005886</td>\n",
              "      <td>0.018835</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006474</td>\n",
              "      <td>0.201295</td>\n",
              "      <td>0.014715</td>\n",
              "      <td>0.128311</td>\n",
              "      <td>0.011183</td>\n",
              "      <td>0.210712</td>\n",
              "      <td>0.018835</td>\n",
              "      <td>0.216009</td>\n",
              "      <td>0.019423</td>\n",
              "      <td>0.244850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>0.015569</td>\n",
              "      <td>0.041916</td>\n",
              "      <td>0.006587</td>\n",
              "      <td>0.018563</td>\n",
              "      <td>0.031138</td>\n",
              "      <td>0.043713</td>\n",
              "      <td>0.009581</td>\n",
              "      <td>0.019760</td>\n",
              "      <td>0.006587</td>\n",
              "      <td>0.030539</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013772</td>\n",
              "      <td>0.201198</td>\n",
              "      <td>0.007186</td>\n",
              "      <td>0.113772</td>\n",
              "      <td>0.020958</td>\n",
              "      <td>0.233533</td>\n",
              "      <td>0.017365</td>\n",
              "      <td>0.216168</td>\n",
              "      <td>0.027545</td>\n",
              "      <td>0.259281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>0.015792</td>\n",
              "      <td>0.036169</td>\n",
              "      <td>0.008151</td>\n",
              "      <td>0.019358</td>\n",
              "      <td>0.030056</td>\n",
              "      <td>0.048905</td>\n",
              "      <td>0.007641</td>\n",
              "      <td>0.019358</td>\n",
              "      <td>0.009679</td>\n",
              "      <td>0.024452</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006623</td>\n",
              "      <td>0.177789</td>\n",
              "      <td>0.009679</td>\n",
              "      <td>0.140092</td>\n",
              "      <td>0.012736</td>\n",
              "      <td>0.235354</td>\n",
              "      <td>0.017830</td>\n",
              "      <td>0.226184</td>\n",
              "      <td>0.017830</td>\n",
              "      <td>0.239939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>0.017332</td>\n",
              "      <td>0.038866</td>\n",
              "      <td>0.009979</td>\n",
              "      <td>0.019958</td>\n",
              "      <td>0.036239</td>\n",
              "      <td>0.047794</td>\n",
              "      <td>0.006828</td>\n",
              "      <td>0.019433</td>\n",
              "      <td>0.007878</td>\n",
              "      <td>0.029937</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009454</td>\n",
              "      <td>0.176996</td>\n",
              "      <td>0.008929</td>\n",
              "      <td>0.122374</td>\n",
              "      <td>0.012605</td>\n",
              "      <td>0.213235</td>\n",
              "      <td>0.017332</td>\n",
              "      <td>0.200630</td>\n",
              "      <td>0.019958</td>\n",
              "      <td>0.237920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>0.013691</td>\n",
              "      <td>0.036509</td>\n",
              "      <td>0.010268</td>\n",
              "      <td>0.023388</td>\n",
              "      <td>0.035368</td>\n",
              "      <td>0.055334</td>\n",
              "      <td>0.006275</td>\n",
              "      <td>0.011979</td>\n",
              "      <td>0.007986</td>\n",
              "      <td>0.027952</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007416</td>\n",
              "      <td>0.198517</td>\n",
              "      <td>0.013120</td>\n",
              "      <td>0.139760</td>\n",
              "      <td>0.012550</td>\n",
              "      <td>0.210496</td>\n",
              "      <td>0.019395</td>\n",
              "      <td>0.206503</td>\n",
              "      <td>0.024529</td>\n",
              "      <td>0.250428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>0.014118</td>\n",
              "      <td>0.031176</td>\n",
              "      <td>0.007647</td>\n",
              "      <td>0.022353</td>\n",
              "      <td>0.028235</td>\n",
              "      <td>0.044118</td>\n",
              "      <td>0.004118</td>\n",
              "      <td>0.010588</td>\n",
              "      <td>0.007647</td>\n",
              "      <td>0.028824</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.172941</td>\n",
              "      <td>0.012941</td>\n",
              "      <td>0.127059</td>\n",
              "      <td>0.014706</td>\n",
              "      <td>0.202941</td>\n",
              "      <td>0.019412</td>\n",
              "      <td>0.218824</td>\n",
              "      <td>0.022941</td>\n",
              "      <td>0.226471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>0.009423</td>\n",
              "      <td>0.034158</td>\n",
              "      <td>0.006478</td>\n",
              "      <td>0.013545</td>\n",
              "      <td>0.032391</td>\n",
              "      <td>0.051826</td>\n",
              "      <td>0.008245</td>\n",
              "      <td>0.015901</td>\n",
              "      <td>0.005300</td>\n",
              "      <td>0.021201</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014723</td>\n",
              "      <td>0.162544</td>\n",
              "      <td>0.012956</td>\n",
              "      <td>0.140754</td>\n",
              "      <td>0.013545</td>\n",
              "      <td>0.203769</td>\n",
              "      <td>0.024735</td>\n",
              "      <td>0.226148</td>\n",
              "      <td>0.022968</td>\n",
              "      <td>0.222026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>0.014346</td>\n",
              "      <td>0.035069</td>\n",
              "      <td>0.008502</td>\n",
              "      <td>0.018066</td>\n",
              "      <td>0.033475</td>\n",
              "      <td>0.048353</td>\n",
              "      <td>0.006908</td>\n",
              "      <td>0.021254</td>\n",
              "      <td>0.007439</td>\n",
              "      <td>0.025505</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014346</td>\n",
              "      <td>0.187566</td>\n",
              "      <td>0.008502</td>\n",
              "      <td>0.154623</td>\n",
              "      <td>0.010627</td>\n",
              "      <td>0.208289</td>\n",
              "      <td>0.018066</td>\n",
              "      <td>0.227418</td>\n",
              "      <td>0.026567</td>\n",
              "      <td>0.251328</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>291 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     machine learning_(title)  machine learning_(abstracts)  \\\n",
              "0                    0.001976                      0.001976   \n",
              "1                    0.001916                      0.001916   \n",
              "2                    0.001757                      0.001757   \n",
              "3                    0.001848                      0.001848   \n",
              "4                    0.001698                      0.001698   \n",
              "5                    0.001842                      0.001842   \n",
              "6                    0.001761                      0.001761   \n",
              "7                    0.001905                      0.001905   \n",
              "8                    0.001869                      0.001869   \n",
              "9                    0.001815                      0.001815   \n",
              "10                   0.001739                      0.001739   \n",
              "11                   0.001761                      0.001761   \n",
              "12                   0.001908                      0.001908   \n",
              "13                   0.001754                      0.001754   \n",
              "14                   0.001425                      0.001425   \n",
              "15                   0.001610                      0.001610   \n",
              "16                   0.001453                      0.001453   \n",
              "17                   0.001399                      0.001399   \n",
              "18                   0.001506                      0.001506   \n",
              "19                   0.001653                      0.001653   \n",
              "20                   0.001642                      0.001642   \n",
              "21                   0.001416                      0.001416   \n",
              "22                   0.001520                      0.001520   \n",
              "23                   0.001776                      0.001776   \n",
              "24                   0.001536                      0.001536   \n",
              "25                   0.001686                      0.001686   \n",
              "26                   0.001416                      0.001416   \n",
              "27                   0.001486                      0.001486   \n",
              "28                   0.001399                      0.001399   \n",
              "29                   0.001546                      0.001546   \n",
              "30                   0.001267                      0.001267   \n",
              "31                   0.001517                      0.001517   \n",
              "32                   0.001346                      0.001346   \n",
              "33                   0.001307                      0.001307   \n",
              "34                   0.001330                      0.001330   \n",
              "35                   0.001422                      0.001422   \n",
              "36                   0.001460                      0.002920   \n",
              "37                   0.001488                      0.001488   \n",
              "38                   0.001381                      0.001381   \n",
              "39                   0.001333                      0.001333   \n",
              "40                   0.001290                      0.001290   \n",
              "41                   0.001250                      0.002500   \n",
              "42                   0.001205                      0.001205   \n",
              "43                   0.001429                      0.001429   \n",
              "44                   0.001255                      0.001255   \n",
              "45                   0.001181                      0.001181   \n",
              "46                   0.001395                      0.001395   \n",
              "47                   0.001242                      0.001242   \n",
              "48                   0.001374                      0.001374   \n",
              "49                   0.001323                      0.002646   \n",
              "50                   0.001206                      0.001206   \n",
              "51                   0.001221                      0.001221   \n",
              "52                   0.001263                      0.001263   \n",
              "53                   0.001167                      0.001167   \n",
              "54                   0.001109                      0.001109   \n",
              "55                   0.001328                      0.001328   \n",
              "56                   0.001190                      0.001190   \n",
              "57                   0.001163                      0.001163   \n",
              "58                   0.001198                      0.001198   \n",
              "59                   0.001160                      0.001160   \n",
              "60                   0.001214                      0.001214   \n",
              "61                   0.001361                      0.001361   \n",
              "62                   0.001152                      0.001152   \n",
              "63                   0.001104                      0.002208   \n",
              "64                   0.001178                      0.001178   \n",
              "65                   0.001110                      0.001110   \n",
              "66                   0.001181                      0.002361   \n",
              "67                   0.001195                      0.001195   \n",
              "68                   0.001122                      0.001122   \n",
              "69                   0.001078                      0.001078   \n",
              "70                   0.001183                      0.001183   \n",
              "71                   0.001203                      0.001203   \n",
              "72                   0.001236                      0.001236   \n",
              "73                   0.001299                      0.001299   \n",
              "74                   0.001057                      0.001057   \n",
              "75                   0.001325                      0.001325   \n",
              "76                   0.001195                      0.001195   \n",
              "77                   0.001125                      0.001125   \n",
              "78                   0.001096                      0.001096   \n",
              "79                   0.001227                      0.001227   \n",
              "80                   0.001110                      0.001110   \n",
              "81                   0.001049                      0.001049   \n",
              "82                   0.001131                      0.001131   \n",
              "83                   0.001289                      0.001289   \n",
              "84                   0.001182                      0.001182   \n",
              "85                   0.001290                      0.001290   \n",
              "86                   0.001072                      0.001072   \n",
              "87                   0.001259                      0.001259   \n",
              "88                   0.001120                      0.001120   \n",
              "89                   0.001147                      0.001147   \n",
              "90                   0.001087                      0.001087   \n",
              "91                   0.001157                      0.003472   \n",
              "92                   0.001112                      0.001112   \n",
              "93                   0.001045                      0.001045   \n",
              "94                   0.001152                      0.001152   \n",
              "95                   0.001248                      0.002497   \n",
              "96                   0.001157                      0.001157   \n",
              "97                   0.001253                      0.001253   \n",
              "98                   0.001066                      0.001066   \n",
              "99                   0.001044                      0.001044   \n",
              "100                  0.001116                      0.001116   \n",
              "101                  0.000986                      0.001972   \n",
              "102                  0.000914                      0.000914   \n",
              "103                  0.001081                      0.001081   \n",
              "104                  0.001073                      0.001073   \n",
              "105                  0.000955                      0.000955   \n",
              "106                  0.001042                      0.001042   \n",
              "107                  0.001091                      0.001091   \n",
              "108                  0.001186                      0.001186   \n",
              "109                  0.001103                      0.001103   \n",
              "110                  0.000989                      0.000989   \n",
              "111                  0.001094                      0.001094   \n",
              "112                  0.001095                      0.001095   \n",
              "113                  0.001073                      0.001073   \n",
              "114                  0.000971                      0.001942   \n",
              "115                  0.001163                      0.001163   \n",
              "116                  0.001012                      0.001012   \n",
              "117                  0.000986                      0.000986   \n",
              "118                  0.001030                      0.001030   \n",
              "119                  0.000973                      0.000973   \n",
              "120                  0.001101                      0.001101   \n",
              "121                  0.001071                      0.001071   \n",
              "122                  0.000845                      0.000845   \n",
              "123                  0.000928                      0.000928   \n",
              "124                  0.000932                      0.000932   \n",
              "125                  0.000895                      0.000895   \n",
              "126                  0.000898                      0.000898   \n",
              "127                  0.000904                      0.000904   \n",
              "128                  0.000874                      0.000874   \n",
              "129                  0.000842                      0.000842   \n",
              "130                  0.000894                      0.001787   \n",
              "131                  0.000894                      0.000894   \n",
              "132                  0.000962                      0.000962   \n",
              "133                  0.000935                      0.000935   \n",
              "134                  0.000823                      0.000823   \n",
              "135                  0.000918                      0.000918   \n",
              "136                  0.000829                      0.001657   \n",
              "137                  0.000818                      0.000818   \n",
              "138                  0.000883                      0.000883   \n",
              "139                  0.000860                      0.000860   \n",
              "140                  0.001692                      0.001692   \n",
              "141                  0.000856                      0.000856   \n",
              "142                  0.000773                      0.000773   \n",
              "143                  0.000845                      0.000845   \n",
              "144                  0.000890                      0.000890   \n",
              "145                  0.000848                      0.000848   \n",
              "146                  0.000814                      0.000814   \n",
              "147                  0.000812                      0.000812   \n",
              "148                  0.000826                      0.000826   \n",
              "149                  0.000773                      0.000773   \n",
              "150                  0.000756                      0.001512   \n",
              "151                  0.001685                      0.000842   \n",
              "152                  0.000873                      0.000873   \n",
              "153                  0.000740                      0.001479   \n",
              "154                  0.000783                      0.000783   \n",
              "155                  0.000838                      0.001676   \n",
              "156                  0.000797                      0.000797   \n",
              "157                  0.001577                      0.001577   \n",
              "158                  0.000758                      0.001515   \n",
              "159                  0.000774                      0.000774   \n",
              "160                  0.000789                      0.000789   \n",
              "161                  0.001631                      0.002447   \n",
              "162                  0.001404                      0.002107   \n",
              "163                  0.000841                      0.000841   \n",
              "164                  0.000741                      0.000741   \n",
              "165                  0.001467                      0.002201   \n",
              "166                  0.000820                      0.000820   \n",
              "167                  0.000806                      0.000806   \n",
              "168                  0.001674                      0.002510   \n",
              "169                  0.000814                      0.001627   \n",
              "170                  0.000713                      0.000713   \n",
              "171                  0.000838                      0.002515   \n",
              "172                  0.001627                      0.001627   \n",
              "173                  0.001488                      0.001488   \n",
              "174                  0.000673                      0.000673   \n",
              "175                  0.001621                      0.002431   \n",
              "176                  0.002185                      0.002913   \n",
              "177                  0.002094                      0.003489   \n",
              "178                  0.000801                      0.001603   \n",
              "179                  0.001423                      0.000712   \n",
              "180                  0.000808                      0.002423   \n",
              "181                  0.000763                      0.002290   \n",
              "182                  0.001342                      0.002013   \n",
              "183                  0.000758                      0.000758   \n",
              "184                  0.000722                      0.002166   \n",
              "185                  0.001357                      0.002714   \n",
              "186                  0.000672                      0.000672   \n",
              "187                  0.001500                      0.001500   \n",
              "188                  0.001395                      0.002789   \n",
              "189                  0.000693                      0.001386   \n",
              "190                  0.000714                      0.001428   \n",
              "191                  0.002764                      0.003455   \n",
              "192                  0.000717                      0.002151   \n",
              "193                  0.001432                      0.001432   \n",
              "194                  0.001299                      0.001948   \n",
              "195                  0.001441                      0.001441   \n",
              "196                  0.001331                      0.001331   \n",
              "197                  0.001368                      0.005472   \n",
              "198                  0.001388                      0.002082   \n",
              "199                  0.002122                      0.003536   \n",
              "200                  0.004578                      0.004578   \n",
              "201                  0.001944                      0.004537   \n",
              "202                  0.003966                      0.007270   \n",
              "203                  0.000674                      0.004046   \n",
              "204                  0.001526                      0.004577   \n",
              "205                  0.000762                      0.003808   \n",
              "206                  0.002446                      0.006116   \n",
              "207                  0.003019                      0.005283   \n",
              "208                  0.003819                      0.007002   \n",
              "209                  0.004394                      0.007533   \n",
              "210                  0.005098                      0.003642   \n",
              "211                  0.005682                      0.008523   \n",
              "212                  0.008421                      0.012632   \n",
              "213                  0.007412                      0.012353   \n",
              "214                  0.001319                      0.005937   \n",
              "215                  0.002007                      0.005351   \n",
              "216                  0.003516                      0.006329   \n",
              "217                  0.005204                      0.009665   \n",
              "218                  0.009479                      0.016588   \n",
              "219                  0.006766                      0.010149   \n",
              "220                  0.005044                      0.006936   \n",
              "221                  0.004436                      0.009506   \n",
              "222                  0.006489                      0.009085   \n",
              "223                  0.006901                      0.011042   \n",
              "224                  0.006711                      0.014094   \n",
              "225                  0.008751                      0.014586   \n",
              "226                  0.007555                      0.015797   \n",
              "227                  0.003303                      0.010568   \n",
              "228                  0.013228                      0.016534   \n",
              "229                  0.006077                      0.013504   \n",
              "230                  0.011816                      0.013682   \n",
              "231                  0.006297                      0.015743   \n",
              "232                  0.006615                      0.012628   \n",
              "233                  0.008135                      0.016270   \n",
              "234                  0.007177                      0.012560   \n",
              "235                  0.009759                      0.013663   \n",
              "236                  0.006567                      0.013134   \n",
              "237                  0.007058                      0.016287   \n",
              "238                  0.006897                      0.011285   \n",
              "239                  0.006271                      0.015964   \n",
              "240                  0.011765                      0.018954   \n",
              "241                  0.009862                      0.017751   \n",
              "242                  0.010613                      0.021816   \n",
              "243                  0.012493                      0.025554   \n",
              "244                  0.011871                      0.025438   \n",
              "245                  0.013468                      0.024691   \n",
              "246                  0.009855                      0.019191   \n",
              "247                  0.011786                      0.022392   \n",
              "248                  0.007986                      0.014832   \n",
              "249                  0.010863                      0.027444   \n",
              "250                  0.009050                      0.022059   \n",
              "251                  0.007837                      0.021944   \n",
              "252                  0.013601                      0.031088   \n",
              "253                  0.011553                      0.023107   \n",
              "254                  0.007067                      0.019687   \n",
              "255                  0.008323                      0.016052   \n",
              "256                  0.009884                      0.019767   \n",
              "257                  0.010435                      0.028406   \n",
              "258                  0.013978                      0.026209   \n",
              "259                  0.010821                      0.027371   \n",
              "260                  0.009858                      0.019717   \n",
              "261                  0.014140                      0.023190   \n",
              "262                  0.016298                      0.031432   \n",
              "263                  0.012379                      0.020990   \n",
              "264                  0.016960                      0.030779   \n",
              "265                  0.017210                      0.035034   \n",
              "266                  0.013492                      0.029142   \n",
              "267                  0.007208                      0.022936   \n",
              "268                  0.012173                      0.034693   \n",
              "269                  0.013530                      0.026445   \n",
              "270                  0.012971                      0.028413   \n",
              "271                  0.018856                      0.035827   \n",
              "272                  0.010284                      0.029643   \n",
              "273                  0.014043                      0.033353   \n",
              "274                  0.016696                      0.031604   \n",
              "275                  0.015528                      0.033540   \n",
              "276                  0.012953                      0.029793   \n",
              "277                  0.009536                      0.024793   \n",
              "278                  0.016824                      0.025762   \n",
              "279                  0.018205                      0.033160   \n",
              "280                  0.014239                      0.032859   \n",
              "281                  0.018878                      0.037757   \n",
              "282                  0.014180                      0.030630   \n",
              "283                  0.015303                      0.038846   \n",
              "284                  0.015569                      0.041916   \n",
              "285                  0.015792                      0.036169   \n",
              "286                  0.017332                      0.038866   \n",
              "287                  0.013691                      0.036509   \n",
              "288                  0.014118                      0.031176   \n",
              "289                  0.009423                      0.034158   \n",
              "290                  0.014346                      0.035069   \n",
              "\n",
              "     topological insulator_(title)  topological insulator_(abstracts)  \\\n",
              "0                         0.001976                           0.001976   \n",
              "1                         0.001916                           0.001916   \n",
              "2                         0.001757                           0.001757   \n",
              "3                         0.001848                           0.001848   \n",
              "4                         0.001698                           0.001698   \n",
              "5                         0.001842                           0.001842   \n",
              "6                         0.001761                           0.001761   \n",
              "7                         0.001905                           0.001905   \n",
              "8                         0.001869                           0.003738   \n",
              "9                         0.001815                           0.001815   \n",
              "10                        0.001739                           0.001739   \n",
              "11                        0.001761                           0.001761   \n",
              "12                        0.001908                           0.001908   \n",
              "13                        0.001754                           0.001754   \n",
              "14                        0.001425                           0.001425   \n",
              "15                        0.001610                           0.001610   \n",
              "16                        0.001453                           0.001453   \n",
              "17                        0.001399                           0.001399   \n",
              "18                        0.001506                           0.001506   \n",
              "19                        0.001653                           0.001653   \n",
              "20                        0.001642                           0.001642   \n",
              "21                        0.001416                           0.001416   \n",
              "22                        0.001520                           0.001520   \n",
              "23                        0.001776                           0.001776   \n",
              "24                        0.001536                           0.001536   \n",
              "25                        0.001686                           0.001686   \n",
              "26                        0.001416                           0.001416   \n",
              "27                        0.001486                           0.001486   \n",
              "28                        0.001399                           0.001399   \n",
              "29                        0.001546                           0.001546   \n",
              "30                        0.001267                           0.001267   \n",
              "31                        0.001517                           0.001517   \n",
              "32                        0.001346                           0.001346   \n",
              "33                        0.001307                           0.001307   \n",
              "34                        0.001330                           0.001330   \n",
              "35                        0.001422                           0.001422   \n",
              "36                        0.001460                           0.001460   \n",
              "37                        0.001488                           0.001488   \n",
              "38                        0.001381                           0.001381   \n",
              "39                        0.001333                           0.001333   \n",
              "40                        0.001290                           0.001290   \n",
              "41                        0.001250                           0.001250   \n",
              "42                        0.001205                           0.001205   \n",
              "43                        0.001429                           0.001429   \n",
              "44                        0.001255                           0.001255   \n",
              "45                        0.001181                           0.001181   \n",
              "46                        0.001395                           0.001395   \n",
              "47                        0.001242                           0.001242   \n",
              "48                        0.001374                           0.001374   \n",
              "49                        0.001323                           0.001323   \n",
              "50                        0.001206                           0.001206   \n",
              "51                        0.001221                           0.001221   \n",
              "52                        0.001263                           0.001263   \n",
              "53                        0.001167                           0.001167   \n",
              "54                        0.001109                           0.001109   \n",
              "55                        0.001328                           0.001328   \n",
              "56                        0.001190                           0.001190   \n",
              "57                        0.001163                           0.001163   \n",
              "58                        0.001198                           0.001198   \n",
              "59                        0.002320                           0.002320   \n",
              "60                        0.001214                           0.001214   \n",
              "61                        0.001361                           0.001361   \n",
              "62                        0.001152                           0.001152   \n",
              "63                        0.001104                           0.001104   \n",
              "64                        0.001178                           0.002356   \n",
              "65                        0.001110                           0.001110   \n",
              "66                        0.001181                           0.001181   \n",
              "67                        0.001195                           0.001195   \n",
              "68                        0.001122                           0.001122   \n",
              "69                        0.001078                           0.001078   \n",
              "70                        0.001183                           0.001183   \n",
              "71                        0.001203                           0.001203   \n",
              "72                        0.001236                           0.001236   \n",
              "73                        0.001299                           0.001299   \n",
              "74                        0.001057                           0.001057   \n",
              "75                        0.001325                           0.001325   \n",
              "76                        0.001195                           0.001195   \n",
              "77                        0.001125                           0.001125   \n",
              "78                        0.002193                           0.004386   \n",
              "79                        0.001227                           0.001227   \n",
              "80                        0.001110                           0.001110   \n",
              "81                        0.001049                           0.001049   \n",
              "82                        0.002262                           0.002262   \n",
              "83                        0.001289                           0.001289   \n",
              "84                        0.001182                           0.001182   \n",
              "85                        0.001290                           0.002581   \n",
              "86                        0.001072                           0.001072   \n",
              "87                        0.001259                           0.001259   \n",
              "88                        0.003359                           0.003359   \n",
              "89                        0.001147                           0.001147   \n",
              "90                        0.002174                           0.002174   \n",
              "91                        0.001157                           0.001157   \n",
              "92                        0.001112                           0.001112   \n",
              "93                        0.002090                           0.002090   \n",
              "94                        0.001152                           0.001152   \n",
              "95                        0.001248                           0.001248   \n",
              "96                        0.001157                           0.002315   \n",
              "97                        0.001253                           0.002506   \n",
              "98                        0.002132                           0.003198   \n",
              "99                        0.002088                           0.003132   \n",
              "100                       0.001116                           0.002232   \n",
              "101                       0.000986                           0.000986   \n",
              "102                       0.000914                           0.001828   \n",
              "103                       0.002162                           0.002162   \n",
              "104                       0.001073                           0.002146   \n",
              "105                       0.000955                           0.003820   \n",
              "106                       0.002083                           0.003125   \n",
              "107                       0.002181                           0.004362   \n",
              "108                       0.004745                           0.002372   \n",
              "109                       0.004410                           0.005513   \n",
              "110                       0.002967                           0.003956   \n",
              "111                       0.006565                           0.005470   \n",
              "112                       0.005476                           0.004381   \n",
              "113                       0.004292                           0.004292   \n",
              "114                       0.005825                           0.008738   \n",
              "115                       0.012791                           0.018605   \n",
              "116                       0.011134                           0.015182   \n",
              "117                       0.009862                           0.010848   \n",
              "118                       0.008239                           0.009269   \n",
              "119                       0.011673                           0.015564   \n",
              "120                       0.011013                           0.017621   \n",
              "121                       0.013919                           0.018201   \n",
              "122                       0.014370                           0.022823   \n",
              "123                       0.011132                           0.017625   \n",
              "124                       0.014911                           0.016775   \n",
              "125                       0.012534                           0.015219   \n",
              "126                       0.017969                           0.020665   \n",
              "127                       0.009946                           0.011754   \n",
              "128                       0.007867                           0.012238   \n",
              "129                       0.014322                           0.019377   \n",
              "130                       0.013405                           0.019660   \n",
              "131                       0.018767                           0.023235   \n",
              "132                       0.018287                           0.025987   \n",
              "133                       0.008419                           0.014032   \n",
              "134                       0.017284                           0.023868   \n",
              "135                       0.019284                           0.024793   \n",
              "136                       0.022370                           0.033140   \n",
              "137                       0.009820                           0.016367   \n",
              "138                       0.015018                           0.022085   \n",
              "139                       0.024076                           0.037833   \n",
              "140                       0.019459                           0.026227   \n",
              "141                       0.017123                           0.027397   \n",
              "142                       0.017015                           0.023975   \n",
              "143                       0.021133                           0.025359   \n",
              "144                       0.016904                           0.028470   \n",
              "145                       0.019508                           0.027142   \n",
              "146                       0.015472                           0.028502   \n",
              "147                       0.015422                           0.022727   \n",
              "148                       0.016515                           0.025599   \n",
              "149                       0.023957                           0.036321   \n",
              "150                       0.009070                           0.022676   \n",
              "151                       0.026116                           0.033698   \n",
              "152                       0.016579                           0.027923   \n",
              "153                       0.017012                           0.020710   \n",
              "154                       0.018794                           0.025842   \n",
              "155                       0.017603                           0.033529   \n",
              "156                       0.013546                           0.026295   \n",
              "157                       0.018927                           0.028391   \n",
              "158                       0.020455                           0.029545   \n",
              "159                       0.015480                           0.020898   \n",
              "160                       0.015773                           0.030757   \n",
              "161                       0.017945                           0.030995   \n",
              "162                       0.014747                           0.028090   \n",
              "163                       0.017662                           0.033642   \n",
              "164                       0.016308                           0.022980   \n",
              "165                       0.016141                           0.026412   \n",
              "166                       0.014766                           0.022970   \n",
              "167                       0.012893                           0.025786   \n",
              "168                       0.018410                           0.028452   \n",
              "169                       0.016273                           0.026037   \n",
              "170                       0.014979                           0.032097   \n",
              "171                       0.014250                           0.027661   \n",
              "172                       0.013019                           0.030106   \n",
              "173                       0.012649                           0.029018   \n",
              "174                       0.016162                           0.030303   \n",
              "175                       0.015397                           0.034036   \n",
              "176                       0.021122                           0.029862   \n",
              "177                       0.015352                           0.030007   \n",
              "178                       0.011218                           0.016827   \n",
              "179                       0.019217                           0.034875   \n",
              "180                       0.011309                           0.024233   \n",
              "181                       0.012214                           0.034351   \n",
              "182                       0.016107                           0.040268   \n",
              "183                       0.015909                           0.035606   \n",
              "184                       0.020939                           0.038989   \n",
              "185                       0.018996                           0.036635   \n",
              "186                       0.010760                           0.026227   \n",
              "187                       0.014254                           0.021005   \n",
              "188                       0.017434                           0.028591   \n",
              "189                       0.016632                           0.030492   \n",
              "190                       0.021413                           0.034261   \n",
              "191                       0.015204                           0.031099   \n",
              "192                       0.020789                           0.040143   \n",
              "193                       0.015032                           0.030064   \n",
              "194                       0.016234                           0.037662   \n",
              "195                       0.015130                           0.036744   \n",
              "196                       0.015968                           0.029275   \n",
              "197                       0.012996                           0.028728   \n",
              "198                       0.019431                           0.034004   \n",
              "199                       0.013437                           0.028996   \n",
              "200                       0.016351                           0.027469   \n",
              "201                       0.007129                           0.022683   \n",
              "202                       0.011897                           0.025116   \n",
              "203                       0.017532                           0.030344   \n",
              "204                       0.013730                           0.025172   \n",
              "205                       0.012947                           0.027418   \n",
              "206                       0.015902                           0.028746   \n",
              "207                       0.010566                           0.021132   \n",
              "208                       0.008275                           0.021642   \n",
              "209                       0.012555                           0.027621   \n",
              "210                       0.013110                           0.026220   \n",
              "211                       0.018466                           0.028409   \n",
              "212                       0.011228                           0.023158   \n",
              "213                       0.008030                           0.028413   \n",
              "214                       0.012533                           0.027704   \n",
              "215                       0.012709                           0.022074   \n",
              "216                       0.014065                           0.030239   \n",
              "217                       0.008922                           0.029740   \n",
              "218                       0.007109                           0.020142   \n",
              "219                       0.016915                           0.027740   \n",
              "220                       0.015132                           0.028373   \n",
              "221                       0.012674                           0.036122   \n",
              "222                       0.011681                           0.029202   \n",
              "223                       0.019324                           0.033126   \n",
              "224                       0.018121                           0.036913   \n",
              "225                       0.012252                           0.030338   \n",
              "226                       0.014423                           0.025412   \n",
              "227                       0.013210                           0.032365   \n",
              "228                       0.010582                           0.027116   \n",
              "229                       0.008103                           0.018231   \n",
              "230                       0.009328                           0.027363   \n",
              "231                       0.014484                           0.023300   \n",
              "232                       0.013830                           0.037282   \n",
              "233                       0.012516                           0.024406   \n",
              "234                       0.014354                           0.035885   \n",
              "235                       0.013012                           0.022772   \n",
              "236                       0.008955                           0.022687   \n",
              "237                       0.013572                           0.028230   \n",
              "238                       0.021317                           0.032602   \n",
              "239                       0.010832                           0.027366   \n",
              "240                       0.016993                           0.035948   \n",
              "241                       0.008547                           0.019066   \n",
              "242                       0.012972                           0.025943   \n",
              "243                       0.019875                           0.034639   \n",
              "244                       0.010741                           0.026003   \n",
              "245                       0.008418                           0.025253   \n",
              "246                       0.012967                           0.031120   \n",
              "247                       0.015321                           0.027696   \n",
              "248                       0.013120                           0.022248   \n",
              "249                       0.014866                           0.028588   \n",
              "250                       0.007919                           0.016403   \n",
              "251                       0.013062                           0.026123   \n",
              "252                       0.013601                           0.028497   \n",
              "253                       0.007060                           0.021181   \n",
              "254                       0.008582                           0.020697   \n",
              "255                       0.013080                           0.022592   \n",
              "256                       0.011047                           0.021512   \n",
              "257                       0.011014                           0.026087   \n",
              "258                       0.011066                           0.029703   \n",
              "259                       0.010821                           0.028644   \n",
              "260                       0.007394                           0.027110   \n",
              "261                       0.009615                           0.028281   \n",
              "262                       0.008731                           0.026193   \n",
              "263                       0.011841                           0.027449   \n",
              "264                       0.006281                           0.018216   \n",
              "265                       0.010449                           0.024585   \n",
              "266                       0.008635                           0.023206   \n",
              "267                       0.007208                           0.024246   \n",
              "268                       0.013999                           0.028606   \n",
              "269                       0.011070                           0.019680   \n",
              "270                       0.011736                           0.025324   \n",
              "271                       0.008799                           0.027027   \n",
              "272                       0.009679                           0.018149   \n",
              "273                       0.008192                           0.019895   \n",
              "274                       0.005963                           0.020274   \n",
              "275                       0.014907                           0.026708   \n",
              "276                       0.012306                           0.023316   \n",
              "277                       0.010172                           0.019708   \n",
              "278                       0.006835                           0.022608   \n",
              "279                       0.011053                           0.026658   \n",
              "280                       0.007667                           0.020263   \n",
              "281                       0.005552                           0.014992   \n",
              "282                       0.009075                           0.019853   \n",
              "283                       0.012949                           0.022366   \n",
              "284                       0.006587                           0.018563   \n",
              "285                       0.008151                           0.019358   \n",
              "286                       0.009979                           0.019958   \n",
              "287                       0.010268                           0.023388   \n",
              "288                       0.007647                           0.022353   \n",
              "289                       0.006478                           0.013545   \n",
              "290                       0.008502                           0.018066   \n",
              "\n",
              "     graphene_(title)  graphene_(abstracts)  majorana_(title)  \\\n",
              "0            0.001976              0.003953          0.001976   \n",
              "1            0.001916              0.003831          0.001916   \n",
              "2            0.001757              0.001757          0.001757   \n",
              "3            0.001848              0.001848          0.001848   \n",
              "4            0.001698              0.001698          0.001698   \n",
              "5            0.001842              0.001842          0.001842   \n",
              "6            0.003521              0.003521          0.001761   \n",
              "7            0.001905              0.001905          0.001905   \n",
              "8            0.001869              0.001869          0.001869   \n",
              "9            0.001815              0.007260          0.003630   \n",
              "10           0.001739              0.005217          0.001739   \n",
              "11           0.001761              0.003521          0.001761   \n",
              "12           0.001908              0.003817          0.001908   \n",
              "13           0.001754              0.001754          0.001754   \n",
              "14           0.001425              0.002849          0.001425   \n",
              "15           0.001610              0.001610          0.001610   \n",
              "16           0.001453              0.001453          0.001453   \n",
              "17           0.001399              0.001399          0.001399   \n",
              "18           0.003012              0.003012          0.001506   \n",
              "19           0.001653              0.004959          0.003306   \n",
              "20           0.001642              0.001642          0.001642   \n",
              "21           0.001416              0.002833          0.001416   \n",
              "22           0.001520              0.003040          0.001520   \n",
              "23           0.001776              0.001776          0.001776   \n",
              "24           0.001536              0.001536          0.001536   \n",
              "25           0.001686              0.001686          0.001686   \n",
              "26           0.001416              0.007082          0.001416   \n",
              "27           0.001486              0.001486          0.001486   \n",
              "28           0.001399              0.002797          0.001399   \n",
              "29           0.001546              0.001546          0.001546   \n",
              "30           0.001267              0.001267          0.001267   \n",
              "31           0.001517              0.004552          0.001517   \n",
              "32           0.001346              0.001346          0.001346   \n",
              "33           0.001307              0.001307          0.001307   \n",
              "34           0.001330              0.003989          0.001330   \n",
              "35           0.001422              0.002845          0.001422   \n",
              "36           0.001460              0.001460          0.001460   \n",
              "37           0.001488              0.001488          0.001488   \n",
              "38           0.001381              0.001381          0.001381   \n",
              "39           0.001333              0.001333          0.001333   \n",
              "40           0.001290              0.002581          0.003871   \n",
              "41           0.001250              0.001250          0.001250   \n",
              "42           0.001205              0.001205          0.001205   \n",
              "43           0.004286              0.005714          0.001429   \n",
              "44           0.002509              0.003764          0.001255   \n",
              "45           0.001181              0.002361          0.001181   \n",
              "46           0.002789              0.004184          0.001395   \n",
              "47           0.001242              0.002484          0.001242   \n",
              "48           0.001374              0.001374          0.001374   \n",
              "49           0.001323              0.002646          0.001323   \n",
              "50           0.002413              0.002413          0.001206   \n",
              "51           0.001221              0.002442          0.001221   \n",
              "52           0.001263              0.001263          0.001263   \n",
              "53           0.001167              0.001167          0.001167   \n",
              "54           0.001109              0.001109          0.001109   \n",
              "55           0.001328              0.002656          0.001328   \n",
              "56           0.001190              0.002381          0.001190   \n",
              "57           0.004651              0.005814          0.001163   \n",
              "58           0.002395              0.005988          0.001198   \n",
              "59           0.001160              0.002320          0.001160   \n",
              "60           0.001214              0.001214          0.001214   \n",
              "61           0.001361              0.002721          0.001361   \n",
              "62           0.001152              0.002304          0.001152   \n",
              "63           0.001104              0.002208          0.001104   \n",
              "64           0.002356              0.004711          0.001178   \n",
              "65           0.002220              0.004440          0.002220   \n",
              "66           0.002361              0.003542          0.001181   \n",
              "67           0.003584              0.001195          0.001195   \n",
              "68           0.004489              0.005612          0.001122   \n",
              "69           0.001078              0.002155          0.001078   \n",
              "70           0.001183              0.002367          0.001183   \n",
              "71           0.006017              0.009627          0.001203   \n",
              "72           0.001236              0.003708          0.002472   \n",
              "73           0.011688              0.010390          0.002597   \n",
              "74           0.012685              0.014799          0.001057   \n",
              "75           0.019868              0.021192          0.001325   \n",
              "76           0.007168              0.008363          0.001195   \n",
              "77           0.022497              0.023622          0.002250   \n",
              "78           0.015351              0.016447          0.002193   \n",
              "79           0.013497              0.017178          0.002454   \n",
              "80           0.014428              0.017758          0.002220   \n",
              "81           0.018888              0.020986          0.001049   \n",
              "82           0.028281              0.031674          0.001131   \n",
              "83           0.029639              0.029639          0.002577   \n",
              "84           0.026005              0.027187          0.002364   \n",
              "85           0.033548              0.036129          0.002581   \n",
              "86           0.028939              0.030011          0.001072   \n",
              "87           0.040302              0.042821          0.001259   \n",
              "88           0.044793              0.050392          0.001120   \n",
              "89           0.042431              0.043578          0.001147   \n",
              "90           0.051087              0.051087          0.002174   \n",
              "91           0.033565              0.031250          0.001157   \n",
              "92           0.040044              0.043382          0.001112   \n",
              "93           0.037618              0.039707          0.001045   \n",
              "94           0.041475              0.048387          0.002304   \n",
              "95           0.046192              0.051186          0.001248   \n",
              "96           0.040509              0.042824          0.001157   \n",
              "97           0.050125              0.051378          0.001253   \n",
              "98           0.045842              0.050107          0.001066   \n",
              "99           0.038622              0.040710          0.001044   \n",
              "100          0.046875              0.050223          0.002232   \n",
              "101          0.055227              0.064103          0.001972   \n",
              "102          0.049360              0.053016          0.000914   \n",
              "103          0.059459              0.064865          0.002162   \n",
              "104          0.048283              0.050429          0.001073   \n",
              "105          0.051576              0.054441          0.000955   \n",
              "106          0.039583              0.045833          0.003125   \n",
              "107          0.053435              0.058888          0.002181   \n",
              "108          0.060498              0.068802          0.001186   \n",
              "109          0.040794              0.051819          0.001103   \n",
              "110          0.053412              0.057369          0.003956   \n",
              "111          0.056893              0.065646          0.002188   \n",
              "112          0.052574              0.061336          0.002191   \n",
              "113          0.072961              0.080472          0.002146   \n",
              "114          0.057282              0.067961          0.005825   \n",
              "115          0.065116              0.075581          0.002326   \n",
              "116          0.059717              0.062753          0.003036   \n",
              "117          0.071006              0.078895          0.001972   \n",
              "118          0.047374              0.052523          0.004119   \n",
              "119          0.063230              0.064202          0.003891   \n",
              "120          0.052863              0.057269          0.003304   \n",
              "121          0.063169              0.072805          0.003212   \n",
              "122          0.071851              0.083686          0.002536   \n",
              "123          0.051948              0.060297          0.009276   \n",
              "124          0.058714              0.068034          0.001864   \n",
              "125          0.060877              0.068935          0.003581   \n",
              "126          0.069182              0.077269          0.005391   \n",
              "127          0.074141              0.081374          0.003617   \n",
              "128          0.076049              0.084790          0.003497   \n",
              "129          0.063184              0.075821          0.004212   \n",
              "130          0.063450              0.075067          0.006256   \n",
              "131          0.063450              0.070599          0.009830   \n",
              "132          0.069297              0.087584          0.003850   \n",
              "133          0.074836              0.083255          0.007484   \n",
              "134          0.065844              0.071605          0.004938   \n",
              "135          0.083563              0.092746          0.004591   \n",
              "136          0.085336              0.096935          0.005800   \n",
              "137          0.050736              0.063830          0.009002   \n",
              "138          0.072438              0.080389          0.005300   \n",
              "139          0.072227              0.088564          0.007739   \n",
              "140          0.060914              0.072758          0.007614   \n",
              "141          0.057363              0.068493          0.006849   \n",
              "142          0.064192              0.078886          0.009281   \n",
              "143          0.065089              0.076923          0.009298   \n",
              "144          0.068505              0.075623          0.007117   \n",
              "145          0.076336              0.087362          0.009330   \n",
              "146          0.073290              0.078176          0.009772   \n",
              "147          0.073864              0.088474          0.012175   \n",
              "148          0.072667              0.081751          0.007432   \n",
              "149          0.062597              0.073416          0.014683   \n",
              "150          0.054422              0.067271          0.007559   \n",
              "151          0.058130              0.074979          0.008425   \n",
              "152          0.045375              0.056719          0.004363   \n",
              "153          0.062870              0.075444          0.008876   \n",
              "154          0.065779              0.079092          0.010180   \n",
              "155          0.053646              0.062867          0.008382   \n",
              "156          0.068526              0.082072          0.007171   \n",
              "157          0.063880              0.072555          0.012618   \n",
              "158          0.056061              0.073485          0.012121   \n",
              "159          0.071981              0.088235          0.008514   \n",
              "160          0.070189              0.078076          0.008675   \n",
              "161          0.044861              0.056281          0.008972   \n",
              "162          0.054775              0.070225          0.012640   \n",
              "163          0.066442              0.074853          0.010934   \n",
              "164          0.054855              0.070423          0.008154   \n",
              "165          0.053558              0.069699          0.008070   \n",
              "166          0.060705              0.072190          0.004922   \n",
              "167          0.047542              0.064464          0.008058   \n",
              "168          0.056904              0.070293          0.010879   \n",
              "169          0.048820              0.055330          0.005696   \n",
              "170          0.043509              0.066334          0.014265   \n",
              "171          0.065381              0.075440          0.008382   \n",
              "172          0.063466              0.073230          0.009764   \n",
              "173          0.063244              0.074405          0.005952   \n",
              "174          0.051852              0.070034          0.006734   \n",
              "175          0.048622              0.069692          0.007293   \n",
              "176          0.045157              0.068463          0.011653   \n",
              "177          0.048151              0.069784          0.006281   \n",
              "178          0.060897              0.076923          0.009615   \n",
              "179          0.056228              0.078292          0.009964   \n",
              "180          0.045234              0.063005          0.008885   \n",
              "181          0.050382              0.067939          0.003053   \n",
              "182          0.056376              0.077852          0.006711   \n",
              "183          0.052273              0.075000          0.011364   \n",
              "184          0.047653              0.067148          0.007220   \n",
              "185          0.053596              0.071913          0.011533   \n",
              "186          0.052455              0.065905          0.008742   \n",
              "187          0.060015              0.072768          0.006002   \n",
              "188          0.050907              0.071130          0.010460   \n",
              "189          0.048510              0.062370          0.006930   \n",
              "190          0.052106              0.071378          0.008565   \n",
              "191          0.046994              0.071182          0.005529   \n",
              "192          0.060932              0.078853          0.011470   \n",
              "193          0.049392              0.067287          0.012169   \n",
              "194          0.042857              0.059091          0.011688   \n",
              "195          0.038184              0.053314          0.006484   \n",
              "196          0.035928              0.046574          0.009315   \n",
              "197          0.038304              0.057456          0.008208   \n",
              "198          0.040944              0.059681          0.007634   \n",
              "199          0.044554              0.067893          0.007072   \n",
              "200          0.045128              0.068672          0.011118   \n",
              "201          0.048607              0.069345          0.003889   \n",
              "202          0.044944              0.060806          0.011236   \n",
              "203          0.043156              0.056642          0.008766   \n",
              "204          0.039664              0.055683          0.006865   \n",
              "205          0.061691              0.078446          0.008378   \n",
              "206          0.051376              0.072171          0.008563   \n",
              "207          0.040000              0.055094          0.006038   \n",
              "208          0.047104              0.061744          0.006365   \n",
              "209          0.033898              0.045825          0.007533   \n",
              "210          0.040787              0.064822          0.008012   \n",
              "211          0.036222              0.052557          0.009233   \n",
              "212          0.042105              0.053333          0.007018   \n",
              "213          0.043854              0.056825          0.014824   \n",
              "214          0.041557              0.056728          0.011214   \n",
              "215          0.044816              0.065552          0.011371   \n",
              "216          0.048523              0.071730          0.012658   \n",
              "217          0.040149              0.058736          0.007435   \n",
              "218          0.034953              0.056872          0.008886   \n",
              "219          0.053451              0.071042          0.007442   \n",
              "220          0.063682              0.086381          0.009458   \n",
              "221          0.044360              0.058935          0.010773   \n",
              "222          0.033095              0.051265          0.010383   \n",
              "223          0.044168              0.072464          0.010352   \n",
              "224          0.047651              0.061074          0.012752   \n",
              "225          0.039090              0.064177          0.005834   \n",
              "226          0.037088              0.061126          0.013049   \n",
              "227          0.042933              0.063408          0.011889   \n",
              "228          0.050926              0.066799          0.009259   \n",
              "229          0.029710              0.045240          0.010804   \n",
              "230          0.052239              0.070274          0.006219   \n",
              "231          0.030227              0.054156          0.011965   \n",
              "232          0.049308              0.067949          0.010222   \n",
              "233          0.035670              0.051940          0.005632   \n",
              "234          0.045455              0.066388          0.006579   \n",
              "235          0.031880              0.053351          0.003904   \n",
              "236          0.038806              0.051940          0.009552   \n",
              "237          0.041802              0.060803          0.007058   \n",
              "238          0.042006              0.063950          0.010031   \n",
              "239          0.042189              0.057013          0.006842   \n",
              "240          0.028758              0.040523          0.006536   \n",
              "241          0.034188              0.054569          0.008547   \n",
              "242          0.039505              0.060142          0.005307   \n",
              "243          0.035207              0.057354          0.011357   \n",
              "244          0.047484              0.065008          0.003957   \n",
              "245          0.043210              0.065657          0.010101   \n",
              "246          0.043568              0.062759          0.006743   \n",
              "247          0.038303              0.061874          0.005893   \n",
              "248          0.036509              0.055904          0.007986   \n",
              "249          0.034877              0.052030          0.005718   \n",
              "250          0.036199              0.060520          0.005090   \n",
              "251          0.036573              0.055381          0.008359   \n",
              "252          0.032383              0.047280          0.006477   \n",
              "253          0.034660              0.053915          0.005777   \n",
              "254          0.038869              0.056537          0.007067   \n",
              "255          0.038644              0.058859          0.004756   \n",
              "256          0.027326              0.045349          0.007558   \n",
              "257          0.042319              0.064348          0.006377   \n",
              "258          0.036109              0.053582          0.006989   \n",
              "259          0.034373              0.055379          0.007002   \n",
              "260          0.037585              0.057301          0.003697   \n",
              "261          0.034502              0.055430          0.007919   \n",
              "262          0.037253              0.056461          0.009313   \n",
              "263          0.037137              0.054360          0.008073   \n",
              "264          0.030779              0.045854          0.005653   \n",
              "265          0.030117              0.051629          0.005532   \n",
              "266          0.031301              0.051268          0.009174   \n",
              "267          0.033421              0.050459          0.004587   \n",
              "268          0.027998              0.040170          0.007912   \n",
              "269          0.044280              0.062116          0.005535   \n",
              "270          0.032736              0.053737          0.007412   \n",
              "271          0.036455              0.057197          0.006914   \n",
              "272          0.028433              0.045372          0.004840   \n",
              "273          0.037449              0.060269          0.006437   \n",
              "274          0.033393              0.050089          0.007156   \n",
              "275          0.032919              0.048447          0.005590   \n",
              "276          0.035622              0.058290          0.007772   \n",
              "277          0.026065              0.038779          0.005722   \n",
              "278          0.031546              0.052050          0.009989   \n",
              "279          0.030559              0.049415          0.006502   \n",
              "280          0.035049              0.053122          0.007119   \n",
              "281          0.029983              0.044975          0.007773   \n",
              "282          0.034600              0.048213          0.006239   \n",
              "283          0.024720              0.045321          0.007652   \n",
              "284          0.031138              0.043713          0.009581   \n",
              "285          0.030056              0.048905          0.007641   \n",
              "286          0.036239              0.047794          0.006828   \n",
              "287          0.035368              0.055334          0.006275   \n",
              "288          0.028235              0.044118          0.004118   \n",
              "289          0.032391              0.051826          0.008245   \n",
              "290          0.033475              0.048353          0.006908   \n",
              "\n",
              "     majorana_(abstracts)  topological phase_(title)  \\\n",
              "0                0.003953                   0.001976   \n",
              "1                0.001916                   0.001916   \n",
              "2                0.003515                   0.001757   \n",
              "3                0.001848                   0.001848   \n",
              "4                0.003396                   0.001698   \n",
              "5                0.001842                   0.001842   \n",
              "6                0.001761                   0.001761   \n",
              "7                0.003810                   0.001905   \n",
              "8                0.001869                   0.001869   \n",
              "9                0.003630                   0.001815   \n",
              "10               0.001739                   0.003478   \n",
              "11               0.003521                   0.001761   \n",
              "12               0.001908                   0.001908   \n",
              "13               0.001754                   0.001754   \n",
              "14               0.002849                   0.001425   \n",
              "15               0.001610                   0.003221   \n",
              "16               0.004360                   0.001453   \n",
              "17               0.002797                   0.001399   \n",
              "18               0.001506                   0.001506   \n",
              "19               0.004959                   0.001653   \n",
              "20               0.001642                   0.001642   \n",
              "21               0.001416                   0.001416   \n",
              "22               0.001520                   0.003040   \n",
              "23               0.001776                   0.001776   \n",
              "24               0.001536                   0.001536   \n",
              "25               0.003373                   0.001686   \n",
              "26               0.001416                   0.001416   \n",
              "27               0.001486                   0.001486   \n",
              "28               0.001399                   0.001399   \n",
              "29               0.003091                   0.001546   \n",
              "30               0.001267                   0.001267   \n",
              "31               0.001517                   0.001517   \n",
              "32               0.001346                   0.001346   \n",
              "33               0.001307                   0.001307   \n",
              "34               0.001330                   0.001330   \n",
              "35               0.001422                   0.001422   \n",
              "36               0.001460                   0.001460   \n",
              "37               0.001488                   0.001488   \n",
              "38               0.001381                   0.001381   \n",
              "39               0.001333                   0.001333   \n",
              "40               0.003871                   0.003871   \n",
              "41               0.002500                   0.002500   \n",
              "42               0.001205                   0.002410   \n",
              "43               0.002857                   0.001429   \n",
              "44               0.002509                   0.003764   \n",
              "45               0.001181                   0.001181   \n",
              "46               0.001395                   0.001395   \n",
              "47               0.001242                   0.002484   \n",
              "48               0.001374                   0.002747   \n",
              "49               0.001323                   0.001323   \n",
              "50               0.001206                   0.001206   \n",
              "51               0.001221                   0.004884   \n",
              "52               0.001263                   0.001263   \n",
              "53               0.003501                   0.002334   \n",
              "54               0.001109                   0.001109   \n",
              "55               0.001328                   0.002656   \n",
              "56               0.002381                   0.001190   \n",
              "57               0.001163                   0.001163   \n",
              "58               0.003593                   0.002395   \n",
              "59               0.001160                   0.001160   \n",
              "60               0.001214                   0.001214   \n",
              "61               0.001361                   0.002721   \n",
              "62               0.002304                   0.001152   \n",
              "63               0.001104                   0.001104   \n",
              "64               0.001178                   0.003534   \n",
              "65               0.002220                   0.001110   \n",
              "66               0.001181                   0.001181   \n",
              "67               0.003584                   0.002389   \n",
              "68               0.002245                   0.001122   \n",
              "69               0.003233                   0.001078   \n",
              "70               0.003550                   0.002367   \n",
              "71               0.001203                   0.001203   \n",
              "72               0.002472                   0.002472   \n",
              "73               0.002597                   0.001299   \n",
              "74               0.001057                   0.002114   \n",
              "75               0.001325                   0.001325   \n",
              "76               0.003584                   0.001195   \n",
              "77               0.002250                   0.001125   \n",
              "78               0.002193                   0.001096   \n",
              "79               0.003681                   0.001227   \n",
              "80               0.003330                   0.002220   \n",
              "81               0.003148                   0.001049   \n",
              "82               0.002262                   0.002262   \n",
              "83               0.002577                   0.001289   \n",
              "84               0.002364                   0.001182   \n",
              "85               0.002581                   0.001290   \n",
              "86               0.003215                   0.001072   \n",
              "87               0.002519                   0.001259   \n",
              "88               0.001120                   0.001120   \n",
              "89               0.002294                   0.001147   \n",
              "90               0.004348                   0.002174   \n",
              "91               0.003472                   0.001157   \n",
              "92               0.003337                   0.002225   \n",
              "93               0.001045                   0.003135   \n",
              "94               0.003456                   0.001152   \n",
              "95               0.002497                   0.002497   \n",
              "96               0.002315                   0.001157   \n",
              "97               0.001253                   0.001253   \n",
              "98               0.005330                   0.002132   \n",
              "99               0.003132                   0.001044   \n",
              "100              0.003348                   0.002232   \n",
              "101              0.004931                   0.000986   \n",
              "102              0.000914                   0.002742   \n",
              "103              0.002162                   0.001081   \n",
              "104              0.001073                   0.002146   \n",
              "105              0.000955                   0.000955   \n",
              "106              0.005208                   0.003125   \n",
              "107              0.002181                   0.002181   \n",
              "108              0.002372                   0.001186   \n",
              "109              0.002205                   0.001103   \n",
              "110              0.003956                   0.000989   \n",
              "111              0.003282                   0.003282   \n",
              "112              0.003286                   0.001095   \n",
              "113              0.004292                   0.002146   \n",
              "114              0.007767                   0.001942   \n",
              "115              0.002326                   0.003488   \n",
              "116              0.005061                   0.002024   \n",
              "117              0.005917                   0.001972   \n",
              "118              0.007209                   0.001030   \n",
              "119              0.005837                   0.002918   \n",
              "120              0.005507                   0.001101   \n",
              "121              0.004283                   0.003212   \n",
              "122              0.007608                   0.002536   \n",
              "123              0.012059                   0.001855   \n",
              "124              0.004660                   0.001864   \n",
              "125              0.008953                   0.000895   \n",
              "126              0.007188                   0.002695   \n",
              "127              0.007233                   0.002712   \n",
              "128              0.004371                   0.002622   \n",
              "129              0.006740                   0.000842   \n",
              "130              0.011618                   0.002681   \n",
              "131              0.012511                   0.004468   \n",
              "132              0.006737                   0.003850   \n",
              "133              0.011225                   0.003742   \n",
              "134              0.008230                   0.002469   \n",
              "135              0.006428                   0.003673   \n",
              "136              0.013256                   0.002486   \n",
              "137              0.012275                   0.003273   \n",
              "138              0.007951                   0.003534   \n",
              "139              0.014617                   0.004299   \n",
              "140              0.010152                   0.002538   \n",
              "141              0.011986                   0.003425   \n",
              "142              0.016241                   0.003094   \n",
              "143              0.015216                   0.001691   \n",
              "144              0.009786                   0.001779   \n",
              "145              0.012723                   0.004241   \n",
              "146              0.014658                   0.004886   \n",
              "147              0.019481                   0.008117   \n",
              "148              0.010735                   0.004129   \n",
              "149              0.022411                   0.004637   \n",
              "150              0.013605                   0.005291   \n",
              "151              0.019377                   0.003370   \n",
              "152              0.007853                   0.006981   \n",
              "153              0.018491                   0.004438   \n",
              "154              0.017228                   0.007048   \n",
              "155              0.013412                   0.010059   \n",
              "156              0.015139                   0.006375   \n",
              "157              0.025237                   0.007098   \n",
              "158              0.021212                   0.006818   \n",
              "159              0.010836                   0.005418   \n",
              "160              0.015773                   0.003155   \n",
              "161              0.020392                   0.005710   \n",
              "162              0.018961                   0.004916   \n",
              "163              0.022708                   0.005887   \n",
              "164              0.015567                   0.003706   \n",
              "165              0.015407                   0.006603   \n",
              "166              0.011485                   0.004102   \n",
              "167              0.012893                   0.004029   \n",
              "168              0.013389                   0.006695   \n",
              "169              0.016273                   0.004882   \n",
              "170              0.024964                   0.008559   \n",
              "171              0.016764                   0.004191   \n",
              "172              0.020342                   0.005696   \n",
              "173              0.013393                   0.010417   \n",
              "174              0.016835                   0.005387   \n",
              "175              0.019449                   0.002431   \n",
              "176              0.019665                   0.007283   \n",
              "177              0.014655                   0.006281   \n",
              "178              0.014423                   0.005609   \n",
              "179              0.017794                   0.004270   \n",
              "180              0.020194                   0.004847   \n",
              "181              0.012977                   0.005344   \n",
              "182              0.016107                   0.007383   \n",
              "183              0.017424                   0.006818   \n",
              "184              0.011552                   0.005054   \n",
              "185              0.025780                   0.005427   \n",
              "186              0.016812                   0.006052   \n",
              "187              0.015004                   0.012003   \n",
              "188              0.016736                   0.006276   \n",
              "189              0.012474                   0.005544   \n",
              "190              0.017131                   0.004996   \n",
              "191              0.018659                   0.006911   \n",
              "192              0.025806                   0.010753   \n",
              "193              0.018611                   0.007158   \n",
              "194              0.022727                   0.009091   \n",
              "195              0.018732                   0.007925   \n",
              "196              0.019960                   0.007319   \n",
              "197              0.011628                   0.006840   \n",
              "198              0.020125                   0.005552   \n",
              "199              0.011315                   0.006365   \n",
              "200              0.021583                   0.007194   \n",
              "201              0.012314                   0.012314   \n",
              "202              0.018506                   0.005288   \n",
              "203              0.018206                   0.005394   \n",
              "204              0.020595                   0.006865   \n",
              "205              0.012186                   0.006093   \n",
              "206              0.025076                   0.006728   \n",
              "207              0.015094                   0.007547   \n",
              "208              0.019096                   0.007002   \n",
              "209              0.019460                   0.009416   \n",
              "210              0.022578                   0.009468   \n",
              "211              0.024858                   0.007812   \n",
              "212              0.016140                   0.010526   \n",
              "213              0.025324                   0.009265   \n",
              "214              0.019129                   0.010554   \n",
              "215              0.024080                   0.008696   \n",
              "216              0.028833                   0.009142   \n",
              "217              0.015613                   0.013383   \n",
              "218              0.018365                   0.008294   \n",
              "219              0.021651                   0.006089   \n",
              "220              0.020177                   0.004414   \n",
              "221              0.022814                   0.008238   \n",
              "222              0.024010                   0.009085   \n",
              "223              0.023464                   0.007591   \n",
              "224              0.030201                   0.006711   \n",
              "225              0.018086                   0.008168   \n",
              "226              0.024725                   0.006181   \n",
              "227              0.023778                   0.007926   \n",
              "228              0.019180                   0.005952   \n",
              "229              0.018231                   0.004727   \n",
              "230              0.017413                   0.008706   \n",
              "231              0.026448                   0.008816   \n",
              "232              0.022850                   0.009020   \n",
              "233              0.019399                   0.010013   \n",
              "234              0.019139                   0.009569   \n",
              "235              0.018217                   0.007807   \n",
              "236              0.022090                   0.007164   \n",
              "237              0.019544                   0.004886   \n",
              "238              0.020690                   0.005016   \n",
              "239              0.014823                   0.006842   \n",
              "240              0.015686                   0.009804   \n",
              "241              0.021039                   0.003287   \n",
              "242              0.020047                   0.007665   \n",
              "243              0.024418                   0.007382   \n",
              "244              0.014698                   0.006218   \n",
              "245              0.022447                   0.005051   \n",
              "246              0.019710                   0.006224   \n",
              "247              0.016500                   0.008839   \n",
              "248              0.018254                   0.007416   \n",
              "249              0.015437                   0.008576   \n",
              "250              0.017534                   0.005656   \n",
              "251              0.021944                   0.003657   \n",
              "252              0.020078                   0.004534   \n",
              "253              0.015404                   0.005777   \n",
              "254              0.022716                   0.007067   \n",
              "255              0.016647                   0.006540   \n",
              "256              0.020349                   0.006395   \n",
              "257              0.016232                   0.006957   \n",
              "258              0.015725                   0.004077   \n",
              "259              0.017823                   0.001273   \n",
              "260              0.014171                   0.008626   \n",
              "261              0.017534                   0.006787   \n",
              "262              0.018626                   0.008149   \n",
              "263              0.019376                   0.008073   \n",
              "264              0.010050                   0.008166   \n",
              "265              0.011063                   0.009834   \n",
              "266              0.017809                   0.005936   \n",
              "267              0.015072                   0.007208   \n",
              "268              0.017651                   0.007304   \n",
              "269              0.017220                   0.006765   \n",
              "270              0.018530                   0.002471   \n",
              "271              0.017599                   0.007542   \n",
              "272              0.016334                   0.006655   \n",
              "273              0.018139                   0.008777   \n",
              "274              0.023852                   0.007156   \n",
              "275              0.022360                   0.010559   \n",
              "276              0.020078                   0.002591   \n",
              "277              0.017800                   0.008264   \n",
              "278              0.020505                   0.008938   \n",
              "279              0.018856                   0.007802   \n",
              "280              0.013143                   0.009858   \n",
              "281              0.013881                   0.004442   \n",
              "282              0.014748                   0.004538   \n",
              "283              0.015303                   0.005886   \n",
              "284              0.019760                   0.006587   \n",
              "285              0.019358                   0.009679   \n",
              "286              0.019433                   0.007878   \n",
              "287              0.011979                   0.007986   \n",
              "288              0.010588                   0.007647   \n",
              "289              0.015901                   0.005300   \n",
              "290              0.021254                   0.007439   \n",
              "\n",
              "     topological phase_(abstracts)  ...  calculations_(title)  \\\n",
              "0                         0.003953  ...              0.013834   \n",
              "1                         0.005747  ...              0.009579   \n",
              "2                         0.001757  ...              0.010545   \n",
              "3                         0.001848  ...              0.009242   \n",
              "4                         0.001698  ...              0.006791   \n",
              "5                         0.003683  ...              0.007366   \n",
              "6                         0.003521  ...              0.012324   \n",
              "7                         0.003810  ...              0.019048   \n",
              "8                         0.001869  ...              0.016822   \n",
              "9                         0.001815  ...              0.010889   \n",
              "10                        0.003478  ...              0.029565   \n",
              "11                        0.001761  ...              0.010563   \n",
              "12                        0.001908  ...              0.007634   \n",
              "13                        0.001754  ...              0.012281   \n",
              "14                        0.001425  ...              0.017094   \n",
              "15                        0.001610  ...              0.009662   \n",
              "16                        0.001453  ...              0.008721   \n",
              "17                        0.002797  ...              0.006993   \n",
              "18                        0.004518  ...              0.012048   \n",
              "19                        0.001653  ...              0.011570   \n",
              "20                        0.001642  ...              0.019704   \n",
              "21                        0.001416  ...              0.009915   \n",
              "22                        0.003040  ...              0.024316   \n",
              "23                        0.003552  ...              0.012433   \n",
              "24                        0.001536  ...              0.010753   \n",
              "25                        0.001686  ...              0.013491   \n",
              "26                        0.002833  ...              0.004249   \n",
              "27                        0.005944  ...              0.007429   \n",
              "28                        0.001399  ...              0.009790   \n",
              "29                        0.004637  ...              0.009274   \n",
              "30                        0.002535  ...              0.015209   \n",
              "31                        0.001517  ...              0.010622   \n",
              "32                        0.001346  ...              0.017497   \n",
              "33                        0.002614  ...              0.016993   \n",
              "34                        0.002660  ...              0.010638   \n",
              "35                        0.002845  ...              0.015647   \n",
              "36                        0.001460  ...              0.011679   \n",
              "37                        0.001488  ...              0.008929   \n",
              "38                        0.001381  ...              0.011050   \n",
              "39                        0.002667  ...              0.012000   \n",
              "40                        0.003871  ...              0.009032   \n",
              "41                        0.005000  ...              0.005000   \n",
              "42                        0.001205  ...              0.013253   \n",
              "43                        0.001429  ...              0.014286   \n",
              "44                        0.006274  ...              0.012547   \n",
              "45                        0.002361  ...              0.012987   \n",
              "46                        0.002789  ...              0.011158   \n",
              "47                        0.002484  ...              0.006211   \n",
              "48                        0.005495  ...              0.024725   \n",
              "49                        0.001323  ...              0.005291   \n",
              "50                        0.002413  ...              0.009650   \n",
              "51                        0.002442  ...              0.010989   \n",
              "52                        0.002525  ...              0.013889   \n",
              "53                        0.001167  ...              0.012835   \n",
              "54                        0.001109  ...              0.009978   \n",
              "55                        0.002656  ...              0.010624   \n",
              "56                        0.001190  ...              0.020238   \n",
              "57                        0.002326  ...              0.009302   \n",
              "58                        0.001198  ...              0.002395   \n",
              "59                        0.001160  ...              0.010441   \n",
              "60                        0.001214  ...              0.013350   \n",
              "61                        0.006803  ...              0.013605   \n",
              "62                        0.002304  ...              0.009217   \n",
              "63                        0.003311  ...              0.009934   \n",
              "64                        0.003534  ...              0.014134   \n",
              "65                        0.002220  ...              0.007769   \n",
              "66                        0.001181  ...              0.014168   \n",
              "67                        0.002389  ...              0.013142   \n",
              "68                        0.002245  ...              0.008979   \n",
              "69                        0.002155  ...              0.011853   \n",
              "70                        0.003550  ...              0.014201   \n",
              "71                        0.001203  ...              0.010830   \n",
              "72                        0.001236  ...              0.019778   \n",
              "73                        0.002597  ...              0.010390   \n",
              "74                        0.003171  ...              0.014799   \n",
              "75                        0.001325  ...              0.005298   \n",
              "76                        0.003584  ...              0.013142   \n",
              "77                        0.001125  ...              0.012373   \n",
              "78                        0.002193  ...              0.008772   \n",
              "79                        0.001227  ...              0.012270   \n",
              "80                        0.004440  ...              0.017758   \n",
              "81                        0.003148  ...              0.020986   \n",
              "82                        0.002262  ...              0.019231   \n",
              "83                        0.002577  ...              0.014175   \n",
              "84                        0.002364  ...              0.013002   \n",
              "85                        0.001290  ...              0.014194   \n",
              "86                        0.001072  ...              0.013934   \n",
              "87                        0.003778  ...              0.016373   \n",
              "88                        0.001120  ...              0.010078   \n",
              "89                        0.002294  ...              0.010321   \n",
              "90                        0.003261  ...              0.021739   \n",
              "91                        0.003472  ...              0.012731   \n",
              "92                        0.004449  ...              0.015573   \n",
              "93                        0.003135  ...              0.021944   \n",
              "94                        0.003456  ...              0.017281   \n",
              "95                        0.002497  ...              0.011236   \n",
              "96                        0.001157  ...              0.016204   \n",
              "97                        0.003759  ...              0.016291   \n",
              "98                        0.004264  ...              0.011727   \n",
              "99                        0.001044  ...              0.008351   \n",
              "100                       0.004464  ...              0.010045   \n",
              "101                       0.001972  ...              0.013807   \n",
              "102                       0.002742  ...              0.015539   \n",
              "103                       0.003243  ...              0.017297   \n",
              "104                       0.002146  ...              0.008584   \n",
              "105                       0.000955  ...              0.018147   \n",
              "106                       0.003125  ...              0.015625   \n",
              "107                       0.006543  ...              0.009815   \n",
              "108                       0.005931  ...              0.009490   \n",
              "109                       0.003308  ...              0.015436   \n",
              "110                       0.002967  ...              0.020772   \n",
              "111                       0.004376  ...              0.008753   \n",
              "112                       0.001095  ...              0.019715   \n",
              "113                       0.003219  ...              0.015021   \n",
              "114                       0.003883  ...              0.011650   \n",
              "115                       0.005814  ...              0.013953   \n",
              "116                       0.005061  ...              0.010121   \n",
              "117                       0.001972  ...              0.012821   \n",
              "118                       0.001030  ...              0.017508   \n",
              "119                       0.003891  ...              0.012646   \n",
              "120                       0.006608  ...              0.016520   \n",
              "121                       0.004283  ...              0.007495   \n",
              "122                       0.005917  ...              0.015216   \n",
              "123                       0.004638  ...              0.008349   \n",
              "124                       0.005592  ...              0.013979   \n",
              "125                       0.008057  ...              0.011638   \n",
              "126                       0.006289  ...              0.012579   \n",
              "127                       0.006329  ...              0.018083   \n",
              "128                       0.002622  ...              0.018357   \n",
              "129                       0.005055  ...              0.010952   \n",
              "130                       0.006256  ...              0.016979   \n",
              "131                       0.009830  ...              0.008937   \n",
              "132                       0.003850  ...              0.013474   \n",
              "133                       0.004677  ...              0.017774   \n",
              "134                       0.004938  ...              0.013992   \n",
              "135                       0.008264  ...              0.011938   \n",
              "136                       0.006628  ...              0.014085   \n",
              "137                       0.008183  ...              0.009002   \n",
              "138                       0.004417  ...              0.014134   \n",
              "139                       0.007739  ...              0.011178   \n",
              "140                       0.005076  ...              0.011844   \n",
              "141                       0.006849  ...              0.010274   \n",
              "142                       0.009281  ...              0.015468   \n",
              "143                       0.005917  ...              0.010144   \n",
              "144                       0.008007  ...              0.011566   \n",
              "145                       0.007634  ...              0.011026   \n",
              "146                       0.004072  ...              0.011401   \n",
              "147                       0.012987  ...              0.009740   \n",
              "148                       0.009909  ...              0.009083   \n",
              "149                       0.009274  ...              0.008501   \n",
              "150                       0.011338  ...              0.010582   \n",
              "151                       0.009267  ...              0.006740   \n",
              "152                       0.012216  ...              0.012216   \n",
              "153                       0.008876  ...              0.008136   \n",
              "154                       0.012529  ...              0.011746   \n",
              "155                       0.012573  ...              0.010897   \n",
              "156                       0.012749  ...              0.011155   \n",
              "157                       0.011830  ...              0.013407   \n",
              "158                       0.012879  ...              0.016667   \n",
              "159                       0.017802  ...              0.010836   \n",
              "160                       0.011830  ...              0.011830   \n",
              "161                       0.011419  ...              0.014682   \n",
              "162                       0.012640  ...              0.007725   \n",
              "163                       0.013457  ...              0.010093   \n",
              "164                       0.013343  ...              0.011861   \n",
              "165                       0.008804  ...              0.018342   \n",
              "166                       0.013946  ...              0.009024   \n",
              "167                       0.010475  ...              0.008864   \n",
              "168                       0.018410  ...              0.015900   \n",
              "169                       0.011391  ...              0.013832   \n",
              "170                       0.019258  ...              0.007846   \n",
              "171                       0.008382  ...              0.014250   \n",
              "172                       0.014646  ...              0.008950   \n",
              "173                       0.016369  ...              0.011161   \n",
              "174                       0.018182  ...              0.014141   \n",
              "175                       0.015397  ...              0.008104   \n",
              "176                       0.019665  ...              0.011653   \n",
              "177                       0.016050  ...              0.012561   \n",
              "178                       0.008814  ...              0.014423   \n",
              "179                       0.012811  ...              0.009964   \n",
              "180                       0.013732  ...              0.012116   \n",
              "181                       0.019847  ...              0.007634   \n",
              "182                       0.018792  ...              0.018121   \n",
              "183                       0.012879  ...              0.010606   \n",
              "184                       0.014440  ...              0.013718   \n",
              "185                       0.021031  ...              0.014925   \n",
              "186                       0.011432  ...              0.008070   \n",
              "187                       0.014254  ...              0.004501   \n",
              "188                       0.018131  ...              0.013250   \n",
              "189                       0.011088  ...              0.013167   \n",
              "190                       0.022841  ...              0.009279   \n",
              "191                       0.017968  ...              0.012440   \n",
              "192                       0.020789  ...              0.016487   \n",
              "193                       0.013601  ...              0.010737   \n",
              "194                       0.018182  ...              0.014935   \n",
              "195                       0.024496  ...              0.012248   \n",
              "196                       0.022621  ...              0.009315   \n",
              "197                       0.014364  ...              0.010944   \n",
              "198                       0.022207  ...              0.007634   \n",
              "199                       0.018388  ...              0.009901   \n",
              "200                       0.018967  ...              0.007194   \n",
              "201                       0.016850  ...              0.010369   \n",
              "202                       0.019167  ...              0.011897   \n",
              "203                       0.022252  ...              0.012812   \n",
              "204                       0.022121  ...              0.017544   \n",
              "205                       0.012186  ...              0.005331   \n",
              "206                       0.017125  ...              0.007951   \n",
              "207                       0.012830  ...              0.012075   \n",
              "208                       0.026098  ...              0.008912   \n",
              "209                       0.020088  ...              0.012555   \n",
              "210                       0.024035  ...              0.012382   \n",
              "211                       0.018466  ...              0.009943   \n",
              "212                       0.026667  ...              0.009123   \n",
              "213                       0.027177  ...              0.009883   \n",
              "214                       0.023087  ...              0.011873   \n",
              "215                       0.025418  ...              0.012040   \n",
              "216                       0.024613  ...              0.011252   \n",
              "217                       0.027509  ...              0.006691   \n",
              "218                       0.023697  ...              0.010071   \n",
              "219                       0.017591  ...              0.012179   \n",
              "220                       0.026482  ...              0.014502   \n",
              "221                       0.022814  ...              0.012674   \n",
              "222                       0.026606  ...              0.010383   \n",
              "223                       0.024155  ...              0.013112   \n",
              "224                       0.029530  ...              0.012081   \n",
              "225                       0.020420  ...              0.012252   \n",
              "226                       0.018544  ...              0.012363   \n",
              "227                       0.021797  ...              0.010568   \n",
              "228                       0.022487  ...              0.011243   \n",
              "229                       0.016880  ...              0.012154   \n",
              "230                       0.024254  ...              0.008706   \n",
              "231                       0.020151  ...              0.010076   \n",
              "232                       0.036681  ...              0.007817   \n",
              "233                       0.028160  ...              0.005006   \n",
              "234                       0.024522  ...              0.010167   \n",
              "235                       0.021470  ...              0.011711   \n",
              "236                       0.030448  ...              0.009552   \n",
              "237                       0.029859  ...              0.007058   \n",
              "238                       0.026332  ...              0.010658   \n",
              "239                       0.026226  ...              0.011403   \n",
              "240                       0.031373  ...              0.011765   \n",
              "241                       0.015122  ...              0.012492   \n",
              "242                       0.025354  ...              0.013561   \n",
              "243                       0.031232  ...              0.003975   \n",
              "244                       0.023742  ...              0.011871   \n",
              "245                       0.022447  ...              0.010101   \n",
              "246                       0.021784  ...              0.006743   \n",
              "247                       0.024750  ...              0.010607   \n",
              "248                       0.025670  ...              0.011409   \n",
              "249                       0.024585  ...              0.011435   \n",
              "250                       0.026584  ...              0.006787   \n",
              "251                       0.020899  ...              0.008359   \n",
              "252                       0.018135  ...              0.009715   \n",
              "253                       0.021823  ...              0.006418   \n",
              "254                       0.027259  ...              0.010096   \n",
              "255                       0.023781  ...              0.010702   \n",
              "256                       0.022093  ...              0.010465   \n",
              "257                       0.029565  ...              0.008116   \n",
              "258                       0.029703  ...              0.007571   \n",
              "259                       0.025461  ...              0.008275   \n",
              "260                       0.030191  ...              0.007394   \n",
              "261                       0.019231  ...              0.007919   \n",
              "262                       0.025611  ...              0.010477   \n",
              "263                       0.037675  ...              0.009150   \n",
              "264                       0.028894  ...              0.008166   \n",
              "265                       0.029502  ...              0.011678   \n",
              "266                       0.023206  ...              0.013492   \n",
              "267                       0.020970  ...              0.011140   \n",
              "268                       0.019477  ...              0.010347   \n",
              "269                       0.023370  ...              0.005535   \n",
              "270                       0.025324  ...              0.008030   \n",
              "271                       0.026398  ...              0.009428   \n",
              "272                       0.029643  ...              0.008469   \n",
              "273                       0.028672  ...              0.014043   \n",
              "274                       0.023852  ...              0.009541   \n",
              "275                       0.028571  ...              0.007453   \n",
              "276                       0.023316  ...              0.012953   \n",
              "277                       0.023522  ...              0.012079   \n",
              "278                       0.024185  ...              0.011041   \n",
              "279                       0.026658  ...              0.007802   \n",
              "280                       0.030120  ...              0.007667   \n",
              "281                       0.026097  ...              0.007773   \n",
              "282                       0.020987  ...              0.012479   \n",
              "283                       0.018835  ...              0.006474   \n",
              "284                       0.030539  ...              0.013772   \n",
              "285                       0.024452  ...              0.006623   \n",
              "286                       0.029937  ...              0.009454   \n",
              "287                       0.027952  ...              0.007416   \n",
              "288                       0.028824  ...              0.010000   \n",
              "289                       0.021201  ...              0.014723   \n",
              "290                       0.025505  ...              0.014346   \n",
              "\n",
              "     calculations_(abstracts)  behavior_(title)  behavior_(abstracts)  \\\n",
              "0                    0.247036          0.021739              0.116601   \n",
              "1                    0.199234          0.011494              0.130268   \n",
              "2                    0.203866          0.017575              0.117750   \n",
              "3                    0.184843          0.018484              0.131238   \n",
              "4                    0.220713          0.018676              0.137521   \n",
              "5                    0.187845          0.016575              0.134438   \n",
              "6                    0.195423          0.024648              0.135563   \n",
              "7                    0.186667          0.017143              0.125714   \n",
              "8                    0.164486          0.018692              0.138318   \n",
              "9                    0.201452          0.021779              0.116152   \n",
              "10                   0.215652          0.005217              0.130435   \n",
              "11                   0.163732          0.022887              0.125000   \n",
              "12                   0.230916          0.020992              0.122137   \n",
              "13                   0.217544          0.021053              0.126316   \n",
              "14                   0.179487          0.019943              0.113960   \n",
              "15                   0.183575          0.016103              0.123994   \n",
              "16                   0.210756          0.020349              0.136628   \n",
              "17                   0.197203          0.012587              0.117483   \n",
              "18                   0.177711          0.015060              0.138554   \n",
              "19                   0.166942          0.009917              0.122314   \n",
              "20                   0.213465          0.026273              0.100164   \n",
              "21                   0.171388          0.012748              0.117564   \n",
              "22                   0.205167          0.018237              0.104863   \n",
              "23                   0.209591          0.014210              0.115453   \n",
              "24                   0.205837          0.012289              0.118280   \n",
              "25                   0.220911          0.018550              0.139966   \n",
              "26                   0.195467          0.024079              0.130312   \n",
              "27                   0.196137          0.016345              0.123328   \n",
              "28                   0.176224          0.019580              0.146853   \n",
              "29                   0.166924          0.021638              0.120556   \n",
              "30                   0.200253          0.020279              0.141952   \n",
              "31                   0.198786          0.018209              0.121396   \n",
              "32                   0.173620          0.025572              0.122476   \n",
              "33                   0.184314          0.018301              0.129412   \n",
              "34                   0.168883          0.011968              0.114362   \n",
              "35                   0.199147          0.024182              0.142248   \n",
              "36                   0.186861          0.018978              0.138686   \n",
              "37                   0.188988          0.016369              0.138393   \n",
              "38                   0.182320          0.016575              0.106354   \n",
              "39                   0.197333          0.018667              0.089333   \n",
              "40                   0.198710          0.023226              0.125161   \n",
              "41                   0.181250          0.017500              0.138750   \n",
              "42                   0.200000          0.021687              0.136145   \n",
              "43                   0.218571          0.022857              0.131429   \n",
              "44                   0.179423          0.012547              0.116688   \n",
              "45                   0.171192          0.016529              0.140496   \n",
              "46                   0.175732          0.016736              0.132497   \n",
              "47                   0.162733          0.016149              0.125466   \n",
              "48                   0.195055          0.012363              0.122253   \n",
              "49                   0.187831          0.018519              0.125661   \n",
              "50                   0.153197          0.015682              0.121834   \n",
              "51                   0.183150          0.010989              0.130647   \n",
              "52                   0.193182          0.021465              0.121212   \n",
              "53                   0.185531          0.011669              0.120187   \n",
              "54                   0.207317          0.018847              0.128603   \n",
              "55                   0.193891          0.022576              0.140770   \n",
              "56                   0.176190          0.022619              0.123810   \n",
              "57                   0.176744          0.018605              0.131395   \n",
              "58                   0.201198          0.004790              0.124551   \n",
              "59                   0.198376          0.015081              0.129930   \n",
              "60                   0.182039          0.015777              0.126214   \n",
              "61                   0.198639          0.017687              0.108844   \n",
              "62                   0.171659          0.010369              0.107143   \n",
              "63                   0.177704          0.019868              0.112583   \n",
              "64                   0.190813          0.018846              0.117786   \n",
              "65                   0.194229          0.014428              0.130966   \n",
              "66                   0.193625          0.022432              0.128689   \n",
              "67                   0.197133          0.014337              0.112306   \n",
              "68                   0.168350          0.022447              0.133558   \n",
              "69                   0.213362          0.012931              0.110991   \n",
              "70                   0.207101          0.015385              0.123077   \n",
              "71                   0.219013          0.018051              0.132371   \n",
              "72                   0.203956          0.019778              0.126082   \n",
              "73                   0.172727          0.018182              0.107792   \n",
              "74                   0.195560          0.021142              0.133192   \n",
              "75                   0.185430          0.026490              0.125828   \n",
              "76                   0.182796          0.007168              0.121864   \n",
              "77                   0.191226          0.015748              0.105737   \n",
              "78                   0.192982          0.008772              0.116228   \n",
              "79                   0.179141          0.019632              0.120245   \n",
              "80                   0.189789          0.017758              0.106548   \n",
              "81                   0.183631          0.012592              0.125918   \n",
              "82                   0.184389          0.012443              0.111991   \n",
              "83                   0.185567          0.015464              0.135309   \n",
              "84                   0.202128          0.014184              0.127660   \n",
              "85                   0.209032          0.012903              0.114839   \n",
              "86                   0.196141          0.013934              0.128617   \n",
              "87                   0.211587          0.017632              0.136020   \n",
              "88                   0.188130          0.007839              0.124300   \n",
              "89                   0.180046          0.021789              0.141055   \n",
              "90                   0.203261          0.010870              0.113043   \n",
              "91                   0.189815          0.015046              0.119213   \n",
              "92                   0.171301          0.012236              0.104561   \n",
              "93                   0.190178          0.016719              0.143156   \n",
              "94                   0.193548          0.013825              0.116359   \n",
              "95                   0.196005          0.024969              0.119850   \n",
              "96                   0.228009          0.015046              0.127315   \n",
              "97                   0.189223          0.021303              0.111529   \n",
              "98                   0.195096          0.009595              0.122601   \n",
              "99                   0.182672          0.013570              0.106472   \n",
              "100                  0.196429          0.014509              0.130580   \n",
              "101                  0.205128          0.013807              0.113412   \n",
              "102                  0.183729          0.007313              0.118830   \n",
              "103                  0.190270          0.010811              0.128649   \n",
              "104                  0.190987          0.017167              0.111588   \n",
              "105                  0.197708          0.017192              0.133715   \n",
              "106                  0.200000          0.019792              0.129167   \n",
              "107                  0.188659          0.016358              0.119956   \n",
              "108                  0.204033          0.013049              0.125741   \n",
              "109                  0.212789          0.018743              0.127894   \n",
              "110                  0.196835          0.020772              0.118694   \n",
              "111                  0.221007          0.012035              0.125821   \n",
              "112                  0.176342          0.017525              0.122673   \n",
              "113                  0.182403          0.013948              0.097639   \n",
              "114                  0.196117          0.014563              0.118447   \n",
              "115                  0.213953          0.013953              0.148837   \n",
              "116                  0.209514          0.021255              0.141700   \n",
              "117                  0.194280          0.011834              0.127219   \n",
              "118                  0.215242          0.018538              0.138002   \n",
              "119                  0.189689          0.011673              0.134241   \n",
              "120                  0.211454          0.018722              0.140969   \n",
              "121                  0.224839          0.014989              0.123126   \n",
              "122                  0.177515          0.016906              0.140321   \n",
              "123                  0.188312          0.012059              0.099258   \n",
              "124                  0.182665          0.019571              0.124884   \n",
              "125                  0.181737          0.017905              0.117278   \n",
              "126                  0.203953          0.010782              0.126685   \n",
              "127                  0.190778          0.009946              0.132911   \n",
              "128                  0.204545          0.015734              0.124126   \n",
              "129                  0.177759          0.014322              0.128896   \n",
              "130                  0.194817          0.011618              0.123324   \n",
              "131                  0.191242          0.012511              0.133155   \n",
              "132                  0.203080          0.013474              0.121270   \n",
              "133                  0.193639          0.012161              0.100094   \n",
              "134                  0.190123          0.009877              0.114403   \n",
              "135                  0.203857          0.015611              0.117539   \n",
              "136                  0.196355          0.012428              0.107705   \n",
              "137                  0.192308          0.016367              0.116203   \n",
              "138                  0.193463          0.016784              0.115724   \n",
              "139                  0.182287          0.010318              0.124678   \n",
              "140                  0.187817          0.006768              0.116751   \n",
              "141                  0.166952          0.011986              0.133562   \n",
              "142                  0.217324          0.017788              0.124517   \n",
              "143                  0.180051          0.009298              0.126796   \n",
              "144                  0.191281          0.008897              0.120996   \n",
              "145                  0.181510          0.013571              0.127226   \n",
              "146                  0.194625          0.009772              0.134365   \n",
              "147                  0.176136          0.012987              0.127435   \n",
              "148                  0.201486          0.013212              0.127993   \n",
              "149                  0.195518          0.015456              0.136012   \n",
              "150                  0.174603          0.018896              0.128496   \n",
              "151                  0.186184          0.014322              0.116259   \n",
              "152                  0.195462          0.013089              0.127400   \n",
              "153                  0.231509          0.005917              0.127219   \n",
              "154                  0.194205          0.010963              0.110415   \n",
              "155                  0.182733          0.015926              0.124895   \n",
              "156                  0.181673          0.007171              0.120319   \n",
              "157                  0.194006          0.007098              0.122240   \n",
              "158                  0.213636          0.009091              0.131061   \n",
              "159                  0.195046          0.016254              0.111455   \n",
              "160                  0.205836          0.011041              0.119085   \n",
              "161                  0.203100          0.009788              0.137031   \n",
              "162                  0.192416          0.011236              0.127809   \n",
              "163                  0.191758          0.010934              0.124474   \n",
              "164                  0.198666          0.011861              0.126019   \n",
              "165                  0.202494          0.007337              0.118855   \n",
              "166                  0.205086          0.010664              0.127153   \n",
              "167                  0.188558          0.008864              0.130540   \n",
              "168                  0.200000          0.012552              0.097908   \n",
              "169                  0.187144          0.007323              0.113914   \n",
              "170                  0.196862          0.007846              0.120542   \n",
              "171                  0.195306          0.011735              0.121542   \n",
              "172                  0.211554          0.013019              0.113100   \n",
              "173                  0.188988          0.010417              0.118304   \n",
              "174                  0.183165          0.016162              0.131987   \n",
              "175                  0.176661          0.018639              0.120746   \n",
              "176                  0.195193          0.016752              0.116533   \n",
              "177                  0.195394          0.013957              0.120726   \n",
              "178                  0.206731          0.010417              0.129808   \n",
              "179                  0.193594          0.019217              0.115302   \n",
              "180                  0.229402          0.014540              0.115509   \n",
              "181                  0.172519          0.006107              0.119084   \n",
              "182                  0.182550          0.005369              0.114765   \n",
              "183                  0.198485          0.008333              0.118939   \n",
              "184                  0.191336          0.006498              0.109747   \n",
              "185                  0.185889          0.014247              0.103121   \n",
              "186                  0.182919          0.008070              0.128447   \n",
              "187                  0.204051          0.013503              0.123031   \n",
              "188                  0.186890          0.014644              0.107392   \n",
              "189                  0.198198          0.007623              0.118503   \n",
              "190                  0.178444          0.013562              0.131335   \n",
              "191                  0.201106          0.006220              0.117484   \n",
              "192                  0.200717          0.012186              0.116846   \n",
              "193                  0.202577          0.015748              0.132427   \n",
              "194                  0.220779          0.009091              0.105195   \n",
              "195                  0.201729          0.009366              0.124640   \n",
              "196                  0.198270          0.008649              0.119760   \n",
              "197                  0.207934          0.011628              0.114911   \n",
              "198                  0.185982          0.010409              0.120056   \n",
              "199                  0.198020          0.012730              0.117397   \n",
              "200                  0.181164          0.009156              0.117070   \n",
              "201                  0.191186          0.011666              0.137395   \n",
              "202                  0.201586          0.012558              0.120952   \n",
              "203                  0.198921          0.010115              0.118004   \n",
              "204                  0.186880          0.011442              0.134249   \n",
              "205                  0.188880          0.008378              0.131759   \n",
              "206                  0.196942          0.007951              0.116208   \n",
              "207                  0.200755          0.009811              0.117736   \n",
              "208                  0.194780          0.010185              0.120306   \n",
              "209                  0.188324          0.008788              0.116133   \n",
              "210                  0.182083          0.009468              0.120175   \n",
              "211                  0.198153          0.010653              0.113636   \n",
              "212                  0.206316          0.011930              0.124211   \n",
              "213                  0.204447          0.011118              0.117356   \n",
              "214                  0.201187          0.014512              0.140501   \n",
              "215                  0.195318          0.016722              0.121739   \n",
              "216                  0.202532          0.014065              0.123769   \n",
              "217                  0.183643          0.008922              0.130855   \n",
              "218                  0.200829          0.011848              0.117299   \n",
              "219                  0.186739          0.011502              0.132612   \n",
              "220                  0.191677          0.010719              0.117276   \n",
              "221                  0.200253          0.008238              0.117871   \n",
              "222                  0.200519          0.012330              0.122648   \n",
              "223                  0.197378          0.011042              0.132505   \n",
              "224                  0.203356          0.014765              0.120805   \n",
              "225                  0.177946          0.012835              0.127188   \n",
              "226                  0.195742          0.004121              0.117445   \n",
              "227                  0.175694          0.017173              0.126816   \n",
              "228                  0.187169          0.009259              0.103175   \n",
              "229                  0.201891          0.008778              0.118163   \n",
              "230                  0.187811          0.012438              0.124378   \n",
              "231                  0.184509          0.011335              0.132872   \n",
              "232                  0.201443          0.007216              0.123271   \n",
              "233                  0.178974          0.013141              0.132040   \n",
              "234                  0.202153          0.011364              0.117225   \n",
              "235                  0.184776          0.012362              0.121666   \n",
              "236                  0.174328          0.008358              0.125373   \n",
              "237                  0.192182          0.010858              0.129207   \n",
              "238                  0.191223          0.011912              0.124765   \n",
              "239                  0.192132          0.011973              0.118586   \n",
              "240                  0.182353          0.009804              0.108497   \n",
              "241                  0.190007          0.011834              0.116371   \n",
              "242                  0.192217          0.015330              0.133844   \n",
              "243                  0.176036          0.013629              0.128336   \n",
              "244                  0.184850          0.010741              0.113058   \n",
              "245                  0.177890          0.011785              0.109989   \n",
              "246                  0.193983          0.011929              0.123963   \n",
              "247                  0.206246          0.008250              0.113141   \n",
              "248                  0.187108          0.007986              0.124929   \n",
              "249                  0.198971          0.005718              0.112064   \n",
              "250                  0.186652          0.012443              0.119344   \n",
              "251                  0.188610          0.014629              0.127482   \n",
              "252                  0.185881          0.010363              0.106218   \n",
              "253                  0.175225          0.011553              0.127086   \n",
              "254                  0.172640          0.008077              0.115093   \n",
              "255                  0.188466          0.011296              0.117717   \n",
              "256                  0.211047          0.008721              0.125000   \n",
              "257                  0.180870          0.004058              0.111884   \n",
              "258                  0.172394          0.009901              0.133372   \n",
              "259                  0.184596          0.008912              0.120306   \n",
              "260                  0.191620          0.009858              0.117683   \n",
              "261                  0.188914          0.009615              0.114253   \n",
              "262                  0.189173          0.014552              0.128056   \n",
              "263                  0.184069          0.005920              0.107643   \n",
              "264                  0.189698          0.010678              0.122487   \n",
              "265                  0.215120          0.011678              0.111862   \n",
              "266                  0.177010          0.007016              0.121425   \n",
              "267                  0.186763          0.004587              0.111402   \n",
              "268                  0.189288          0.009738              0.118077   \n",
              "269                  0.191267          0.010455              0.121156   \n",
              "270                  0.172329          0.012971              0.112415   \n",
              "271                  0.179133          0.007542              0.115651   \n",
              "272                  0.188143          0.011494              0.113733   \n",
              "273                  0.202458          0.010532              0.126390   \n",
              "274                  0.189028          0.011330              0.119857   \n",
              "275                  0.178882          0.010559              0.121739   \n",
              "276                  0.207254          0.009067              0.126295   \n",
              "277                  0.190718          0.010172              0.125874   \n",
              "278                  0.181914          0.007361              0.120925   \n",
              "279                  0.191808          0.009103              0.132640   \n",
              "280                  0.194962          0.007667              0.135268   \n",
              "281                  0.162132          0.011105              0.126041   \n",
              "282                  0.175269          0.007941              0.132728   \n",
              "283                  0.201295          0.014715              0.128311   \n",
              "284                  0.201198          0.007186              0.113772   \n",
              "285                  0.177789          0.009679              0.140092   \n",
              "286                  0.176996          0.008929              0.122374   \n",
              "287                  0.198517          0.013120              0.139760   \n",
              "288                  0.172941          0.012941              0.127059   \n",
              "289                  0.162544          0.012956              0.140754   \n",
              "290                  0.187566          0.008502              0.154623   \n",
              "\n",
              "     one_(title)  one_(abstracts)  dependence_(title)  dependence_(abstracts)  \\\n",
              "0       0.033597         0.199605            0.031621                0.227273   \n",
              "1       0.038314         0.216475            0.015326                0.212644   \n",
              "2       0.022847         0.240773            0.031634                0.231986   \n",
              "3       0.033272         0.192237            0.033272                0.210721   \n",
              "4       0.033956         0.258065            0.022071                0.229202   \n",
              "5       0.027624         0.224678            0.023941                0.220994   \n",
              "6       0.028169         0.234155            0.012324                0.214789   \n",
              "7       0.032381         0.194286            0.011429                0.240000   \n",
              "8       0.031776         0.228037            0.020561                0.209346   \n",
              "9       0.034483         0.221416            0.021779                0.206897   \n",
              "10      0.029565         0.253913            0.022609                0.226087   \n",
              "11      0.026408         0.183099            0.021127                0.234155   \n",
              "12      0.034351         0.202290            0.013359                0.198473   \n",
              "13      0.036842         0.212281            0.015789                0.194737   \n",
              "14      0.024217         0.220798            0.022792                0.230769   \n",
              "15      0.022544         0.202899            0.030596                0.244767   \n",
              "16      0.020349         0.207849            0.021802                0.218023   \n",
              "17      0.018182         0.187413            0.027972                0.205594   \n",
              "18      0.024096         0.206325            0.027108                0.225904   \n",
              "19      0.014876         0.181818            0.028099                0.221488   \n",
              "20      0.021346         0.226601            0.026273                0.246305   \n",
              "21      0.025496         0.208215            0.026912                0.223796   \n",
              "22      0.025836         0.189970            0.031915                0.221884   \n",
              "23      0.019538         0.191829            0.019538                0.220249   \n",
              "24      0.021505         0.193548            0.023041                0.231951   \n",
              "25      0.020236         0.217538            0.032040                0.251265   \n",
              "26      0.026912         0.212465            0.031161                0.245042   \n",
              "27      0.026746         0.216939            0.019316                0.205052   \n",
              "28      0.016783         0.198601            0.018182                0.209790   \n",
              "29      0.018547         0.173107            0.035549                0.231839   \n",
              "30      0.020279         0.205323            0.021546                0.231939   \n",
              "31      0.031866         0.207891            0.019727                0.268589   \n",
              "32      0.022880         0.207268            0.025572                0.231494   \n",
              "33      0.026144         0.206536            0.020915                0.227451   \n",
              "34      0.013298         0.204787            0.021277                0.208777   \n",
              "35      0.024182         0.209104            0.022760                0.238976   \n",
              "36      0.033577         0.210219            0.026277                0.216058   \n",
              "37      0.016369         0.197917            0.014881                0.230655   \n",
              "38      0.022099         0.194751            0.029006                0.223757   \n",
              "39      0.025333         0.216000            0.025333                0.225333   \n",
              "40      0.018065         0.209032            0.018065                0.233548   \n",
              "41      0.028750         0.218750            0.018750                0.232500   \n",
              "42      0.030120         0.207229            0.022892                0.244578   \n",
              "43      0.030000         0.201429            0.027143                0.220000   \n",
              "44      0.025094         0.208281            0.023839                0.217064   \n",
              "45      0.029516         0.219599            0.017710                0.221960   \n",
              "46      0.023710         0.213389            0.025105                0.227336   \n",
              "47      0.016149         0.206211            0.026087                0.219876   \n",
              "48      0.030220         0.207418            0.019231                0.206044   \n",
              "49      0.026455         0.239418            0.021164                0.227513   \n",
              "50      0.019300         0.217129            0.018094                0.213510   \n",
              "51      0.023199         0.208791            0.026862                0.263736   \n",
              "52      0.025253         0.227273            0.017677                0.227273   \n",
              "53      0.016336         0.190198            0.023337                0.266044   \n",
              "54      0.023282         0.220621            0.029933                0.250554   \n",
              "55      0.029216         0.239044            0.023904                0.235060   \n",
              "56      0.017857         0.191667            0.025000                0.223810   \n",
              "57      0.025581         0.211628            0.025581                0.230233   \n",
              "58      0.021557         0.217964            0.026347                0.225150   \n",
              "59      0.025522         0.238979            0.032483                0.232019   \n",
              "60      0.023058         0.202670            0.027913                0.218447   \n",
              "61      0.019048         0.225850            0.031293                0.214966   \n",
              "62      0.017281         0.211982            0.027650                0.252304   \n",
              "63      0.020971         0.218543            0.016556                0.220751   \n",
              "64      0.035336         0.221437            0.020024                0.203769   \n",
              "65      0.028857         0.216426            0.022198                0.236404   \n",
              "66      0.041322         0.224321            0.024793                0.232586   \n",
              "67      0.020311         0.199522            0.021505                0.207885   \n",
              "68      0.023569         0.213244            0.020202                0.252525   \n",
              "69      0.025862         0.209052            0.025862                0.211207   \n",
              "70      0.026036         0.221302            0.023669                0.228402   \n",
              "71      0.032491         0.245487            0.019254                0.234657   \n",
              "72      0.028430         0.233622            0.025958                0.244747   \n",
              "73      0.023377         0.220779            0.025974                0.223377   \n",
              "74      0.019027         0.228330            0.023256                0.281184   \n",
              "75      0.022517         0.214570            0.026490                0.235762   \n",
              "76      0.034648         0.216249            0.021505                0.256870   \n",
              "77      0.033746         0.201350            0.041620                0.236220   \n",
              "78      0.020833         0.218202            0.032895                0.237939   \n",
              "79      0.018405         0.212270            0.026994                0.226994   \n",
              "80      0.018868         0.200888            0.017758                0.219756   \n",
              "81      0.025184         0.208814            0.030430                0.251836   \n",
              "82      0.023756         0.210407            0.015837                0.223982   \n",
              "83      0.025773         0.217784            0.025773                0.253866   \n",
              "84      0.024823         0.198582            0.026005                0.244681   \n",
              "85      0.023226         0.210323            0.024516                0.246452   \n",
              "86      0.012862         0.202572            0.024652                0.218650   \n",
              "87      0.016373         0.214106            0.030227                0.239295   \n",
              "88      0.024636         0.226204            0.029115                0.265398   \n",
              "89      0.020642         0.191514            0.022936                0.237385   \n",
              "90      0.021739         0.215217            0.022826                0.239130   \n",
              "91      0.032407         0.228009            0.031250                0.223380   \n",
              "92      0.026696         0.209121            0.030033                0.228031   \n",
              "93      0.019854         0.211076            0.032393                0.257053   \n",
              "94      0.024194         0.210829            0.017281                0.228111   \n",
              "95      0.026217         0.232210            0.026217                0.227216   \n",
              "96      0.021991         0.206019            0.027778                0.258102   \n",
              "97      0.025063         0.228070            0.021303                0.200501   \n",
              "98      0.022388         0.202559            0.027719                0.205757   \n",
              "99      0.025052         0.196242            0.022965                0.213987   \n",
              "100     0.030134         0.218750            0.026786                0.228795   \n",
              "101     0.023669         0.218935            0.030572                0.258383   \n",
              "102     0.021024         0.209324            0.027422                0.215722   \n",
              "103     0.018378         0.197838            0.023784                0.237838   \n",
              "104     0.019313         0.203863            0.020386                0.244635   \n",
              "105     0.017192         0.185291            0.025788                0.246418   \n",
              "106     0.022917         0.214583            0.029167                0.253125   \n",
              "107     0.020720         0.224646            0.029444                0.257361   \n",
              "108     0.027284         0.206406            0.020166                0.263345   \n",
              "109     0.023153         0.205072            0.038589                0.259096   \n",
              "110     0.012859         0.208704            0.019782                0.225519   \n",
              "111     0.022976         0.200219            0.035011                0.234136   \n",
              "112     0.014239         0.219058            0.025192                0.236583   \n",
              "113     0.020386         0.234979            0.024678                0.215665   \n",
              "114     0.026214         0.207767            0.026214                0.239806   \n",
              "115     0.017442         0.197674            0.029070                0.229070   \n",
              "116     0.025304         0.212551            0.025304                0.223684   \n",
              "117     0.018738         0.216963            0.021696                0.263314   \n",
              "118     0.026777         0.246138            0.028836                0.228630   \n",
              "119     0.017510         0.214008            0.023346                0.246109   \n",
              "120     0.016520         0.204846            0.022026                0.251101   \n",
              "121     0.026767         0.198073            0.022484                0.241970   \n",
              "122     0.027895         0.221471            0.022823                0.240068   \n",
              "123     0.015770         0.246753            0.032468                0.248609   \n",
              "124     0.019571         0.200373            0.021435                0.216216   \n",
              "125     0.011638         0.201432            0.019696                0.240824   \n",
              "126     0.018868         0.203055            0.013477                0.228212   \n",
              "127     0.027125         0.199819            0.033454                0.243219   \n",
              "128     0.020105         0.205420            0.025350                0.222902   \n",
              "129     0.022746         0.216512            0.022746                0.260320   \n",
              "130     0.020554         0.202860            0.028597                0.228776   \n",
              "131     0.025916         0.219839            0.025022                0.237712   \n",
              "132     0.024062         0.243503            0.018287                0.238691   \n",
              "133     0.024322         0.222638            0.013096                0.217025   \n",
              "134     0.022222         0.200823            0.024691                0.240329   \n",
              "135     0.017447         0.185491            0.028466                0.232323   \n",
              "136     0.034797         0.217067            0.029826                0.229495   \n",
              "137     0.026187         0.220131            0.031097                0.230769   \n",
              "138     0.025618         0.207597            0.027385                0.246466   \n",
              "139     0.018917         0.176268            0.021496                0.249355   \n",
              "140     0.016920         0.204738            0.027073                0.253807   \n",
              "141     0.026541         0.227740            0.019692                0.233733   \n",
              "142     0.019335         0.242846            0.020882                0.241299   \n",
              "143     0.022823         0.224007            0.025359                0.221471   \n",
              "144     0.022242         0.208185            0.024021                0.233986   \n",
              "145     0.023749         0.220526            0.018660                0.233249   \n",
              "146     0.021987         0.221498            0.026059                0.232085   \n",
              "147     0.023539         0.202922            0.025974                0.255682   \n",
              "148     0.020644         0.190751            0.026424                0.230388   \n",
              "149     0.021638         0.202473            0.037867                0.257342   \n",
              "150     0.024943         0.235072            0.022676                0.238095   \n",
              "151     0.011794         0.196293            0.022746                0.235889   \n",
              "152     0.024433         0.220768            0.022688                0.242583   \n",
              "153     0.022189         0.198964            0.032544                0.246302   \n",
              "154     0.016445         0.191073            0.027408                0.254503   \n",
              "155     0.021794         0.203688            0.020117                0.238894   \n",
              "156     0.026295         0.232669            0.024701                0.225498   \n",
              "157     0.015773         0.197161            0.033912                0.242902   \n",
              "158     0.021970         0.225758            0.030303                0.253788   \n",
              "159     0.021672         0.208204            0.017028                0.232198   \n",
              "160     0.014984         0.218454            0.026814                0.216877   \n",
              "161     0.023654         0.239804            0.019576                0.234095   \n",
              "162     0.016152         0.212781            0.020365                0.230337   \n",
              "163     0.020185         0.193440            0.025231                0.232128   \n",
              "164     0.020015         0.217198            0.022239                0.223870   \n",
              "165     0.014674         0.224505            0.016141                0.227439   \n",
              "166     0.021329         0.219032            0.022970                0.214930   \n",
              "167     0.025786         0.229654            0.020951                0.230459   \n",
              "168     0.021757         0.225105            0.022594                0.243515   \n",
              "169     0.017901         0.210740            0.023596                0.248983   \n",
              "170     0.022825         0.228245            0.020685                0.232525   \n",
              "171     0.026823         0.220453            0.025147                0.232188   \n",
              "172     0.026037         0.222945            0.016273                0.233523   \n",
              "173     0.023810         0.228423            0.023065                0.253720   \n",
              "174     0.014815         0.212795            0.023569                0.235017   \n",
              "175     0.021880         0.230956            0.021880                0.270665   \n",
              "176     0.018208         0.213401            0.016023                0.209760   \n",
              "177     0.023029         0.229588            0.024424                0.220516   \n",
              "178     0.022436         0.215545            0.024840                0.233974   \n",
              "179     0.024199         0.234164            0.025623                0.244840   \n",
              "180     0.027464         0.226171            0.018578                0.230210   \n",
              "181     0.027481         0.215267            0.019084                0.259542   \n",
              "182     0.025503         0.237584            0.017450                0.236242   \n",
              "183     0.019697         0.224242            0.031818                0.247727   \n",
              "184     0.027437         0.231769            0.026715                0.236823   \n",
              "185     0.014247         0.202171            0.025102                0.227951   \n",
              "186     0.020847         0.223941            0.018830                0.238736   \n",
              "187     0.025506         0.234059            0.021755                0.234809   \n",
              "188     0.025802         0.215481            0.021618                0.231520   \n",
              "189     0.022869         0.224532            0.023562                0.218295   \n",
              "190     0.014989         0.229122            0.022127                0.229836   \n",
              "191     0.015895         0.223912            0.023497                0.241189   \n",
              "192     0.015771         0.222939            0.022222                0.223656   \n",
              "193     0.023622         0.235505            0.023622                0.256263   \n",
              "194     0.019481         0.216234            0.017532                0.241558   \n",
              "195     0.016571         0.223343            0.021614                0.238473   \n",
              "196     0.022621         0.228876            0.028609                0.214238   \n",
              "197     0.021888         0.226402            0.023940                0.223666   \n",
              "198     0.014573         0.222068            0.021513                0.243581   \n",
              "199     0.014144         0.220651            0.021216                0.244696   \n",
              "200     0.019621         0.235448            0.019621                0.247220   \n",
              "201     0.026572         0.215165            0.016850                0.220350   \n",
              "202     0.020489         0.226041            0.018506                0.234633   \n",
              "203     0.020229         0.231962            0.016858                0.236682   \n",
              "204     0.027460         0.225782            0.022121                0.259344   \n",
              "205     0.017517         0.229246            0.031988                0.242193   \n",
              "206     0.014679         0.231193            0.020795                0.234251   \n",
              "207     0.016604         0.215094            0.019623                0.250566   \n",
              "208     0.021006         0.232973            0.017823                0.245703   \n",
              "209     0.016949         0.224733            0.020716                0.252982   \n",
              "210     0.020393         0.257101            0.017480                0.246176   \n",
              "211     0.019886         0.220170            0.029119                0.249290   \n",
              "212     0.013333         0.200000            0.022456                0.236491   \n",
              "213     0.021001         0.232242            0.020383                0.211859   \n",
              "214     0.021108         0.218997            0.021768                0.223615   \n",
              "215     0.019398         0.215385            0.018060                0.239465   \n",
              "216     0.023207         0.248242            0.023910                0.233474   \n",
              "217     0.020818         0.228996            0.024535                0.241636   \n",
              "218     0.020735         0.224526            0.023104                0.241114   \n",
              "219     0.026387         0.228011            0.016238                0.231394   \n",
              "220     0.018916         0.218159            0.020177                0.234552   \n",
              "221     0.021546         0.226869            0.013308                0.232573   \n",
              "222     0.014925         0.217391            0.018170                0.236859   \n",
              "223     0.018634         0.240166            0.019324                0.229124   \n",
              "224     0.020134         0.230201            0.016107                0.228859   \n",
              "225     0.016336         0.235123            0.019837                0.201867   \n",
              "226     0.014423         0.219093            0.016484                0.245192   \n",
              "227     0.016513         0.229855            0.025099                0.235799   \n",
              "228     0.014550         0.220899            0.016534                0.227513   \n",
              "229     0.018231         0.226874            0.019581                0.243079   \n",
              "230     0.025498         0.227612            0.021766                0.235697   \n",
              "231     0.017003         0.214736            0.015743                0.206549   \n",
              "232     0.018641         0.223692            0.017438                0.233915   \n",
              "233     0.012516         0.222153            0.014393                0.238423   \n",
              "234     0.014354         0.209330            0.017943                0.247608   \n",
              "235     0.020820         0.235524            0.022121                0.219909   \n",
              "236     0.019701         0.237015            0.017910                0.202985   \n",
              "237     0.021716         0.226384            0.016830                0.222041   \n",
              "238     0.017555         0.221317            0.018182                0.223197   \n",
              "239     0.011403         0.221779            0.023375                0.220639   \n",
              "240     0.022222         0.222222            0.019608                0.217647   \n",
              "241     0.019066         0.206443            0.021696                0.207101   \n",
              "242     0.022406         0.219929            0.023585                0.228184   \n",
              "243     0.015332         0.233958            0.019875                0.223737   \n",
              "244     0.014698         0.225551            0.019785                0.231769   \n",
              "245     0.017396         0.218855            0.016835                0.231762   \n",
              "246     0.022303         0.228734            0.021784                0.241183   \n",
              "247     0.012964         0.217443            0.015321                0.214496   \n",
              "248     0.017684         0.219053            0.016543                0.225898   \n",
              "249     0.012007         0.202401            0.022298                0.233848   \n",
              "250     0.014706         0.229072            0.020362                0.230769   \n",
              "251     0.016719         0.223093            0.021944                0.225705   \n",
              "252     0.014896         0.218264            0.018782                0.229275   \n",
              "253     0.013479         0.211810            0.016688                0.224005   \n",
              "254     0.014639         0.220091            0.017163                0.226148   \n",
              "255     0.015458         0.243163            0.019025                0.219382   \n",
              "256     0.019767         0.220930            0.016279                0.209884   \n",
              "257     0.014493         0.234203            0.020870                0.239420   \n",
              "258     0.019220         0.222481            0.018055                0.232382   \n",
              "259     0.013367         0.211967            0.017187                0.213240   \n",
              "260     0.015404         0.209489            0.021565                0.229205   \n",
              "261     0.016968         0.221719            0.018665                0.238122   \n",
              "262     0.011059         0.213620            0.018626                0.213620   \n",
              "263     0.018837         0.235199            0.022605                0.242734   \n",
              "264     0.008794         0.211055            0.020101                0.231156   \n",
              "265     0.020283         0.233559            0.016595                0.234173   \n",
              "266     0.018888         0.212628            0.020507                0.219644   \n",
              "267     0.013761         0.233290            0.020315                0.222805   \n",
              "268     0.014607         0.218503            0.015825                0.231893   \n",
              "269     0.020295         0.231857            0.019065                0.222632   \n",
              "270     0.016677         0.222359            0.014824                0.205065   \n",
              "271     0.011942         0.225644            0.020113                0.222502   \n",
              "272     0.012704         0.231095            0.019964                0.228675   \n",
              "273     0.018724         0.225278            0.017554                0.241662   \n",
              "274     0.017889         0.231366            0.018485                0.237925   \n",
              "275     0.009938         0.226087            0.015528                0.215528   \n",
              "276     0.021373         0.228627            0.022668                0.232513   \n",
              "277     0.024158         0.200890            0.026065                0.212969   \n",
              "278     0.019453         0.229758            0.021556                0.231335   \n",
              "279     0.014304         0.222367            0.018856                0.223667   \n",
              "280     0.015882         0.200986            0.012596                0.233844   \n",
              "281     0.009994         0.210439            0.016657                0.213770   \n",
              "282     0.018151         0.233693            0.018151                0.239932   \n",
              "283     0.011183         0.210712            0.018835                0.216009   \n",
              "284     0.020958         0.233533            0.017365                0.216168   \n",
              "285     0.012736         0.235354            0.017830                0.226184   \n",
              "286     0.012605         0.213235            0.017332                0.200630   \n",
              "287     0.012550         0.210496            0.019395                0.206503   \n",
              "288     0.014706         0.202941            0.019412                0.218824   \n",
              "289     0.013545         0.203769            0.024735                0.226148   \n",
              "290     0.010627         0.208289            0.018066                0.227418   \n",
              "\n",
              "     function_(title)  function_(abstracts)  \n",
              "0            0.023715              0.258893  \n",
              "1            0.034483              0.237548  \n",
              "2            0.026362              0.219684  \n",
              "3            0.025878              0.232902  \n",
              "4            0.028862              0.254669  \n",
              "5            0.023941              0.237569  \n",
              "6            0.031690              0.258803  \n",
              "7            0.022857              0.245714  \n",
              "8            0.018692              0.218692  \n",
              "9            0.018149              0.257713  \n",
              "10           0.026087              0.241739  \n",
              "11           0.019366              0.216549  \n",
              "12           0.026718              0.234733  \n",
              "13           0.017544              0.217544  \n",
              "14           0.025641              0.216524  \n",
              "15           0.022544              0.238325  \n",
              "16           0.024709              0.250000  \n",
              "17           0.023776              0.234965  \n",
              "18           0.022590              0.213855  \n",
              "19           0.033058              0.213223  \n",
              "20           0.029557              0.269294  \n",
              "21           0.025496              0.215297  \n",
              "22           0.022796              0.238602  \n",
              "23           0.042629              0.220249  \n",
              "24           0.023041              0.202765  \n",
              "25           0.020236              0.231029  \n",
              "26           0.038244              0.209632  \n",
              "27           0.035661              0.225854  \n",
              "28           0.046154              0.227972  \n",
              "29           0.027821              0.204019  \n",
              "30           0.017744              0.202788  \n",
              "31           0.028832              0.242792  \n",
              "32           0.024226              0.193809  \n",
              "33           0.028758              0.220915  \n",
              "34           0.022606              0.234043  \n",
              "35           0.039829              0.227596  \n",
              "36           0.039416              0.237956  \n",
              "37           0.022321              0.247024  \n",
              "38           0.023481              0.220994  \n",
              "39           0.018667              0.206667  \n",
              "40           0.023226              0.211613  \n",
              "41           0.026250              0.210000  \n",
              "42           0.024096              0.216867  \n",
              "43           0.024286              0.227143  \n",
              "44           0.028858              0.229611  \n",
              "45           0.027155              0.233766  \n",
              "46           0.022315              0.205021  \n",
              "47           0.012422              0.228571  \n",
              "48           0.023352              0.233516  \n",
              "49           0.026455              0.210317  \n",
              "50           0.018094              0.215923  \n",
              "51           0.029304              0.234432  \n",
              "52           0.029040              0.243687  \n",
              "53           0.028005              0.225204  \n",
              "54           0.021064              0.220621  \n",
              "55           0.030544              0.245684  \n",
              "56           0.032143              0.219048  \n",
              "57           0.033721              0.211628  \n",
              "58           0.032335              0.255090  \n",
              "59           0.013921              0.229698  \n",
              "60           0.024272              0.208738  \n",
              "61           0.028571              0.224490  \n",
              "62           0.023041              0.231567  \n",
              "63           0.041943              0.246137  \n",
              "64           0.022379              0.244994  \n",
              "65           0.031077              0.240844  \n",
              "66           0.025974              0.204250  \n",
              "67           0.026284              0.235364  \n",
              "68           0.023569              0.213244  \n",
              "69           0.033405              0.234914  \n",
              "70           0.035503              0.249704  \n",
              "71           0.022864              0.226233  \n",
              "72           0.023486              0.208900  \n",
              "73           0.035065              0.249351  \n",
              "74           0.012685              0.213531  \n",
              "75           0.029139              0.233113  \n",
              "76           0.035842              0.243728  \n",
              "77           0.026997              0.218223  \n",
              "78           0.017544              0.223684  \n",
              "79           0.024540              0.239264  \n",
              "80           0.026637              0.233074  \n",
              "81           0.028332              0.236097  \n",
              "82           0.027149              0.259050  \n",
              "83           0.023196              0.213918  \n",
              "84           0.034279              0.230496  \n",
              "85           0.027097              0.240000  \n",
              "86           0.034298              0.233655  \n",
              "87           0.030227              0.229219  \n",
              "88           0.019037              0.231803  \n",
              "89           0.017202              0.208716  \n",
              "90           0.031522              0.241304  \n",
              "91           0.037037              0.231481  \n",
              "92           0.022247              0.233593  \n",
              "93           0.025078              0.239289  \n",
              "94           0.036866              0.256912  \n",
              "95           0.033708              0.257179  \n",
              "96           0.038194              0.253472  \n",
              "97           0.021303              0.216792  \n",
              "98           0.029851              0.252665  \n",
              "99           0.029228              0.224426  \n",
              "100          0.022321              0.204241  \n",
              "101          0.018738              0.210059  \n",
              "102          0.022852              0.211152  \n",
              "103          0.027027              0.244324  \n",
              "104          0.034335              0.263948  \n",
              "105          0.031519              0.246418  \n",
              "106          0.021875              0.257292  \n",
              "107          0.030534              0.232279  \n",
              "108          0.027284              0.220641  \n",
              "109          0.033076              0.231533  \n",
              "110          0.030663              0.249258  \n",
              "111          0.029540              0.233042  \n",
              "112          0.016429              0.223439  \n",
              "113          0.028970              0.217811  \n",
              "114          0.034951              0.244660  \n",
              "115          0.017442              0.236047  \n",
              "116          0.025304              0.239879  \n",
              "117          0.028600              0.258383  \n",
              "118          0.039135              0.236869  \n",
              "119          0.029183              0.230545  \n",
              "120          0.026432              0.227974  \n",
              "121          0.028908              0.231263  \n",
              "122          0.029586              0.226543  \n",
              "123          0.027829              0.235622  \n",
              "124          0.018639              0.209692  \n",
              "125          0.020591              0.222023  \n",
              "126          0.037736              0.227314  \n",
              "127          0.036166              0.233273  \n",
              "128          0.027098              0.235140  \n",
              "129          0.028644              0.221567  \n",
              "130          0.021448              0.242181  \n",
              "131          0.033065              0.235031  \n",
              "132          0.040423              0.235804  \n",
              "133          0.032741              0.231057  \n",
              "134          0.032099              0.222222  \n",
              "135          0.028466              0.226814  \n",
              "136          0.026512              0.231152  \n",
              "137          0.025368              0.230769  \n",
              "138          0.025618              0.227915  \n",
              "139          0.023216              0.227859  \n",
              "140          0.024535              0.202200  \n",
              "141          0.019692              0.221747  \n",
              "142          0.024749              0.238979  \n",
              "143          0.029586              0.222316  \n",
              "144          0.025801              0.248221  \n",
              "145          0.017812              0.234945  \n",
              "146          0.026873              0.241042  \n",
              "147          0.024351              0.213474  \n",
              "148          0.031379              0.251858  \n",
              "149          0.031685              0.227975  \n",
              "150          0.033258              0.229025  \n",
              "151          0.027801              0.215670  \n",
              "152          0.034031              0.252182  \n",
              "153          0.029586              0.234467  \n",
              "154          0.029757              0.245889  \n",
              "155          0.025147              0.230511  \n",
              "156          0.027092              0.244622  \n",
              "157          0.025237              0.236593  \n",
              "158          0.028788              0.229545  \n",
              "159          0.027864              0.230650  \n",
              "160          0.029180              0.241325  \n",
              "161          0.019576              0.227569  \n",
              "162          0.025983              0.226826  \n",
              "163          0.023549              0.223717  \n",
              "164          0.034841              0.247591  \n",
              "165          0.023478              0.241379  \n",
              "166          0.027892              0.237900  \n",
              "167          0.030620              0.241741  \n",
              "168          0.035146              0.246862  \n",
              "169          0.024410              0.227828  \n",
              "170          0.026391              0.248217  \n",
              "171          0.019279              0.237217  \n",
              "172          0.025224              0.222132  \n",
              "173          0.031250              0.232887  \n",
              "174          0.029630              0.226936  \n",
              "175          0.014587              0.217990  \n",
              "176          0.019665              0.217771  \n",
              "177          0.018144              0.217027  \n",
              "178          0.027244              0.228365  \n",
              "179          0.024911              0.248399  \n",
              "180          0.034733              0.246365  \n",
              "181          0.015267              0.220611  \n",
              "182          0.027517              0.232886  \n",
              "183          0.024242              0.234848  \n",
              "184          0.023827              0.241155  \n",
              "185          0.020353              0.230665  \n",
              "186          0.026900              0.223941  \n",
              "187          0.027007              0.242311  \n",
              "188          0.027197              0.225941  \n",
              "189          0.029106              0.250173  \n",
              "190          0.020700              0.220557  \n",
              "191          0.030408              0.243953  \n",
              "192          0.021505              0.231541  \n",
              "193          0.029349              0.236220  \n",
              "194          0.029870              0.242208  \n",
              "195          0.019452              0.229107  \n",
              "196          0.019960              0.245509  \n",
              "197          0.017784              0.244870  \n",
              "198          0.025677              0.238723  \n",
              "199          0.020509              0.228430  \n",
              "200          0.022891              0.232832  \n",
              "201          0.022683              0.228127  \n",
              "202          0.022472              0.245869  \n",
              "203          0.020904              0.219825  \n",
              "204          0.018307              0.220442  \n",
              "205          0.018279              0.233816  \n",
              "206          0.020183              0.226911  \n",
              "207          0.022642              0.230189  \n",
              "208          0.022279              0.215786  \n",
              "209          0.018205              0.221594  \n",
              "210          0.026948              0.230881  \n",
              "211          0.031960              0.242898  \n",
              "212          0.032281              0.254737  \n",
              "213          0.030883              0.230389  \n",
              "214          0.023087              0.241425  \n",
              "215          0.029431              0.233445  \n",
              "216          0.028129              0.237693  \n",
              "217          0.021561              0.249071  \n",
              "218          0.024289              0.248815  \n",
              "219          0.018945              0.226658  \n",
              "220          0.027743              0.239596  \n",
              "221          0.018378              0.238276  \n",
              "222          0.022064              0.231668  \n",
              "223          0.024155              0.250518  \n",
              "224          0.020805              0.238926  \n",
              "225          0.023921              0.239790  \n",
              "226          0.019918              0.245879  \n",
              "227          0.023118              0.216645  \n",
              "228          0.030423              0.228836  \n",
              "229          0.021607              0.232951  \n",
              "230          0.021766              0.228856  \n",
              "231          0.031486              0.251889  \n",
              "232          0.024053              0.227300  \n",
              "233          0.023154              0.228411  \n",
              "234          0.018541              0.230263  \n",
              "235          0.029278              0.226415  \n",
              "236          0.014328              0.222090  \n",
              "237          0.026059              0.238871  \n",
              "238          0.023197              0.221317  \n",
              "239          0.022805              0.216078  \n",
              "240          0.020261              0.226144  \n",
              "241          0.020381              0.234714  \n",
              "242          0.022995              0.248821  \n",
              "243          0.024986              0.223737  \n",
              "244          0.025438              0.234031  \n",
              "245          0.017957              0.220539  \n",
              "246          0.020228              0.226141  \n",
              "247          0.018857              0.242192  \n",
              "248          0.020536              0.240730  \n",
              "249          0.027444              0.240137  \n",
              "250          0.026018              0.244344  \n",
              "251          0.015674              0.236677  \n",
              "252          0.026554              0.242876  \n",
              "253          0.024390              0.238768  \n",
              "254          0.026249              0.241292  \n",
              "255          0.030321              0.245541  \n",
              "256          0.022674              0.238372  \n",
              "257          0.025507              0.251014  \n",
              "258          0.022714              0.230635  \n",
              "259          0.024188              0.246976  \n",
              "260          0.024030              0.245841  \n",
              "261          0.023190              0.230769  \n",
              "262          0.023283              0.242724  \n",
              "263          0.025834              0.241119  \n",
              "264          0.023869              0.233040  \n",
              "265          0.028273              0.256915  \n",
              "266          0.021047              0.238532  \n",
              "267          0.025557              0.249017  \n",
              "268          0.018259              0.220329  \n",
              "269          0.016605              0.218942  \n",
              "270          0.020383              0.225448  \n",
              "271          0.022627              0.252043  \n",
              "272          0.022989              0.228675  \n",
              "273          0.026331              0.233470  \n",
              "274          0.022660              0.238521  \n",
              "275          0.023602              0.225466  \n",
              "276          0.018782              0.241580  \n",
              "277          0.023522              0.244755  \n",
              "278          0.028917              0.243428  \n",
              "279          0.018205              0.241873  \n",
              "280          0.019168              0.232202  \n",
              "281          0.017768              0.207107  \n",
              "282          0.026092              0.234260  \n",
              "283          0.019423              0.244850  \n",
              "284          0.027545              0.259281  \n",
              "285          0.017830              0.239939  \n",
              "286          0.019958              0.237920  \n",
              "287          0.024529              0.250428  \n",
              "288          0.022941              0.226471  \n",
              "289          0.022968              0.222026  \n",
              "290          0.026567              0.251328  \n",
              "\n",
              "[291 rows x 1000 columns]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time_series_data = pd.DataFrame()\n",
        "\n",
        "for term in keywords.index.tolist():\n",
        "    title, abstract = get_time_series_data(term)\n",
        "    time_series_data[vocabulary[term]+'_(title)'] = title\n",
        "    time_series_data[vocabulary[term]+'_(abstracts)'] = abstract\n",
        "\n",
        "time_series_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b47576a-4814-4304-a022-5f29b129d7a0",
      "metadata": {
        "id": "2b47576a-4814-4304-a022-5f29b129d7a0"
      },
      "source": [
        "Let's plot the document frequencies of the top 5 words ranked by abstract document frequency over time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7382f878-c360-409a-825b-82905676697a",
      "metadata": {
        "id": "7382f878-c360-409a-825b-82905676697a"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(1,1, figsize=(18, 6))\n",
        "ax.set_title('Frequencies of keywords')\n",
        "ax.set_xlabel('months since March 1992')\n",
        "ax.set_ylabel('frequency')\n",
        "\n",
        "for term in ['machine_learning_(title)', 's_(title)', 'spin_(title)', 'magnetic_(title):\n",
        "    sns.lineplot(x=range(earliest_semester,latest_semester), y=, ax=ax, linewidth=1, label=keywords[i])\n",
        "\n",
        "plt.yscale('log')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "369a759e-33b6-4d5d-908c-3f4cc4de98e8",
      "metadata": {
        "id": "369a759e-33b6-4d5d-908c-3f4cc4de98e8"
      },
      "source": [
        "What is striking is that the document frequencies of these words appear to have converged over time, including the word \"challenge.\" Let's look at the next five most frequent words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31a20b34-03db-480f-a750-121609668eee",
      "metadata": {
        "scrolled": true,
        "id": "31a20b34-03db-480f-a750-121609668eee"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(1,1, figsize=(18, 6))\n",
        "ax.set_title('Frequencies of keywords')\n",
        "ax.set_xlabel('months since March 1992')\n",
        "ax.set_ylabel('frequency')\n",
        "\n",
        "for i in range(5,10):\n",
        "    sns.lineplot(x=range(earliest_semester,latest_semester), y=[get_df(x) for x in semesterly_abstracts_idf[:,list(vocabulary).index(keywords[i])]], ax=ax, linewidth=1, label=keywords[i])\n",
        "\n",
        "plt.yscale('log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95ed915d-7638-4d08-9e97-64aa3f4ae03f",
      "metadata": {
        "id": "95ed915d-7638-4d08-9e97-64aa3f4ae03f"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(1,1, figsize=(18, 6))\n",
        "ax.set_title('Frequencies of keywords')\n",
        "ax.set_xlabel('months since March 1992')\n",
        "ax.set_ylabel('frequency')\n",
        "\n",
        "for word in selected_keywords:\n",
        "    sns.lineplot(x=range(earliest_semester,latest_semester), y=[get_df(x) for x in semesterly_abstracts_idf[:,list(vocabulary).index(word)]], ax=ax, linewidth=1, label=word)\n",
        "\n",
        "#plt.yscale('log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc818b39-c908-4098-a324-438d58d642d2",
      "metadata": {
        "id": "dc818b39-c908-4098-a324-438d58d642d2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b8826d-a3c7-4607-b513-732b76a1a42d",
      "metadata": {
        "id": "25b8826d-a3c7-4607-b513-732b76a1a42d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}